# Default parameters for various LLM models
default_params:
  gpt-4o:
    temperature: 0.5
    max_tokens: 2048
  gpt-4-turbo:
    temperature: 0.3
    max_tokens: 4096
  claude-3-sonnet:
    temperature: 0.6
    max_tokens: 4000
  
  # OpenRouter Free Models
  deepseek/deepseek-chat-v3.1:free:
    temperature: 0.7
    max_tokens: 4096
  x-ai/grok-4-fast:free:
    temperature: 0.7
    max_tokens: 4096
  deepseek/deepseek-r1-0528:free:
    temperature: 0.7
    max_tokens: 4096
  deepseek/deepseek-chat-v3-0324:free:
    temperature: 0.7
    max_tokens: 4096
  tngtech/deepseek-r1t2-chimera:free:
    temperature: 0.7
    max_tokens: 4096
  moonshotai/kimi-k2:free:
    temperature: 0.7
    max_tokens: 4096