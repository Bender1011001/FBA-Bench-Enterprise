<examples>
  <overview>
    Complete workflow examples demonstrating how to generate PhD-level research paper sections
    based on FBA-Bench repo analysis. These illustrate integration of workflow, patterns, best practices,
    and tool usage for benchmark validation and experiment reporting.
  </overview>

  <example name="full_paper_workflow">
    <description>End-to-end generation of a short research paper on benchmark validity.</description>
    <context>User request: "Write a paper validating the FBA-Bench benchmarks and experiments."</context>
    <workflow_steps>
      <step>Initialization: Use list_files on tests/benchmarking/ to map files.</step>
      <step>Analysis: search_files for "metrics" in .py files; read_file on performance_benchmarks.py.</step>
      <step>Writing: Apply abstract and methodology patterns; generate results table from data.</step>
      <step>Validation: execute_command "make test-contracts" to confirm reproducibility.</step>
      <step>Output: write_to_file benchmark_validation.md with full content.</step>
    </workflow_steps>
    <code language="markdown"><![CDATA[
# Validating AI Agent Benchmarks in FBA-Bench: A Rigorous Experimental Framework

## Abstract
[Use abstract pattern: Summarize FBA-Bench as a benchmark for AI agents, highlighting validation via 50+ experiments in medusa_experiments/, showing 18% avg. improvement in metrics from tests/benchmarking/test_advanced_metrics.py. Contributions: reproducible suite, statistical validation, insights on scalability.]

## 1 Introduction
[Use introduction pattern: Motivate with AI eval gaps; relate to learning/ and infrastructure/; outline contributions.]

## 2 Methodology
[Use methodology_benchmarks pattern: Describe scenarios from configs/, runners from tests/integration/runners/, metrics from test_metrics.py.]

## 3 Results
[Use results_presentation pattern: Table with data from perf_results/, e.g.,

| Metric | Baseline | FBA-Bench | p-value |
|--------|----------|-----------|---------|
| Efficiency | 0.65 | 0.82 | <0.001 |

Figure 1: Performance curves from performance_monitor.py.]

## 4 Discussion
[Use discussion pattern: Implications for AI research; limitations from integration tests; future via community/.]

## References
[Use references pattern: Cite repo and 5 external papers.]

    ]]></code>
    <explanation>
      This example produces a 10-page paper draft. Tools ensure data accuracy; patterns maintain structure.
      Line count ~200; output as .md for LaTeX conversion.
    </explanation>
  </example>

  <example name="benchmark_validation_section">
    <description>Focused section on experiment validity, using tool outputs.</description>
    <context>After reading medusa_trainer.py and running tests.</context>
    <code language="markdown"><![CDATA[
### 3.2 Statistical Validation of Experiments
To validate benchmark results, we applied paired t-tests to outputs from medusa_experiments/medusa_trainer.py (lines 45-67: training loop). With n=100 runs via scripts/run_gpt5_inloop_benchmark.py, we observed t(99)=4.23, p<0.001, confirming significant gains in agent adaptation.

Ablation on learning/meta_learning.py (lines 12-30) showed 12% drop without meta-components, underscoring their role (ANOVA F(2,297)=15.6, p<0.01).

Reproducibility: Docker setup in docker-compose-simple.yml; dependencies pinned in poetry.lock.
    ]]></code>
    <explanation>
      Grounds claims in tool results (e.g., read_file lines, execute_command outputs). Follows rigor principle;
      includes stats for peer review.
    </explanation>
  </example>

  <example name="results_table_generation">
    <description>Creating a table from repo data for results section.</description>
    <context>After search_files on perf_results/ and tests/.</context>
    <code language="markdown"><![CDATA[
Table 2: Comparative Benchmark Performance

| Scenario (from scenarios/) | Accuracy (%) | Latency (s) | Scalability (Agents) | Source File |
|----------------------------|--------------|-------------|----------------------|-------------|
| Product Sourcing | 92.3 ± 1.2 | 2.1 ± 0.3 | 50 | tests/integration/test_product_sourcing_flow.py |
| Medusa Evolution | 87.5 ± 2.1 | 5.4 ± 0.8 | 200 | medusa_experiments/schema.py |
| RL Environment | 78.9 ± 1.8 | 3.2 ± 0.5 | 100 | learning/rl_environment.py |

Statistical significance: All improvements over baselines (t-tests, p<0.05). Data aggregated from 20 runs.
    ]]></code>
    <explanation>
      Synthesizes search/read_file outputs into visual evidence. Best practice: Include means, SDs, p-values.
      Cite sources for transparency.
    </explanation>
  </example>

  <example name="limitations_and_ethics">
    <description>Balanced discussion subsection addressing pitfalls.</description>
    <context>Analysis reveals biases in redteam_scripts/.</context>
    <code language="markdown"><![CDATA[
#### 4.2 Limitations and Ethical Considerations
While FBA-Bench excels in controlled simulations, real-world deployment may vary due to unmodeled factors (e.g., network latency beyond infrastructure/distributed_coordinator.py). Our benchmarks focus on efficiency but less on fairness; future integrations with redteam_scripts/examples/phishing_supplier_impersonation_v1.0.0.yaml could address adversarial robustness.

Ethically, we mitigate bias via diverse scenarios in configs/, but recommend audits for deployment. No harmful outputs observed in tests/integration/test_health_and_security.py.
    ]]></code>
    <explanation>
      Applies objectivity principle; references repo for evidence. Preempts reviewer critiques on ethics in AI.
    </explanation>
  </example>

  <usage_notes>
    <note>Scale examples: Use full for complete papers; sections for iterations.</note>
    <note>Integrate tools: Always base on actual repo data, not placeholders.</note>
    <note>Customization: Adapt to user focus (e.g., emphasize medusa for evolution papers).</note>
  </usage_notes>
</examples>