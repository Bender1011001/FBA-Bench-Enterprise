<best_practices>
  <general_principles>
    <principle priority="high">
      <name>Scientific Rigor</name>
      <description>Ensure all claims are supported by empirical evidence from the codebase, experiments, or benchmarks. Avoid speculation; base conclusions on reproducible data.</description>
      <rationale>Maintains credibility and enables peer review validation.</rationale>
      <example>
        <scenario>When discussing benchmark validity</scenario>
        <good>Use statistical tests (e.g., t-tests on results from performance_benchmarks.py) to demonstrate significance.</good>
        <bad>Claim "superior performance" without quantitative metrics.</bad>
      </example>
    </principle>

    <principle priority="high">
      <name>Reproducibility</name>
      <description>Include detailed setup instructions, code references, and configuration excerpts to allow replication of experiments and benchmarks.</description>
      <rationale>Core to scientific research; facilitates verification by the community.</rationale>
      <example>
        <scenario>Describing experiment methodology</scenario>
        <good>Reference specific files like medusa_trainer.py and include YAML configs from configs/.</good>
        <bad>Omit environment details or assume reader familiarity.</bad>
      </example>
    </principle>

    <principle priority="medium">
      <name>Objectivity and Balance</name>
      <description>Present limitations, potential biases in benchmarks, and alternative interpretations fairly.</description>
      <rationale>Enhances trustworthiness and invites constructive critique.</rationale>
      <example>
        <scenario>In results discussion</scenario>
        <good>Acknowledge dataset limitations in FBA-Bench scenarios and suggest improvements.</good>
        <bad>Overstate results without discussing edge cases from tests/integration/.</bad>
      </example>
    </principle>
  </general_principles>

  <writing_conventions>
    <convention category="structure">
      <rule>Follow IMRaD format: Introduction, Methods, Results and Analysis, Discussion. Include abstract, keywords, references, and appendices for supplementary materials.</rule>
      <template>
        Abstract: 150-250 words summarizing contributions.
        Introduction: Problem statement, novelty, and outline.
        Methods: Detailed benchmark/experiment descriptions.
        Results: Tables/figures from repo data.
        Discussion: Implications and limitations.
      </template>
    </convention>
    
    <convention category="language">
      <rule>Use formal, precise academic English. Avoid contractions, jargon without definition, and first-person unless in acknowledgments.</rule>
      <examples>
        <good>"The proposed benchmark demonstrates a 15% improvement in agent efficiency (p < 0.01)."</good>
        <bad>"Our cool new thing works way better lol."</bad>
      </examples>
    </convention>

    <convention category="citations">
      <rule>Cite repo elements inline (e.g., [FBA-Bench, 2025]) and external literature. Use consistent style (e.g., APA, IEEE).</rule>
      <details>For internal: Treat codebase as a technical report; for external: Use DOIs or standard refs.</details>
    </convention>
  </writing_conventions>

  <common_pitfalls>
    <pitfall>
      <description>Overgeneralizing from limited experiments</description>
      <why_problematic>Undermines validity; reviewers expect comprehensive evaluation.</why_problematic>
      <correct_approach>Scope claims to tested scenarios (e.g., reference specific tests in tests/benchmarking/) and suggest generalizations with caveats.</correct_approach>
    </pitfall>

    <pitfall>
      <description>Ignoring ethical considerations in AI benchmarks</description>
      <why_problematic>AI research requires addressing bias, fairness, and societal impact.</why_problematic>
      <correct_approach>Include a subsection on ethics, referencing redteam_scripts/ for adversarial testing.</correct_approach>
    </pitfall>

    <pitfall>
      <description>Poor visualization of results</description>
      <why_problematic>Readers rely on figures/tables for quick insights.</why_problematic>
      <correct_approach>Use Markdown tables or LaTeX for clear, labeled visuals derived from perf_results/.</correct_approach>
    </pitfall>
  </common_pitfalls>

  <quality_checklist>
    <category name="before_drafting">
      <item>Verify data from repo (e.g., run scripts/validate_config.py if needed)</item>
      <item>Outline paper structure aligned with user request</item>
      <item>Identify key contributions from benchmarks/experiments</item>
    </category>
    <category name="during_writing">
      <item>Cross-reference every claim with code/file evidence</item>
      <item>Maintain consistent terminology (e.g., "FBA-Bench" for the project)</item>
      <item>Ensure inclusive, bias-free language</item>
    </category>
    <category name="before_completion">
      <item>Proofread for grammar, clarity, and flow</item>
      <item>Check word count and section balance</item>
      <item>Simulate peer review: Address potential critiques</item>
    </category>
  </quality_checklist>
</best_practices>