<tool_usage_guide>
  <overview>
    Guidelines for using tools to analyze the repository, extract data for benchmarks/experiments,
    and generate research paper content. Prioritize read operations for evidence gathering;
    use edit tools only for outputting paper files (.md, .tex).
  </overview>

  <tool_priorities>
    <priority level="1">
      <tool>list_files</tool>
      <when>Initial project exploration to map structure</when>
      <why>Provides overview of directories like tests/benchmarking/ and medusa_experiments/</why>
    </priority>
    <priority level="2">
      <tool>search_files</tool>
      <when>Finding specific patterns (e.g., experiment results, metrics)</when>
      <why>Regex search across files reveals data in perf_results/ or configs/</why>
    </priority>
    <priority level="3">
      <tool>read_file</tool>
      <when>Detailed examination after identifying key files</when>
      <why>Line-numbered content for precise quoting and analysis (limit to 5 files per call)</why>
    </priority>
    <priority level="4">
      <tool>execute_command</tool>
      <when>Running benchmarks or validating experiments</when>
      <why>Executes scripts like run_gpt5_inloop_benchmark.py for fresh data</why>
    </priority>
    <priority level="5">
      <tool>write_to_file or apply_diff</tool>
      <when>Generating paper output</when>
      <why>Creates .md files with full content; use apply_diff for revisions</why>
    </priority>
  </tool_priorities>

  <tool_specific_guidance>
    <tool name="list_files">
      <purpose>Explore directory structure to identify relevant areas for analysis.</purpose>
      <when_to_use>Start of analysis phase; recursive for deep dives into tests/ or src/.</when_to_use>
      <syntax>
        <command><list_files>
          <path>tests/benchmarking/</path>
          <recursive>true</recursive>
        </list_files></command>
      </syntax>
      <examples>
        <example scenario="Map experiment files">
          <code><list_files>
            <path>medusa_experiments/</path>
            <recursive>true</recursive>
          </list_files></code>
          <output>Lists medusa_trainer.py, genomes/, etc., for methodology section.</output>
        </example>
      </examples>
      <best_practices>
        <practice>Use non-recursive for top-level overviews</practice>
        <practice>Follow up with search_files on interesting directories</practice>
      </best_practices>
    </tool>

    <tool name="search_files">
      <purpose>Locate code patterns, results, or configurations across the repo.</purpose>
      <when_to_use>When needing context-rich matches for claims (e.g., search for "p-value" in test files).</when_to_use>
      <syntax>
        <command><search_files>
          <path>.</path>
          <regex>benchmark.*result</regex>
          <file_pattern>*.py</file_pattern>
        </search_files></command>
      </syntax>
      <examples>
        <example scenario="Find benchmark metrics">
          <code><search_files>
            <path>tests/benchmarking/</path>
            <regex>accuracy|precision|recall</regex>
            <file_pattern>*.py</file_pattern>
          </search_files></code>
          <output>Matches in test_metrics.py for results section tables.</output>
        </example>
      </examples>
      <best_practices>
        <practice>Craft precise regex (Rust syntax) to avoid noise</practice>
        <practice>Analyze surrounding context for deeper insights</practice>
        <practice>Combine with read_file for full file context</practice>
      </best_practices>
    </tool>

    <tool name="read_file">
      <purpose>Retrieve exact content of files for evidence and quoting.</purpose>
      <when_to_use>After search/list to examine up to 5 critical files (e.g., performance_benchmarks.py).</when_to_use>
      <syntax>
        <command><read_file>
          <args>
            <file>
              <path>tests/benchmarks/performance_benchmarks.py</path>
            </file>
          </args>
        </read_file></command>
      </syntax>
      <examples>
        <example scenario="Analyze experiment code">
          <code><read_file>
            <args>
              <file><path>medusa_experiments/medusa_trainer.py</path></file>
              <file><path>learning/reinforcement_learning.py</path></file>
            </args>
          </read_file></code>
          <output>Line-numbered code for methodology description and pseudocode.</output>
        </example>
      </examples>
      <best_practices>
        <practice>Read related files together (up to 5) for efficiency</practice>
        <practice>Use line numbers for precise citations in paper</practice>
        <practice>Handle binary/non-text files carefully (focus on .py, .yaml)</practice>
      </best_practices>
    </tool>

    <tool name="execute_command">
      <purpose>Run repo scripts to generate or validate benchmark/experiment data.</purpose>
      <when_to_use>When static analysis insufficient; e.g., re-run tests for current results.</when_to_use>
      <syntax>
        <command><execute_command>
          <command>poetry run python scripts/run_gpt5_inloop_benchmark.py</command>
        </execute_command></command>
      </syntax>
      <examples>
        <example scenario="Validate benchmarks">
          <code><execute_command>
            <command>make test-all</command>
          </execute_command></code>
          <output>Coverage and results for reproducibility claims.</output>
        </example>
      </examples>
      <best_practices>
        <practice>Tailor to OS (Linux/bash here); use relative paths</practice>
        <practice>Explain command purpose in paper (e.g., "Executed via make test-all")</practice>
        <practice>Avoid long-running without user approval; monitor via environment_details</practice>
      </best_practices>
    </tool>

    <tool name="write_to_file">
      <purpose>Create new paper files or full drafts.</purpose>
      <when_to_use>Final output phase; provide complete content without line numbers.</when_to_use>
      <syntax>
        <command><write_to_file>
          <path>paper_benchmark_validation.md</path>
          <content>[Full Markdown paper]</content>
          <line_count>150</line_count>
        </write_to_file></command>
      </syntax>
      <examples>
        <example scenario="Output full paper">
          <code>[Use complete IMRaD structure from patterns]</code>
          <output>Generates editable .md file for user review/export to PDF.</output>
        </example>
      </examples>
      <best_practices>
        <practice>Include all sections; compute accurate line_count</practice>
        <practice>Use Markdown for versatility (convertible to LaTeX)</practice>
        <practice>Restrict to allowed patterns (.md, .tex) per mode groups</practice>
      </best_practices>
    </tool>

    <tool name="apply_diff">
      <purpose>Surgically revise existing paper drafts.</purpose>
      <when_to_use>Iterative improvements after user feedback.</when_to_use>
      <syntax>
        <command><apply_diff>
          <path>draft.md</path>
          <diff>
            <<<<<<< SEARCH
            :start_line:10
            -------
            [Old text]
            =======
            [Revised text]
            >>>>>>> REPLACE
          </diff>
        </apply_diff></command>
      </syntax>
      <best_practices>
        <practice>Read file first for exact matches</practice>
        <practice>Multiple blocks for efficiency</practice>
        <practice>Preserve academic formatting</practice>
      </best_practices>
    </tool>
  </tool_specific_guidance>

  <general_guidelines>
    <guideline>One tool per message; wait for results before proceeding</guideline>
    <guideline>Document tool outputs as evidence in paper (e.g., "As per search_files results...")</guideline>
    <guideline>Avoid edit tools until analysis complete; focus on read/command for validity</guideline>
    <guideline>If data gaps, use ask_followup_question for user input on external refs</guideline>
  </general_guidelines>
</tool_usage_guide>