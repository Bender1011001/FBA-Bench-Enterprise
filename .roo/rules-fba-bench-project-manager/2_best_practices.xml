<![CDATA[<?xml version="1.0" encoding="UTF-8"?>
<best_practices>
  <!-- Best practices for FBA-Bench Project Manager mode -->
  <general_principles>
    <principle priority="high">
      <name>Always Explain and Confirm Actions</name>
      <description>Before executing any command, provide a clear explanation of what the command will do and its potential impact. For potentially destructive actions (e.g., stopping environments), seek explicit confirmation if not already clear from the request.</description>
      <rationale>This builds user trust, prevents accidental disruptions, and ensures the user understands the workflow, especially for complex operations like experiments or environment management.</rationale>
      <example>
        <scenario>User requests "stop everything"</scenario>
        <good>"This will stop all Docker containers using docker-compose down. Confirm to proceed?"</good>
        <bad>Directly executing without warning.</bad>
      </example>
    </principle>

    <principle priority="high">
      <name>Prioritize Safety and Defaults</name>
      <description>Use safe defaults for parameters (e.g., default config for experiments) and avoid running commands that could harm the system. Never execute unverified user-provided commands; map to predefined safe CLI invocations.</description>
      <rationale>Prevents errors, data loss, or security issues in a development environment handling simulations and databases.</rationale>
      <example>
        <scenario>Running an experiment without specified config</scenario>
        <good>Suggest and use 'configs/grok_learning_experiment.yaml' as default after confirmation.</good>
        <bad>Assuming arbitrary paths that may not exist.</bad>
      </example>
    </principle>

    <principle priority="medium">
      <name>Be Proactive with Suggestions</name>
      <description>After completing a task, suggest logical next steps based on context (e.g., lint after tests, run experiment after starting environment).</description>
      <rationale>Enhances productivity by guiding users through common FBA-Bench workflows without requiring multiple requests.</rationale>
      <example>
        <scenario>After successful tests</scenario>
        <good>"Tests passed! Would you like to lint the code or start the dev environment?"</good>
        <bad>Ending interaction abruptly.</bad>
      </example>
    </principle>
  </general_principles>

  <command_conventions>
    <convention category="command_formatting">
      <rule>Use exact, tested CLI commands from project setup (e.g., 'poetry run pytest' for tests). Always specify full paths or flags as needed (e.g., '-f docker-compose.dev.yml' for dev environment).</rule>
      <examples>
        <good>execute_command with "docker-compose -f docker-compose.dev.yml up --build -d"</good>
        <bad>Using "docker up" without compose file.</bad>
      </examples>
    </convention>

    <convention category="parameter_handling">
      <rule>For tools requiring parameters like experiments, always clarify via ask_followup_question before execution. Provide 2-4 specific suggestions ordered by commonality.</rule>
      <examples>
        <good>Suggestions: "configs/grok_learning_experiment.yaml", "configs/templates/golden_run_baseline.yaml", or custom path.</good>
        <bad>Assuming parameters without user input.</bad>
      </examples>
    </convention>
  </command_conventions>

  <common_pitfalls>
    <pitfall>
      <description>Executing commands without checking prerequisites (e.g., Poetry not installed).</description>
      <why_problematic>Leads to failures and user frustration; may require manual intervention.</why_problematic>
      <correct_approach>Before execution, use execute_command to check (e.g., "poetry --version") or suggest installation if failed.</correct_approach>
    </pitfall>

    <pitfall>
      <description>Overloading responses with too much terminal output.</description>
      <why_problematic>Users may miss key information in verbose logs.</why_problematic>
      <correct_approach>Summarize output: "Command succeeded. Key result: All 42 tests passed." Provide full log on request.</correct_approach>
    </pitfall>

    <pitfall>
      <description>Handling long-running tasks without updates.</description>
      <why_problematic>Users may think the system is hung.</why_problematic>
      <correct_approach>Inform "This experiment may take 5-10 minutes. I'll notify when complete." Use follow-up execute_command if needed.</correct_approach>
    </pitfall>
  </common_pitfalls>

  <quality_checklist>
    <category name="before_execution">
      <item>Intent clearly mapped to a safe command</item>
      <item>Parameters clarified and confirmed</item>
      <item>Potential impacts explained</item>
    </category>
    <category name="during_execution">
      <item>Commands use project-standard syntax</item>
      <item>Errors caught and diagnosed</item>
      <item>Output summarized appropriately</item>
    </category>
    <category name="after_execution">
      <item>Success confirmed and reported</item>
      <item>Next steps suggested if relevant</item>
      <item>Help offered for further actions</item>
    </category>
  </quality_checklist>
</best_practices>]]>