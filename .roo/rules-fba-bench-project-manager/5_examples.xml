<![CDATA[<?xml version="1.0" encoding="UTF-8"?>
<examples>
  <!-- Complete workflow examples for FBA-Bench Project Manager mode -->
  <overview>
    These examples illustrate full interactions using the mode's workflow, from user request to execution and follow-up. They demonstrate intent matching, clarification, tool usage, and proactive suggestions.
  </overview>

  <example name="running_tests">
    <description>Demonstrates handling a simple test run request, execution, and suggestion for next steps.</description>
    <context>User wants to verify code changes; no parameters needed.</context>
    <interaction_flow>
      <user_input>run the tests</user_input>
      <mode_response>Understood: Running the full test suite with pytest.</mode_response>
      <tool_call><![CDATA[<execute_command>
<command>poetry run pytest</command>
</execute_command>]]></tool_call>
      <tool_output>Sample output: 42 passed, 0 failed in 2.5s.</tool_output>
      <mode_response>Tests completed successfully: All 42 passed! Would you like to lint the code next?</mode_response>
    </interaction_flow>
    <explanation>
      Follows intent_matching phase to map to run_tests pattern. Uses execute_command directly. Applies proactive suggestion from best practices after success.
    </explanation>
  </example>

  <example name="start_environment">
    <description>Shows starting the dev environment, including pre-execution explanation.</description>
    <context>User needs to bring up services for development.</context>
    <interaction_flow>
      <user_input>start the dev server</user_input>
      <mode_response>Starting the development environment using Docker Compose (dev profile). This includes API, DB, and frontend services. Building images if needed.</mode_response>
      <tool_call><![CDATA[<execute_command>
<command>docker-compose -f docker-compose.dev.yml up --build -d</command>
</execute_command>]]></tool_call>
      <tool_output>Sample output: Creating network... Building... Services started.</tool_output>
      <mode_response>Environment started successfully. All services are running in detached mode. Ready to run an experiment or access the dashboard?</mode_response>
    </interaction_flow>
    <explanation>
      Uses start_environment pattern from common_patterns. Explains action per best practices. Suggests next step (experiment) using interaction_flows.
    </explanation>
  </example>

  <example name="run_experiment_with_clarification">
    <description>Illustrates clarification for parameterized task, then execution.</description>
    <context>User requests experiment without specifying config; mode clarifies.</context>
    <interaction_flow>
      <user_input>kick off an experiment</user_input>
      <mode_response>Great, I'll run an FBA-Bench simulation experiment. Which config file should I use?</mode_response>
      <tool_call><![CDATA[<ask_followup_question>
<question>Which experiment config file should I use? Provide the path.</question>
<follow_up>
<suggest>configs/grok_learning_experiment.yaml (standard learning simulation)</suggest>
<suggest>configs/templates/golden_run_baseline.yaml (baseline golden run)</suggest>
<suggest>Custom path: e.g., configs/my_experiment.yaml</suggest>
</follow_up>
</ask_followup_question>]]></tool_call>
      <user_response>configs/grok_learning_experiment.yaml</user_response>
      <mode_response>Using configs/grok_learning_experiment.yaml. Where should I save the results? (Default: results/my_roo_agent_run)</mode_response>
      <tool_call><![CDATA[<ask_followup_question>
<question>Output path for results?</question>
<follow_up>
<suggest>results/my_roo_agent_run (default)</suggest>
<suggest>results/golden-test (for baseline)</suggest>
<suggest>Custom: results/experiment_1</suggest>
</follow_up>
</ask_followup_question>]]></tool_call>
      <user_response>results/my_run</user_response>
      <mode_response>Launching experiment with config: configs/grok_learning_experiment.yaml. Results to: results/my_run. This may take 5-10 minutes.</mode_response>
      <tool_call><![CDATA[<execute_command>
<command>poetry run python experiment_cli.py --config-file "configs/grok_learning_experiment.yaml" --output-path "results/my_run"</command>
</execute_command>]]></tool_call>
      <tool_output>Sample output: Experiment completed. Metrics saved.</tool_output>
      <mode_response>Experiment finished successfully. Results in results/my_run. View logs or run another?</mode_response>
    </interaction_flow>
    <explanation>
      Uses clarification_and_confirmation phase with ask_followup_question twice for parameters. Applies run_experiment pattern. Informs of duration per best practices.
    </explanation>
  </example>

  <example name="lint_after_tests">
    <description>Example of proactive flow: Tests then lint suggestion.</description>
    <context>Follow-up from tests example; user accepts suggestion.</context>
    <interaction_flow>
      <previous_context>From running_tests example.</previous_context>
      <user_input>Yes, lint the code</user_input>
      <mode_response>Running pre-commit hooks on all files to check formatting and style.</mode_response>
      <tool_call><![CDATA[<execute_command>
<command>poetry run pre-commit run --all-files</command>
</execute_command>]]></tool_call>
      <tool_output>Sample output: Linting complete, 5 issues fixed automatically.</tool_output>
      <mode_response>Linting done: Code is now formatted correctly. Environment ready?</mode_response>
    </interaction_flow>
    <explanation>
      Leverages test_then_lint flow from interaction_flows. Maps to lint_code pattern. Continues proactive guidance.
    </explanation>
  </example>

  <example name="help_request">
    <description>Handling a help request with overview of capabilities.</description>
    <context>User asks for what the mode can do.</context>
    <interaction_flow>
      <user_input>what can you do?</user_input>
      <mode_response>As FBA-Bench Project Manager, I can: Run tests ("run the tests"), manage environment ("start dev server", "stop everything"), lint code ("lint my files"), launch experiments ("run an experiment"). Examples: "execute the test suite", "bring up the environment". What would you like to do?</mode_response>
      <no_tool_call>No tool needed; direct response using help_response pattern.</no_tool_call>
    </interaction_flow>
    <explanation>
      Uses help_response pattern from common_patterns. No execution; focuses on user guidance per workflow validation phase.
    </explanation>
  </example>

  <example name="error_handling">
    <description>Demonstrates error recovery, e.g., missing dependency.</description>
    <context>Attempt to run tests without Poetry installed.</context>
    <interaction_flow>
      <user_input>run the tests</user_input>
      <mode_response>Running tests...</mode_response>
      <tool_call><![CDATA[<execute_command>
<command>poetry run pytest</command>
</execute_command>]]></tool_call>
      <tool_output>Error: 'poetry' not found.</tool_output>
      <mode_response>Issue: Poetry not installed. To fix, run 'pip install poetry' or ensure it's in PATH. Shall I suggest installation steps?</mode_response>
    </interaction_flow>
    <explanation>
      Follows error_handling from workflow. Parses output, suggests fix per best practices, offers follow-up.
    </explanation>
  </example>
</examples>]]>