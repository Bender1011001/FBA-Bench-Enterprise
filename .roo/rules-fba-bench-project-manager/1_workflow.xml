<![CDATA[<?xml version="1.0" encoding="UTF-8"?>
<workflow_instructions>
  <!-- Main workflow for FBA-Bench Project Manager mode -->
  <mode_overview>
    This mode provides an intelligent CLI interface for managing the FBA-Bench project. It translates natural language requests into precise CLI commands using the execute_command tool, handles common tasks like running tests, managing environments, linting, and experiments. The workflow focuses on intent recognition, clarification, execution, and proactive suggestions without editing files.
  </mode_overview>

  <prerequisites>
    <prerequisite>Project is set up with Poetry and Docker Compose available in the environment.</prerequisite>
    <prerequisite>User requests are in natural language related to FBA-Bench operations.</prerequisite>
    <prerequisite>Access to execute_command for running CLI tools like pytest, docker-compose, pre-commit.</prerequisite>
  </prerequisites>

  <initialization_steps>
    <step number="1">
      <title>Understand User Request</title>
      <description>Parse the incoming user message to identify the primary intent.</description>
      <actions>
        <action>Classify the request into categories: tests, environment management, linting, experiments, help, or other.</action>
        <action>Extract any parameters (e.g., config file paths for experiments).</action>
        <action>If intent is unclear or parameters missing, prepare to use ask_followup_question.</action>
      </actions>
      <validation>Intent matches one of the supported tasks; if not, suggest switching modes or clarify.</validation>
    </step>

    <step number="2">
      <title>Gather Context if Needed</title>
      <description>Check project state or files if required for the task.</description>
      <tools>
        <tool>read_file - To check config files or logs if referenced (e.g., verify experiment config exists).</tool>
        <tool>list_files - To confirm presence of docker-compose files or experiment configs.</tool>
        <tool>execute_command - To check running processes (e.g., 'docker ps' before starting environment).</tool>
      </tools>
      <actions>
        <action>Use read or list tools sparingly; prefer direct execution for most tasks.</action>
      </actions>
      <validation>Context confirms feasibility; flag issues like missing dependencies.</validation>
    </step>
  </initialization_steps>

  <main_workflow>
    <phase name="intent_matching">
      <description>Map user intent to specific actions or commands.</description>
      <steps>
        <step>Match to predefined commands:
          - Tests: 'poetry run pytest'
          - Start env: 'docker-compose -f docker-compose.dev.yml up --build -d'
          - Stop env: 'docker-compose -f docker-compose.dev.yml down'
          - Lint: 'poetry run pre-commit run --all-files'
          - Experiment: 'poetry run python experiment_cli.py --config-file {config} --output-path {output}' (after clarification)
        </step>
        <step>If experiment, always use ask_followup_question to confirm config_file and output_path, suggesting defaults like 'configs/grok_learning_experiment.yaml' and 'results/my_run'.</step>
      </steps>
    </phase>

    <phase name="clarification_and_confirmation">
      <description>Handle ambiguities and confirm before execution.</description>
      <steps>
        <step>If parameters missing (e.g., for experiments), use ask_followup_question with 2-4 suggestions.</step>
        <step>Explain the planned action: e.g., "I'm about to run the full test suite using pytest."</step>
        <step>Be proactive: After tests, suggest "Would you like to lint the code next?"</step>
      </steps>
    </phase>

    <phase name="execution">
      <description>Perform the action using appropriate tools.</description>
      <steps>
        <step>Use execute_command with the mapped CLI command; set cwd to project root if needed.</step>
        <step>For long-running tasks (e.g., experiments), inform user and monitor via follow-up commands if possible.</step>
        <step>Handle errors: If command fails, read output and suggest fixes (e.g., install dependencies).</step>
      </steps>
    </phase>

    <phase name="validation_and_followup">
      <description>Confirm success and offer next steps.</description>
      <steps>
        <step>Parse command output to confirm success (e.g., "All tests passed" or "Containers started").</step>
        <step>Report results briefly: e.g., "Tests completed successfully with 100% pass rate."</step>
        <step>Suggest related actions: e.g., after starting env, "Ready to run an experiment?"</step>
        <step>If help requested, list all supported commands with examples.</step>
      </steps>
    </phase>
  </main_workflow>

  <completion_criteria>
    <criterion>User request fully addressed with successful command execution.</criterion>
    <criterion>Clear confirmation provided, with no unresolved errors.</criterion>
    <criterion>Proactive suggestions offered if applicable.</criterion>
    <criterion>No file modifications attempted; all actions via CLI tools.</criterion>
  </completion_criteria>

  <error_handling>
    <scenario name="command_failure">
      <action>Reread output, diagnose (e.g., missing Poetry install), suggest fix via execute_command.</action>
    </scenario>
    <scenario name="ambiguous_request">
      <action>Use ask_followup_question to clarify, with suggestions like "Do you mean run unit tests or integration tests?"</action>
    </scenario>
  </error_handling>
</workflow_instructions>]]>