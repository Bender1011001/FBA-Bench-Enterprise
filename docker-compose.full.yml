version: "3.9"

services:
  postgres:
    image: postgres:15-alpine
    container_name: fba-postgres-full
    env_file: .env
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-fba}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-ChangeMe_Postgres_#X9m!7Pa}
      POSTGRES_DB: ${POSTGRES_DB:-fba_bench}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
        reservations:
          cpus: "0.5"
          memory: 512M
    networks:
      - backend
    command: postgres -c shared_preload_libraries=pg_stat_statements -c log_statement=all

  redis:
    image: redis:7-alpine
    container_name: fba-redis-full
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-ChangeMe_Redis_#u9YxQn7pLz!} --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD:-ChangeMe_Redis_#u9YxQn7pLz!}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 256M
    networks:
      - backend

  app:
    build:
      context: .
      dockerfile: Dockerfile  # Assumes prod-optimized build with Nginx/API
    container_name: fba-app-full
    env_file: .env
    ports:
      - "80:80"
      - "5173:5173"  # Expose frontend if separate
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      otel-collector:
        condition: service_healthy
      clearml-apiserver:  # For experiment tracking
        condition: service_started
    volumes:
      - ./ssl:/etc/nginx/ssl:ro
      - ./frontend:/app/frontend  # Mount for dev-like frontend if needed
    command: ["/start.sh"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost/nginx-health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: always
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 1G
        reservations:
          cpus: "1.0"
          memory: 512M
    networks:
      - backend
      - frontend  # For ClearML UI

  # ClearML Services (merged from clearml.yml)
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.9
    container_name: clearml-elastic-full
    environment:
      ES_JAVA_OPTS: -Xms1g -Xmx1g -Dlog4j2.formatMsgNoLookups=true
      discovery.type: single-node
    volumes:
      - clearml_elastic_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    restart: unless-stopped
    networks:
      - backend
    deploy:
      resources:
        limits:
          memory: 2G

  mongo:
    image: mongo:4.4
    container_name: clearml-mongo-full
    volumes:
      - clearml_mongo_data:/data/db
    ports:
      - "27017:27017"
    restart: unless-stopped
    networks:
      - backend

  clearml-redis:
    image: redis:7-alpine
    container_name: clearml-redis-full
    volumes:
      - clearml_redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - backend

  clearml-apiserver:
    image: allegroai/clearml:latest
    container_name: clearml-apiserver-full
    command: apiserver
    volumes:
      - clearml_logs:/var/log/clearml
      - clearml_config:/opt/clearml/config
      - clearml_fileserver_data:/mnt/fileserver
    ports:
      - "8008:8008"
    environment:
      CLEARML_REDIS_SERVICE_HOST: clearml-redis
      CLEARML_MONGODB_SERVICE_HOST: mongo
      CLEARML_ELASTIC_SERVICE_HOST: elasticsearch
      CLEARML_CONFIG_DIR: /opt/clearml/config
    depends_on:
      - clearml-redis
      - mongo
      - elasticsearch
    restart: unless-stopped
    networks:
      - backend

  clearml-webserver:
    image: allegroai/clearml:latest
    container_name: clearml-webserver-full
    command: webserver
    ports:
      - "8080:80"
    environment:
      CLEARML_WEB_HOST: "http://localhost:8080"
      CLEARML_API_HOST: "http://clearml-apiserver:8008"
      CLEARML_FILES_HOST: "http://clearml-fileserver:8081"
    depends_on:
      - clearml-apiserver
    restart: unless-stopped
    networks:
      - backend
      - frontend

  clearml-fileserver:
    image: allegroai/clearml:latest
    container_name: clearml-fileserver-full
    command: fileserver
    volumes:
      - clearml_logs:/var/log/clearml
      - clearml_fileserver_data:/mnt/fileserver
      - clearml_config:/opt/clearml/config
    ports:
      - "8081:8081"
    environment:
      CLEARML_REDIS_SERVICE_HOST: clearml-redis
      CLEARML_MONGODB_SERVICE_HOST: mongo
      CLEARML_ELASTIC_SERVICE_HOST: elasticsearch
    depends_on:
      - clearml-redis
      - mongo
      - elasticsearch
    restart: unless-stopped
    networks:
      - backend

  # Observability (merged from prod/otel)
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.99.0
    container_name: otel-collector-full
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro  # Assume prod config
      - ./ssl:/etc/ssl:ro
    env_file: .env
    ports:
      - "4317:4317"
      - "4318:4318"
      - "8888:8888"
      - "8889:8889"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:13133/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: always
    depends_on:
      - app
      - postgres
      - redis
    networks:
      - observability
      - backend
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M

  prometheus:
    image: prom/prometheus:v2.51.0
    container_name: prometheus-full
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./config/prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    env_file: .env
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: always
    depends_on:
      - otel-collector
    networks:
      - observability
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G

  grafana:
    image: grafana/grafana:10.4.1
    container_name: grafana-full
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning:ro  # Assume exists
      - ./config/grafana/dashboards:/etc/grafana/dashboards:ro
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_SERVER_ROOT_URL=http://localhost:3000/grafana
    ports:
      - "3000:3000"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: always
    depends_on:
      - prometheus
    networks:
      - observability
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M

  alertmanager:
    image: prom/alertmanager:v0.27.0
    container_name: alertmanager-full
    volumes:
      - ./config/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro  # Assume exists
      - alertmanager_data:/alertmanager
    ports:
      - "9093:9093"
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093/alertmanager'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9093/#/status"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: always
    networks:
      - observability
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  # Jaeger for tracing (from otel-collector.yml)
  jaeger:
    image: jaegertracing/all-in-one:1.37.0
    container_name: jaeger-full
    ports:
      - "16686:16686"
      - "14250:14250"
      - "14268:14268"
      - "9411:9411"
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    healthcheck:
      test: ["CMD-SHELL", "curl --connect-timeout 5 -s http://localhost:16686 >/dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s
    restart: unless-stopped
    networks:
      - observability
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 256M

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:
  alertmanager_data:
  clearml_elastic_data:
  clearml_mongo_data:
  clearml_redis_data:
  clearml_logs:
  clearml_config:
  clearml_fileserver_data:

networks:
  backend:
    driver: bridge
  frontend:
    driver: bridge
  observability:
    driver: bridge