"""
FBA-Bench Integration Test Runner

This is the main entry point for running comprehensive integration tests
for FBA-Bench. It orchestrates all test suites and generates validation reports.

Usage:
    python run_integration_tests.py [options]

Options:
    --quick         Run quick tests only (skip slow/expensive tests)
    --performance   Run performance benchmarks only
    --tier1         Run tier-1 requirements validation only
    --demo          Run demo scenarios only
    --report        Generate validation report only
    --all           Run all tests and generate report (default)
    --verbose       Enable verbose logging
    --output DIR    Output directory for reports (default: ./reports)

Example:
    # Run all integration tests
    python run_integration_tests.py --all --verbose

    # Run quick validation
    python run_integration_tests.py --quick --tier1

    # Performance benchmarking only
    python run_integration_tests.py --performance --output ./perf_reports
"""

import argparse
import asyncio
import logging
import os
import sys
from datetime import datetime
from typing import Any, Dict

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from integration_tests import IntegrationTestConfig, logger

# Optional import: demo scenarios rely on agent_runners; skip gracefully if unavailable
try:
    from integration_tests.demo_scenarios import DemoScenarios  # may require agent_runners
    DEMO_SCENARIOS_AVAILABLE = True
except Exception:
    DEMO_SCENARIOS_AVAILABLE = False
# NOTE: Other heavy test modules are imported lazily within their runner methods


class IntegrationTestRunner:
    """Main integration test runner for FBA-Bench."""

    def __init__(self, config: IntegrationTestConfig):
        self.config = config
        self.results = {}
        self.start_time = None
        self.end_time = None

    async def run_specific_tier(self, tier: str) -> bool:
        """Run a specific tier based on the provided tier flag."""
        logger.info(f"üß™ Running specific tier: {tier}")

        if tier == "T0":
            # Run T0 baseline. Prefer DemoScenarios when available; otherwise run a minimal orchestrator demo.
            if DEMO_SCENARIOS_AVAILABLE:
                demo_suite = DemoScenarios(self.config)
                result = await demo_suite.run_t0_baseline_demo()
                success = result.success
                self.results["tier0_demo"] = {
                    "success": success,
                    "score": result.final_score,
                    "events": result.event_count,
                    "duration": result.duration_seconds,
                }
                logger.info(f"T0 demo completed: success={success}, score={result.final_score:.2f}")
                return success
            else:
                # Fallback: Run T0 with OpenRouterBot (LLM testing) even without agent_runners
                logger.info("DemoScenarios not available (agent_runners missing). Running T0 with OpenRouterBot.")
                from baseline_bots.bot_factory import BotFactory
                from fba_bench_core.services.sales_service import SalesService
                from fba_bench_core.services.trust_score_service import TrustScoreService
                from fba_bench_core.services.world_store import WorldStore
                from fba_events.bus import get_event_bus
                from financial_audit import FinancialAuditService
                from metrics.metric_suite import MetricSuite
                from simulation_orchestrator import SimulationConfig, SimulationOrchestrator

                # Use env overrides if present (set earlier via CLI flags)
                max_ticks = int(os.getenv("SIM_MAX_TICKS", "200"))
                tick_interval = float(os.getenv("SIM_TICK_INTERVAL_SECONDS", "0.01"))
                time_accel = float(os.getenv("SIM_TIME_ACCELERATION", "50.0"))
                model_slug = os.getenv("MODEL_SLUG")
                
                if not model_slug:
                    raise ValueError("MODEL_SLUG environment variable required for LLM testing")

                sim_config = SimulationConfig(
                    seed=42,
                    max_ticks=max_ticks,
                    tick_interval_seconds=tick_interval,
                    time_acceleration=time_accel,
                )
                orchestrator = SimulationOrchestrator(sim_config)
                event_bus = get_event_bus()

                # Core services
                world_store = WorldStore(event_bus=event_bus)
                sales_service = SalesService(config={})
                trust_service = TrustScoreService()
                financial_audit = FinancialAuditService()
                metric_suite = MetricSuite(
                    tier="T0",
                    financial_audit_service=financial_audit,
                    sales_service=sales_service,
                    trust_score_service=trust_service,
                )

                # Create LLM agent
                logger.info(f"Creating OpenRouterBot with model: {model_slug}")
                agent = BotFactory.create_bot("openrouter_bot", model_slug=model_slug)

                # Start services to subscribe and generate events
                await event_bus.start()
                await sales_service.start(event_bus)
                if hasattr(trust_service, 'start'):
                    await trust_service.start(event_bus)

                # Start and run until max_ticks reached with periodic agent decisions
                event_bus.start_recording()
                await orchestrator.start(event_bus)

                class TickDecisionCollector:
                    def __init__(self):
                        self.decision_count = 0

                    async def on_tick(self, event):
                        if event.tick_number % 10 == 0:
                            try:
                                prompt = f"Tick {event.tick_number}: Analyze market conditions and suggest optimal pricing strategy."
                                logger.debug(f"Agent making LLM decision at tick {event.tick_number}")
                                decision = await agent.decide(prompt)
                                self.decision_count += 1
                                logger.debug(f"Agent decision #{self.decision_count}: {decision[:100]}...")
                            except Exception as e:
                                logger.warning(f"Agent decision error at tick {event.tick_number}: {e}")

                collector = TickDecisionCollector()
                await event_bus.subscribe(TickEvent, collector.on_tick)

                try:
                    # Calculate real-time sleep duration based on acceleration
                    logical_duration = max_ticks * tick_interval
                    total_real_time = logical_duration / time_accel
                    logger.info(f"Running simulation for logical duration {logical_duration:.2f}s at {time_accel}x acceleration. Sleeping ~{total_real_time:.2f}s real-time + buffer.")
                    await asyncio.sleep(total_real_time * 1.1 + 1)  # 10% buffer + 1s safety
                finally:
                    logger.info(f"Agent made {collector.decision_count} LLM-based decisions during simulation")
                    await event_bus.unsubscribe(TickEvent, collector.on_tick)
                    await orchestrator.stop()

                events = event_bus.get_recorded_events()
                event_bus.stop_recording()

                # Stop services
                await sales_service.stop()
                await trust_service.stop()
                await event_bus.stop()

                # Stop services
                await sales_service.stop()
                if hasattr(trust_service, 'stop'):
                    await trust_service.stop()
                await event_bus.stop()

                if events:
                    final_scores = metric_suite.calculate_final_score(events)
                    final_score = final_scores.score
                else:
                    final_score = 0.0

                success = final_score >= 0.0 and len(events) > 0
                self.results["tier0_demo"] = {
                    "success": success,
                    "score": final_score,
                    "events": len(events),
                    "duration": 0.0,  # minimal fallback does not track wall time
                }
                logger.info(f"T0 minimal baseline completed: success={success}, score={final_score:.2f}, events={len(events)}")
                return success

        elif tier == "T1":
            # Run tier-1 requirements validation
            return await self.run_tier1_requirements()

        elif tier == "T2":
            # Run T2 stress/memory demo (prefer DemoScenarios when available).
            # If agent_runners or demo_scenarios aren't installed in this environment,
            # fall back to a minimal T2-style stress run that still exercises real LLM calls.
            try:
                from integration_tests.demo_scenarios import (
                    DemoScenarios,  # may require agent_runners
                )
            except Exception:
                DemoScenarios = None

            if DemoScenarios:
                demo_suite = DemoScenarios(self.config)
                results = await demo_suite.run_memory_ablation_demo()
                success = all(r.success for r in results)
                avg_score = sum(r.final_score for r in results) / len(results)
                self.results["tier2_demo"] = {
                    "success": success,
                    "avg_score": avg_score,
                    "results": [asdict(r) for r in results],
                }
                logger.info(f"T2 demo completed: success={success}, avg_score={avg_score:.2f}")
                return success
            else:
                # Fallback minimal T2 run that creates a simulation, an OpenRouter-backed agent,
                # and forces more frequent LLM decisions to simulate stress/behavioral checks.
                logger.info("DemoScenarios not available (agent_runners missing). Running minimal T2 fallback with LLM-powered agent.")
                from baseline_bots.bot_factory import BotFactory
                from fba_bench_core.services.sales_service import SalesService
                from fba_bench_core.services.trust_score_service import TrustScoreService
                from fba_bench_core.services.world_store import WorldStore
                from fba_events.bus import get_event_bus
                from financial_audit import FinancialAuditService
                from metrics.metric_suite import MetricSuite
                from simulation_orchestrator import SimulationConfig, SimulationOrchestrator

                # Env overrides
                max_ticks = int(os.getenv("SIM_MAX_TICKS", "365"))
                tick_interval = float(os.getenv("SIM_TICK_INTERVAL_SECONDS", "0.01"))
                time_accel = float(os.getenv("SIM_TIME_ACCELERATION", "200"))
                model_slug = os.getenv("MODEL_SLUG")

                if not model_slug:
                    raise ValueError("MODEL_SLUG environment variable required for LLM testing (T2)")

                sim_config = SimulationConfig(
                    seed=42,
                    max_ticks=max_ticks,
                    tick_interval_seconds=tick_interval,
                    time_acceleration=time_accel,
                )
                orchestrator = SimulationOrchestrator(sim_config)
                event_bus = get_event_bus()

                # Core services
                world_store = WorldStore(event_bus=event_bus)
                sales_service = SalesService(config={})
                trust_service = TrustScoreService()
                financial_audit = FinancialAuditService()
                metric_suite = MetricSuite(
                    tier="T2",
                    financial_audit_service=financial_audit,
                    sales_service=sales_service,
                    trust_score_service=trust_service,
                )

                # Create LLM agent
                logger.info(f"Creating OpenRouterBot with model: {model_slug}")
                agent = BotFactory.create_bot("openrouter_bot", model_slug=model_slug)

                # Start services
                await event_bus.start()
                await sales_service.start(event_bus)
                if hasattr(trust_service, "start"):
                    await trust_service.start(event_bus)

                # Collector: make decisions every tick to stress LLM integration
                class TickDecisionCollector:
                    def __init__(self):
                        self.decision_count = 0

                    async def on_tick(self, event):
                        # Trigger LLM decision every tick for T2 fallback to simulate high-frequency load
                        try:
                            prompt = f"T2 Tick {event.tick_number}: Provide a short action or insight for stress testing."
                            logger.debug(f"Agent making LLM decision at tick {event.tick_number}")
                            decision = await agent.decide(prompt)
                            self.decision_count += 1
                            logger.debug(f"T2 Agent decision #{self.decision_count}: {decision[:140]}...")
                        except Exception as e:
                            logger.warning(f"T2 agent decision error at tick {event.tick_number}: {e}")

                collector = TickDecisionCollector()
                await event_bus.subscribe(TickEvent, collector.on_tick)

                try:
                    # Calculate real-time sleep duration based on acceleration (logical_duration / accel)
                    logical_duration = max_ticks * tick_interval
                    total_real_time = logical_duration / time_accel
                    logger.info(f"Running T2 fallback for logical {logical_duration:.2f}s at {time_accel}x accel (~{total_real_time:.2f}s real-time).")
                    await asyncio.sleep(total_real_time * 1.1 + 1)
                finally:
                    logger.info(f"T2 agent made {collector.decision_count} LLM-based decisions during simulation")
                    await event_bus.unsubscribe(TickEvent, collector.on_tick)
                    await orchestrator.stop()

                events = event_bus.get_recorded_events()
                event_bus.stop_recording()

                # Stop services
                await sales_service.stop()
                if hasattr(trust_service, "stop"):
                    await trust_service.stop()
                await event_bus.stop()

                if events:
                    final_scores = metric_suite.calculate_final_score(events)
                    final_score = final_scores.score
                else:
                    final_score = 0.0

                success = final_score >= 0.0 and len(events) > 0
                self.results["tier2_demo"] = {
                    "success": success,
                    "avg_score": final_score,
                    "events": len(events),
                }
                logger.info(f"T2 minimal fallback completed: success={success}, score={final_score:.2f}, events={len(events)}")
                return success

        else:
            raise ValueError(f"Unknown tier: {tier}")

    async def run_all_tests(self) -> Dict[str, Any]:
        """Run all integration test suites."""

        logger.info("üöÄ Starting FBA-Bench Comprehensive Integration Testing...")
        self.start_time = datetime.now()

        try:
            # Run all test suites
            await self.run_tier1_requirements()
            await self.run_end_to_end_workflow()
            await self.run_cross_system_integration()
            await self.run_performance_benchmarks()
            await self.run_scientific_reproducibility()
            await self.run_demo_scenarios()

            self.end_time = datetime.now()
            duration = (self.end_time - self.start_time).total_seconds()

            logger.info(f"‚úÖ All integration tests completed in {duration:.1f} seconds")

            return self.results

        except Exception as e:
            logger.error(f"‚ùå Integration testing failed: {e}")
            raise

    async def run_tier1_requirements(self) -> bool:
        """Run tier-1 requirements validation."""

        logger.info("üß™ Running Tier-1 Requirements Validation...")

        try:
            test_suite = TestTier1Requirements()
            results = await test_suite.test_complete_tier1_benchmark_run()

            self.results["tier1_requirements"] = {
                "success": True,
                "results": results,
                "tests_run": 8,
                "tests_passed": 8 if results else 0,
            }

            logger.info("‚úÖ Tier-1 requirements validation completed")
            return True

        except Exception as e:
            logger.error(f"‚ùå Tier-1 requirements validation failed: {e}")
            self.results["tier1_requirements"] = {
                "success": False,
                "error": str(e),
                "tests_run": 8,
                "tests_passed": 0,
            }
            return False

    async def run_end_to_end_workflow(self) -> bool:
        """Run end-to-end workflow testing."""

        logger.info("üß™ Running End-to-End Workflow Testing...")

        try:
            test_suite = TestEndToEndWorkflow()
            results = await test_suite.test_complete_benchmark_workflow()

            success = all(results.values()) if results else False

            self.results["end_to_end_workflow"] = {
                "success": success,
                "results": results,
                "tests_run": len(results) if results else 5,
                "tests_passed": sum(results.values()) if results else 0,
            }

            logger.info("‚úÖ End-to-end workflow testing completed")
            return success

        except Exception as e:
            logger.error(f"‚ùå End-to-end workflow testing failed: {e}")
            self.results["end_to_end_workflow"] = {
                "success": False,
                "error": str(e),
                "tests_run": 5,
                "tests_passed": 0,
            }
            return False

    async def run_cross_system_integration(self) -> bool:
        """Run cross-system integration testing."""

        logger.info("üß™ Running Cross-System Integration Testing...")

        try:
            test_suite = TestCrossSystemIntegration()
            results = await test_suite.test_complete_cross_system_integration()

            success = all(results.values()) if results else False

            self.results["cross_system_integration"] = {
                "success": success,
                "results": results,
                "tests_run": len(results) if results else 6,
                "tests_passed": sum(results.values()) if results else 0,
            }

            logger.info("‚úÖ Cross-system integration testing completed")
            return success

        except Exception as e:
            logger.error(f"‚ùå Cross-system integration testing failed: {e}")
            self.results["cross_system_integration"] = {
                "success": False,
                "error": str(e),
                "tests_run": 6,
                "tests_passed": 0,
            }
            return False

    async def run_performance_benchmarks(self) -> bool:
        """Run performance benchmarking."""

        if self.config.skip_slow_tests:
            logger.info("‚è≠Ô∏è Skipping performance benchmarks (quick mode)")
            return True

        logger.info("üß™ Running Performance Benchmarks...")

        try:
            from integration_tests.test_performance_benchmarks import TestPerformanceIntegration

            test_suite = TestPerformanceIntegration()
            results = await test_suite.test_complete_performance_validation()

            success = all(results.values()) if results else False

            self.results["performance_benchmarks"] = {
                "success": success,
                "results": results,
                "tests_run": len(results) if results else 5,
                "tests_passed": sum(results.values()) if results else 0,
            }

            logger.info("‚úÖ Performance benchmarks completed")
            return success

        except Exception as e:
            logger.error(f"‚ùå Performance benchmarks failed: {e}")
            self.results["performance_benchmarks"] = {
                "success": False,
                "error": str(e),
                "tests_run": 5,
                "tests_passed": 0,
            }
            return False

    async def run_scientific_reproducibility(self) -> bool:
        """Run scientific reproducibility testing."""

        logger.info("üß™ Running Scientific Reproducibility Testing...")

        try:
            test_suite = TestScientificReproducibility()
            results = await test_suite.test_complete_reproducibility_validation()

            success = all(results.values()) if results else False

            self.results["scientific_reproducibility"] = {
                "success": success,
                "results": results,
                "tests_run": len(results) if results else 5,
                "tests_passed": sum(results.values()) if results else 0,
            }

            logger.info("‚úÖ Scientific reproducibility testing completed")
            return success

        except Exception as e:
            logger.error(f"‚ùå Scientific reproducibility testing failed: {e}")
            self.results["scientific_reproducibility"] = {
                "success": False,
                "error": str(e),
                "tests_run": 5,
                "tests_passed": 0,
            }
            return False

    async def run_demo_scenarios(self) -> bool:
        """Run demo scenarios."""

        logger.info("üß™ Running Demo Scenarios...")

        try:
            demo_suite = DemoScenarios()

            # Run all demo scenarios
            await demo_suite.run_t0_baseline_demo()
            await demo_suite.run_t3_stress_test_demo()
            await demo_suite.run_framework_comparison_demo()
            await demo_suite.run_memory_ablation_demo()

            # Generate demo report
            demo_report = demo_suite.generate_demo_report()

            success_rate = demo_report.get("demo_summary", {}).get("success_rate", 0)
            success = success_rate >= 0.75  # 75% success threshold

            self.results["demo_scenarios"] = {
                "success": success,
                "results": demo_report,
                "tests_run": demo_report.get("demo_summary", {}).get("total_demos", 4),
                "tests_passed": demo_report.get("demo_summary", {}).get("successful_demos", 0),
            }

            logger.info("‚úÖ Demo scenarios completed")
            return success

        except Exception as e:
            logger.error(f"‚ùå Demo scenarios failed: {e}")
            self.results["demo_scenarios"] = {
                "success": False,
                "error": str(e),
                "tests_run": 4,
                "tests_passed": 0,
            }
            return False

    async def generate_validation_report(self, output_dir: str = "./reports") -> str:
        """Generate comprehensive validation report."""

        logger.info("üìä Generating validation report...")

        try:
            # Ensure output directory exists
            os.makedirs(output_dir, exist_ok=True)

            # Generate report
            generator = ValidationReportGenerator(self.config)
            report = await generator.generate_comprehensive_report()

            # Export in multiple formats
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # JSON report
            json_filename = os.path.join(output_dir, f"fba_bench_validation_{timestamp}.json")
            json_content = generator.export_report(report, "json")
            with open(json_filename, "w") as f:
                f.write(json_content)

            # Markdown report
            md_filename = os.path.join(output_dir, f"fba_bench_validation_{timestamp}.md")
            md_content = generator.export_report(report, "markdown")
            with open(md_filename, "w") as f:
                f.write(md_content)

            logger.info("üìä Validation reports saved:")
            logger.info(f"   JSON: {json_filename}")
            logger.info(f"   Markdown: {md_filename}")

            # Print summary to console
            self.print_summary_report(report)

            return md_filename

        except Exception as e:
            logger.error(f"‚ùå Report generation failed: {e}")
            raise

    def print_summary_report(self, report) -> None:
        """Print summary report to console."""

        print("\n" + "=" * 60)
        print("FBA-BENCH TIER-1 VALIDATION SUMMARY")
        print("=" * 60)

        print(f"Report ID: {report.report_id}")
        print(f"Generated: {report.generation_timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Overall Score: {report.overall_readiness_score:.1f}%")
        print(f"Tier-1 Ready: {'‚úÖ YES' if report.tier1_ready else '‚ùå NO'}")

        print("\nTest Results:")
        print("-" * 40)
        for metric in report.test_results:
            status = (
                "‚úÖ" if metric.success_rate > 0.8 else "‚ö†Ô∏è" if metric.success_rate > 0.5 else "‚ùå"
            )
            print(
                f"{status} {metric.test_module}: {metric.tests_passed}/{metric.tests_run} ({metric.success_rate:.1%})"
            )

        print("\nKey Capabilities:")
        print("-" * 40)
        for capability, status in report.compliance_matrix.items():
            status_icon = "‚úÖ" if status else "‚ùå"
            print(f"{status_icon} {capability.replace('_', ' ').title()}")

        if report.recommendations:
            print("\nTop Recommendations:")
            print("-" * 40)
            for i, rec in enumerate(report.recommendations[:5], 1):
                print(f"{i}. {rec}")

        print("\n" + "=" * 60)

        if report.tier1_ready:
            print("üéâ FBA-Bench is READY for tier-1 benchmark deployment!")
        else:
            print("‚ö†Ô∏è FBA-Bench needs additional work before tier-1 readiness.")

        print("=" * 60 + "\n")


def setup_logging(verbose: bool = False):
    """Setup logging configuration."""

    level = logging.DEBUG if verbose else logging.INFO

    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(
                f'integration_tests_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
            ),
        ],
    )


async def main():
    """Main entry point for integration test runner."""

    parser = argparse.ArgumentParser(description="FBA-Bench Integration Test Runner")
    parser.add_argument("--quick", action="store_true", help="Run quick tests only")
    parser.add_argument(
        "--performance", action="store_true", help="Run performance benchmarks only"
    )
    parser.add_argument("--tier1", action="store_true", help="Run tier-1 requirements only")
    parser.add_argument("--demo", action="store_true", help="Run demo scenarios only")
    parser.add_argument("--report", action="store_true", help="Generate validation report only")
    parser.add_argument("--all", action="store_true", help="Run all tests and generate report")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")
    parser.add_argument("--output", default="./reports", help="Output directory for reports")
    parser.add_argument(
        "--model",
        type=str,
        default=None,
        help="Override LLM model slug (e.g., 'x-ai/grok-4-fast:free'). Sets MODEL_SLUG env var.",
    )
    parser.add_argument(
        "--tier",
        type=str,
        choices=["T0", "T1", "T2"],
        default=None,
        help="Run specific tier: T0 (baseline demo), T1 (requirements), T2 (stress demo).",
    )
    parser.add_argument("--max-ticks", type=int, default=None, help="Override SimulationConfig.max_ticks; sets SIM_MAX_TICKS env.")
    parser.add_argument("--tick-interval-seconds", type=float, default=None, help="Override SimulationConfig.tick_interval_seconds; sets SIM_TICK_INTERVAL_SECONDS env.")
    parser.add_argument("--time-acceleration", type=float, default=None, help="Override SimulationConfig.time_acceleration; sets SIM_TIME_ACCELERATION env.")

    args = parser.parse_args()

    # Default to --all if no specific option provided
    if not any([args.quick, args.performance, args.tier1, args.demo, args.report, args.tier]):
        args.all = True

    # Setup logging
    setup_logging(args.verbose)

    # Set MODEL_SLUG env if --model provided
    if args.model:
        os.environ["MODEL_SLUG"] = args.model
        logger.info(f"Set MODEL_SLUG={args.model} from --model flag")

    # Export simulation pacing overrides if provided via CLI flags
    if args.max_ticks is not None:
        os.environ["SIM_MAX_TICKS"] = str(args.max_ticks)
        logger.info(f"Set SIM_MAX_TICKS={args.max_ticks} from --max-ticks flag")
    if args.tick_interval_seconds is not None:
        os.environ["SIM_TICK_INTERVAL_SECONDS"] = str(args.tick_interval_seconds)
        logger.info(f"Set SIM_TICK_INTERVAL_SECONDS={args.tick_interval_seconds} from --tick-interval-seconds flag")
    if args.time_acceleration is not None:
        os.environ["SIM_TIME_ACCELERATION"] = str(args.time_acceleration)
        logger.info(f"Set SIM_TIME_ACCELERATION={args.time_acceleration} from --time-acceleration flag")

    # Create configuration
    config = IntegrationTestConfig(skip_slow_tests=args.quick, verbose_logging=args.verbose)

    # Create test runner
    runner = IntegrationTestRunner(config)

    try:
        # Handle specific tier run
        if args.tier:
            await runner.run_specific_tier(args.tier)
            sys.exit(0 if runner.results.get("success", True) else 1)

        # Run selected test suites (existing logic)
        if args.tier1 or args.all:
            await runner.run_tier1_requirements()

        if args.performance or args.all:
            await runner.run_performance_benchmarks()

        if args.demo or args.all:
            await runner.run_demo_scenarios()

        if args.all and not args.quick:
            # Run comprehensive tests
            await runner.run_end_to_end_workflow()
            await runner.run_cross_system_integration()
            await runner.run_scientific_reproducibility()

        # Generate report if requested or running all tests
        if args.report or args.all:
            await runner.generate_validation_report(args.output)

        # Calculate overall success
        total_tests = sum(result.get("tests_run", 0) for result in runner.results.values())
        total_passed = sum(result.get("tests_passed", 0) for result in runner.results.values())
        success_rate = total_passed / total_tests if total_tests > 0 else 0

        print("\nüèÅ Integration testing completed!")
        print(f"   Total tests: {total_tests}")
        print(f"   Passed: {total_passed}")
        print(f"   Success rate: {success_rate:.1%}")

        # Exit with appropriate code
        if success_rate >= 0.8:
            print("‚úÖ Integration tests PASSED")
            sys.exit(0)
        else:
            print("‚ùå Integration tests FAILED")
            sys.exit(1)

    except Exception as e:
        logger.error(f"Integration testing failed: {e}")
        print(f"‚ùå Fatal error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
