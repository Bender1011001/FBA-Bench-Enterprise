Metadata-Version: 2.4
Name: fba-bench
Version: 3.0.0rc1
Summary: FBA-Bench: A pip-installable benchmarking and simulation framework for financial/business agents with ClearML integration and CLI tools
License: MIT
License-File: LICENSE
Keywords: agents,benchmarking,simulation,fastapi,research,reinforcement-learning,clearml,cli,mlops,pip-installable,experiments
Author: FBA-Bench Maintainers
Author-email: info@fba-bench.com
Requires-Python: >=3.9,<3.13
Classifier: Development Status :: 4 - Beta
Classifier: Environment :: Console
Classifier: Framework :: FastAPI
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Topic :: Scientific/Engineering
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Testing
Provides-Extra: clearml
Requires-Dist: PyJWT (>=2.10.1,<3.0.0)
Requires-Dist: SQLAlchemy (>=2.0.32,<3.0.0)
Requires-Dist: aiosqlite (>=0.20.0,<0.21.0)
Requires-Dist: alembic (>=1.12.0)
Requires-Dist: anthropic (>=0.34.0,<0.35.0)
Requires-Dist: anyio (>=4.4.0,<5.0.0)
Requires-Dist: clearml (>=1.16) ; extra == "clearml"
Requires-Dist: dependency-injector (>=4.45.0,<5.0.0)
Requires-Dist: fastapi (>=0.116.0,<0.117.0)
Requires-Dist: gunicorn (>=23.0.0,<24)
Requires-Dist: gymnasium (>=0.29.1,<0.30.0)
Requires-Dist: httpx (>=0.27.0,<0.28.0)
Requires-Dist: jsonschema (>=4.22.0,<5.0.0)
Requires-Dist: langchain (==0.3.27)
Requires-Dist: langchain-anthropic (>=0.2.4,<0.3.0)
Requires-Dist: langchain-community (>=0.3.27,<0.4.0)
Requires-Dist: langchain-openai (>=0.2.7,<0.3.0)
Requires-Dist: loguru (>=0.7.2,<0.8.0)
Requires-Dist: numpy (>=1.26.4,<2.0.0)
Requires-Dist: openai (>=1.60.0,<2.0.0)
Requires-Dist: opentelemetry-api (>=1.25.0,<2.0.0)
Requires-Dist: opentelemetry-exporter-otlp (>=1.25.0,<2.0.0)
Requires-Dist: opentelemetry-instrumentation-fastapi (>=0.46b0,<0.47)
Requires-Dist: opentelemetry-sdk (>=1.25.0,<2.0.0)
Requires-Dist: pandas (>=2.2.2,<3.0.0)
Requires-Dist: pydantic (>=2.8.0,<3.0.0)
Requires-Dist: pydantic-settings (>=2.5.0,<3.0.0)
Requires-Dist: python-dotenv (>=1.0.1,<2.0.0)
Requires-Dist: python-json-logger (>=3.2.1,<4.0.0)
Requires-Dist: pyyaml (>=6.0.1,<7.0.0)
Requires-Dist: redis (>=5.0.0)
Requires-Dist: requests (>=2.32.4,<3.0.0)
Requires-Dist: scikit-learn (>=1.5.0,<2.0.0)
Requires-Dist: scipy (>=1.12.0,<2.0.0)
Requires-Dist: slowapi (>=0.1.9,<0.2.0)
Requires-Dist: starlette (>=0.47.3,<0.48)
Requires-Dist: statsmodels (>=0.14.2,<0.15.0)
Requires-Dist: tenacity (>=9.0.0,<10.0.0)
Requires-Dist: typer (>=0.12.3,<0.13.0)
Requires-Dist: ujson (>=5.10.0,<6.0.0)
Requires-Dist: uvicorn[standard] (>=0.30.0,<0.31.0)
Project-URL: Documentation, https://docs.fba-bench.com
Project-URL: Homepage, https://github.com/fba-bench/fba-bench
Project-URL: Repository, https://github.com/fba-bench/fba-bench
Description-Content-Type: text/markdown

# FBA-Bench v3 Research Toolkit

FBA-Bench v3 is a powerful, deterministic, and scalable research toolkit designed for simulating complex financial and business scenarios, particularly in e-commerce environments (e.g., Fulfillment by Amazon - FBA). It provides a robust framework for developing, benchmarking, and analyzing the performance of AI agents and business strategies under various market conditions and adversarial events.

This toolkit emphasizes reproducibility, financial integrity, and a modular, event-driven architecture to support advanced research and rigorous analysis.

<a id="quickstart"></a>
## üöÄ Quickstart

Get started with FBA-Bench in under 2 minutes.

### Option 1: With Local ClearML Server (Recommended for first-time users)
Requirements: Docker Desktop/Engine must be running.

```bash
# Install FBA-Bench
pip install fba-bench

# Launch with local ClearML server (requires Docker)
fba-bench launch --with-server
```

This will:
- Start a local ClearML server stack via Docker using [docker-compose.clearml.yml](docker-compose.clearml.yml:1)
- Run your first FBA simulation via the orchestrator (CLI entry: [python.function main()](fba_bench/cli.py:490))
- Open the ClearML Web UI in your browser (default http://localhost:8080)
- Track all experiments automatically (integration point: [python.function SimulationOrchestrator._init_clearml_task()](simulation_orchestrator.py:557))

### Option 2: With ClearML Cloud

```bash
# Install FBA-Bench
pip install fba-bench

# Launch with ClearML cloud (requires account)
fba-bench launch
```

First-time setup: Create a free account at https://app.clear.ml and configure credentials when prompted (or run `clearml-init`, or set env variables as shown below). Experiments will appear under the ‚ÄúFBA-Bench‚Äù project.

## Table of Contents
- [üöÄ Quickstart](#quickstart)
- [üìö Documentation](#-documentation)
- [üõ†Ô∏è Installation and Setup](#-installation-and-setup)
  - [Pip Installation (Recommended)](#pip-installation-recommended)
  - [Prerequisites](#prerequisites)
  - [Local Services (Docker Compose)](#local-services-docker-compose)
  - [Manual Docker Build (API only)](#manual-docker-build-api-only)
  - [Backend Setup](#backend-setup)
  - [Environment Variables](#environment-variables)
- [CLI Usage](#cli-usage)
- [üèÉ‚Äç‚ôÄÔ∏è Usage](#-usage)
- [‚öôÔ∏è Configuration](#-configuration)
- [Golden Run Baseline](#golden-run-baseline)
- [Experiment Tracking UI Migration: ClearML](#experiment-tracking-ui-migration-clearml)
- [Realtime WebSockets (Redis pub/sub)](#realtime-websockets-redis-pubsub)
- [Infrastructure: Docker resource limits, health checks, and sizing guidance](#infrastructure-docker-resource-limits-health-checks-and-sizing-guidance)
- [ü§ù Contribution Guidelines](#-contribution-guidelines)
- [Migration: From development setup to pip package](#migration-from-development-setup-to-pip-package)
- [üìù License](#-license)

## üìö Documentation

For a guided new-user experience, start here:
- Quick Start Guide: [docs/getting-started/README.md](docs/getting-started/README.md)
- API Reference: [docs/api/README.md](docs/api/README.md)
- Troubleshooting: [docs/troubleshooting/README.md](docs/troubleshooting/README.md)
- Full docs index: [docs/README.md](docs/README.md)

Notes:
- In protected environments, API docs (/docs, /redoc, /openapi.json) may be gated when auth is enabled. See behavior in [python.function create_app()](fba_bench_api/server/app_factory.py:241).
## üöÄ Features and Capabilities

*   **Deterministic Simulation Engine**: Ensures reproducible results across multiple runs and environments, critical for scientific benchmarking.
*   **Event-Driven Architecture**: Utilizes a flexible `EventBus` (`event_bus.py`) for decoupled communication, supporting both in-memory and distributed (Redis-backed) event processing.
*   **Modular Agent System**:
    *   **Skills Framework**: Agents are composed of specialized `BaseSkill` modules (`agents/skill_modules/base_skill.py`) that handle domain-specific responsibilities (e.g., `FinancialAnalystSkill`, `SupplyManager`).
    *   **Skill Coordination**: `agents/skill_coordinator.py` manages event dispatch to skills, prioritizes actions, resolves conflicts, and tracks skill performance.
    *   **Strategic Control**: `agents/multi_domain_controller.py` provides a "CEO-level" arbitration layer, aligning agent actions with high-level business objectives and managing resource allocation.
*   **Plug-and-Play LLM Integration**: Seamlessly integrate with various Large Language Models (LLMs) (e.g., OpenAI, OpenRouter) via a standardized `llm_interface/` for advanced reasoning and decision-making within agents.
*   **Comprehensive Scenario Management**:
    *   Define complex business scenarios and market dynamics using flexible YAML configurations (`scenarios/scenario_framework.py`).
    *   Inject external events (e.g., supply disruptions, market shifts, adversarial attacks) to test agent resilience.
*   **Rigorous Financial Auditing**: `financial_audit.py` strictly validates all financial transactions against accounting principles, ensuring data integrity and halting simulations on critical violations.
*   **Benchmarking & Evaluation**: Robust framework ([`benchmarking/`](benchmarking/)) for defining evaluation metrics, running experiments, and analyzing agent performance.
*   **Precise Financial Modeling**: Utilizes the [python.file money.py](fba_bench/money.py:1) module for accurate and safe monetary calculations, preventing common floating-point errors.
*   **Scalable Architecture**: Supports distributed simulation execution leveraging a Redis-backed `DistributedEventBus` (`infrastructure/distributed_event_bus.py`) for multi-process/multi-node deployments.
*   **Experiment Tracking UI**:
    *   `fba_bench_api/` provides a FastAPI backend for simulation control, configuration management, and real-time data streaming (via WebSockets).
    *   ClearML Web UI serves as the primary interface for experiment setup, live monitoring, and analysis.
    *   A modern React dashboard is available under [directory frontend/](frontend:1) for optional local monitoring during development. It is decoupled from the Python package and does not affect pip installation. Install and run it only if desired:
        - Prerequisites: Node.js 18+ and npm 9+
        - Commands:
          ```bash
          cd frontend
          npm ci    # or: npm install
          npm start # dev server; proxies API via [json.key proxy](frontend/package.json:62)
          ```
*   **Toolbox API Abstraction**: `services/toolbox_api_service.py` provides a simplified interface for agents to interact with the simulation environment, decoupling agent logic from low-level event mechanisms.

## üèõÔ∏è Architecture Overview

FBA-Bench v3 employs a microservices-inspired, event-driven architecture designed for modularity, scalability, and maintainability.

```mermaid
graph TD
    subgraph ClearML_UI [Experiment Tracking (ClearML Web)]
        UI[ClearML Web UI]
    end

    subgraph Backend [FBA-Bench API (FastAPI)]
        FastAPI_API[RESTful API & WebSockets] -- Config/Control --> SimulationManager[Simulation Manager]
        FastAPI_API -- Data Query --> Persistence[Database / Persistence Layer]
        SimulationManager -- Orchestration --> EventBus
    end

    subgraph Simulation Core [Python Services & Agents]
        Orchestrator[SimulationOrchestrator] -- Tick Events --> EventBus
        EventBus --> FinancialAuditService[Financial Audit Service]
        EventBus --> AgentSystem[Agent System]
        EventBus --> ToolboxAPIService[Toolbox API Service]
        EventBus --> OtherServices[Other Simulation Services]

        subgraph Agent System
            Skill1[Skill Module 1 (e.g., Financial Analyst)]
            Skill2[Skill Module 2 (e.g., Marketing Manager)]
            SkillN[Skill Modules]
            SkillCoordinator[Skill Coordinator] -- Arbitrates & Prioritizes --> MultiDomainController[Multi-Domain Controller]
            MultiDomainController -- Approved Actions --> ToolboxAPIService
            SkillCoordinator --> Skill1 & Skill2 & SkillN
        end

        ToolboxAPIService -- Publish Commands/Events --> EventBus
        ToolboxAPIService -- Observe State (snapshots + updates) --> WorldState[World State cache (snapshots + incremental updates)]
        WorldState -- Updates --> EventBus
    end

    subgraph Experiment Execution [CLI]
        ExperimentCLI[experiment_cli.py] -- Run Scenarios --> ScenarioEngine[Scenario Engine]
        ScenarioEngine -- Initializes --> Orchestrator & AgentSystem & OtherServices
        ScenarioEngine -- Results --> ExperimentCLI
    end

    subgraph Distributed Infrastructure
        EventBus <--- DistributedEventBus[Distributed Event Bus (Redis, Kafka)]
        DistributedEventBus -- Worker Reg. & Load Bal. --> Workers[Multiple Simulation Workers]
    end

    subgraph LLM Integration
        AgentSystem -- LLM Calls --> LLM_Interface[LLM Interface (OpenAI/OpenRouter)]
        LLM_Interface -- API Requests --> ExternalLLM[External LLM Providers]
    end

    style Frontend fill:#f9f,stroke:#333,stroke-width:2px
    style Backend fill:#bbf,stroke:#333,stroke-width:2px
    style SimulationCore fill:#bfb,stroke:#333,stroke-width:2px
    style AgentSystem fill:#dfd,stroke:#333,stroke-width:2px
    style ExperimentExecution fill:#ffb,stroke:#333,stroke-width:2px
    style DistributedInfrastructure fill:#ccf,stroke:#333,stroke-width:2px
    style LLMIntegration fill:#fbc,stroke:#333,stroke-width:2px
```

**Key Interactions and Data Flow:**

1.  **Initialization**: The `experiment_cli.py` or the FastAPI application (`fba_bench_api/main.py`) initiates a simulation run, which involves the `ScenarioEngine` to load scenario configurations.
2.  **Simulation Loop**: `SimulationOrchestrator` acts as the clock, continuously emitting `TickEvent`s onto the `EventBus`.
3.  **Event Propagation**: The `EventBus` (potentially distributed via `RedisBroker` in `infrastructure/distributed_event_bus.py`) dispatches `TickEvent`s and other domain-specific `BaseEvent`s (e.g., `SaleOccurred`, `CompetitorPricesUpdated`) to all subscribed components.
4.  **Agent Decision Cycle**:
    *   `BaseSkill` modules within the Agent System receive relevant events.
    *   Skills process these events, analyze the `SkillContext` (current state, market data, etc.), possibly query LLMs via `llm_interface/`, and generate `SkillAction` proposals.
    *   The `SkillCoordinator` collects these `SkillAction`s, prioritizes them, and resolves potential conflicts.
    *   For sophisticated agents, the `MultiDomainController` applies high-level strategic alignment checks, business rules, and allocates resources before approving final actions.
    *   Approved actions are then translated into commands (e.g., `SetPriceCommand`) and sent to the `ToolboxAPIService` for execution.
5.  **World Interaction**: `ToolboxAPIService` both (a) publishes agent-approved commands to the `EventBus` (e.g., `SetPriceCommand`, inventory updates) and (b) observes simulation state via a hybrid model: periodic snapshot pulls and event-driven incremental updates. It maintains an in-process `WorldState` cache assembled from `WorldStateSnapshotEvent` snapshots plus subsequent delta events. Agents and connectors (e.g., `JupyterConnector`) can refresh snapshots or subscribe to updates; events are pushed (e.g., via WebSocket), while snapshots are pulled on demand.
6.  **State Management & Auditing**: Core simulation state is updated by various internal services (implied by events) and the `FinancialAuditService` rigorously checks all financial transactions for integrity.
7.  **Observability & UI**: The FastAPI backend exposes data via API endpoints (including real-time WebSockets) for internal dashboards, while the ClearML Web UI provides experiment setup, live monitoring, and analysis.

## üõ†Ô∏è Installation and Setup

### Pip Installation (Recommended)

For most users, the fastest path is to install the pip package and use the CLI:
```bash
pip install fba-bench

# Launch with ClearML Cloud
fba-bench launch

# Or launch with a local ClearML server (Docker required)
fba-bench launch --with-server
```

When to use pip vs development installation:
- Use pip if you want to run simulations and track results in ClearML with minimal setup.
- Use development installation if you plan to contribute code, run the API in edit/reload mode, or work across internal modules. See Backend Setup below.

Notes:
- The CLI entry point is [python.function main()](fba_bench/cli.py:490) exposed as `fba-bench`.
- Local server mode uses [docker-compose.clearml.yml](docker-compose.clearml.yml:1) automatically discovered via [python.function find_compose_file()](fba_bench/cli.py:123) and waits for services via [python.function wait_for_clearml_ready()](fba_bench/cli.py:164).
- ClearML experiment logging is automatic from the orchestrator ([python.function SimulationOrchestrator._init_clearml_task()](simulation_orchestrator.py:557)); disable by setting `CLEARML_AUTO_CONNECT=false`.


### Prerequisites

*   **Python 3.9+**: For the backend services and simulation core.
*   **Docker (Optional)**: For running Redis or other containerized services in a distributed setup.
*   **Redis (Optional)**: Required for Distributed Event Bus functionality.

### Local Services (Docker Compose)

- Backend API (development):
  ```bash
  docker compose up --build -d
  # API: http://localhost:8000
  ```

- ClearML Server (experiment tracking UI):
  ```bash
  docker compose -f docker-compose.clearml.yml up -d
  # Web UI: http://localhost:8080
  # API:    http://localhost:8008
  # Files:  http://localhost:8081
  ```
  Refer to the section "Experiment Tracking UI Migration: ClearML" below for credentials and usage.

  Note for pip users:
  - If you installed via pip, you generally do not need to run docker compose manually.
  - Prefer the CLI:
    ```bash
    fba-bench launch --with-server
    ```
    This will discover [docker-compose.clearml.yml](docker-compose.clearml.yml:1) via [python.function find_compose_file()](fba_bench/cli.py:123), launch the stack, wait for readiness ([python.function wait_for_clearml_ready()](fba_bench/cli.py:164)), and open the UI automatically.
### Manual Docker Build (API only)

For a standalone API container without the Nginx/SPA stack:

1. Ensure Docker is installed and running.
2. Build the backend API image using the API Dockerfile:
   ```bash
   docker build -f Dockerfile.api -t fba-bench-api .
   ```
   File: [Dockerfile.api](Dockerfile.api:1)

3. Run the API container:
   ```bash
   docker run -p 8000:8000 fba-bench-api
   ```
   The API will be accessible at http://localhost:8000.

### Backend Setup

1.  **Clone the repository**:
    ```bash
    git clone https://github.com/fba-bench/fba-bench.git
    cd fba-bench
    ```

2.  **Create and activate a Python virtual environment**:
    ```bash
    python -m venv .venv
    # On Windows
    .venv\Scripts\activate
    # On macOS/Linux
    source .venv/bin/activate
    ```

3.  **Install dependencies with Poetry (recommended)**:
    ```bash
    pip install -U pip setuptools wheel
    pip install poetry
    poetry install --with dev
    ```

4.  **Database Initialization (if applicable)**:
    If using a persistent database (e.g., PostgreSQL configured in `fba_bench_api/core/database.py`), ensure it's running and apply any necessary migrations. For a default SQLite setup, no explicit initialization might be needed beyond running the application.

5.  **Environment Variables**:
    Create a `.env` file in the root directory based on `.env.example`.
    ```ini
    # .env
    OPENROUTER_API_KEY="your_openrouter_api_key_here"
    OPENAI_API_KEY="your_openai_api_key_here"
    FBA_BENCH_REDIS_URL="redis://localhost:6379/0"  # Only needed for distributed mode
    FBA_BENCH_DB_URL="sqlite:///./fba_bench.db"     # Default SQLite, can be postgresql/mysql
    ```


<a id="cli-usage"></a>
## CLI Usage

The `fba-bench` command provides one-click launch workflows.

Commands:
- Show help:
  ```bash
  fba-bench
  fba-bench --help
  ```
- Launch using an existing ClearML setup (cloud or already-configured server):
  ```bash
  fba-bench launch
  ```
- Launch and bring up a local ClearML server via Docker Compose:
  ```bash
  fba-bench launch --with-server
  ```

Behavior:
- `--with-server` will:
  - Locate [docker-compose.clearml.yml](docker-compose.clearml.yml:1) using [python.function find_compose_file()](fba_bench/cli.py:123)
  - Start the stack using [python.function start_docker_compose()](fba_bench/cli.py:269)
  - Wait for ports (Web/UI, API, Files) using [python.function wait_for_clearml_ready()](fba_bench/cli.py:164)
  - Run the simulation orchestrator (see [python.function main()](fba_bench/cli.py:490) ‚Üí [python.function run_simulation_orchestrator()](fba_bench/cli.py:312))
  - Open the ClearML UI in your browser

Environment variables (optional):
- CLI behavior
  - `FBA_BENCH_LOG_LEVEL` (default INFO): Adjust CLI verbosity (DEBUG/INFO/WARN/ERROR).
  - `FBA_BENCH_ROOT`: Hint path to locate the repository root and compose file.
  - `FBA_BENCH_CLEARML_COMPOSE` (default docker-compose.clearml.yml): Override compose filename.
  - `FBA_BENCH_CLEARML_WEB_PORT` / `FBA_BENCH_CLEARML_API_PORT` / `FBA_BENCH_CLEARML_FILE_PORT`: Port overrides for readiness checks.
- ClearML auto-logging from the orchestrator
  - `CLEARML_AUTO_CONNECT=true|false` (default true): Disable to run without ClearML.
  - `CLEARML_PROJECT_NAME` (default "FBA-Bench"): Project in ClearML UI.
  - `CLEARML_TASK_NAME_PREFIX` (default "FBA-Simulation"): Task name prefix.
- ClearML connectivity (Cloud or self-hosted)
  - `CLEARML_API_SERVER`, `CLEARML_WEB_SERVER`, `CLEARML_FILES_SERVER` to point SDK at your server.
  - Credentials: `CLEARML_ACCESS_KEY`, `CLEARML_SECRET_KEY` or run `clearml-init` to generate `~/.clearml/clearml.conf`.
  - Local server defaults when using `--with-server`:
    - API: http://localhost:${FBA_BENCH_CLEARML_API_PORT:-8008}
    - Web: http://localhost:${FBA_BENCH_CLEARML_WEB_PORT:-8080}
    - Files: http://localhost:${FBA_BENCH_CLEARML_FILE_PORT:-8081}

Troubleshooting (CLI):
- "Docker is not installed or daemon not running"
  - Install Docker Desktop/Engine and ensure it is running. The CLI checks via [python.function check_docker_available()](fba_bench/cli.py:230).
- "Could not find 'docker-compose.clearml.yml'"
  - Set `FBA_BENCH_ROOT` to your project root, run from the repo directory, or provide a full path in `FBA_BENCH_CLEARML_COMPOSE`.
- "Ports in use"
  - Change port env overrides or stop the conflicting services; then retry the launch.
- "Browser failed to open"
  - Manually visit the ClearML UI URL printed by the CLI.
- "No experiments in UI"
  - Ensure ClearML credentials are configured for cloud (run `clearml-init`, or set env vars), or use `--with-server` to run locally.

## üèÉ‚Äç‚ôÄÔ∏è Usage

### Running the Backend API

From the root directory of the project, with your Python virtual environment activated:

```bash
poetry run uvicorn fba_bench_api.main:app --reload --port 8000
```
The API will be accessible at `http://localhost:8000`.


### Running CLI Experiments

From the root directory of the project, with your Python virtual environment activated:

1.  **Execute an experiment sweep**:
    ```bash
    python experiment_cli.py run config/templates/benchmark_basic.yaml --parallel 4
    ```
    This command runs the experiment defined in `benchmark_basic.yaml` using 4 parallel processes. Results will be saved in a timestamped directory under `results/`.

2.  **Analyze experiment results**:
    ```bash
    python experiment_cli.py analyze results/your_experiment_name_timestamp/
    ```
    Replace `results/your_experiment_name_timestamp/` with the actual path to your experiment results directory.

### Running Tests

The project includes unit, integration, and accessibility tests for both backend and frontend components.

#### Backend Tests

From the root directory:

```bash
pytest
```
To run specific test modules or apply filters, refer to the `pytest` documentation.



#### Legacy Frontend Tests

The legacy React UI has been retired in favor of the ClearML UI. Frontend‚Äìbackend end-to-end suites tied to that UI are explicitly skipped for compatibility. This is expected, not a failure. See module-level skips:
- [python.statement pytest.skip](tests/integration/test_frontend_backend_integration.py:3)
- [python.statement pytest.skip](tests/integration/test_end_to_end_scenarios.py:3)

## ‚öôÔ∏è Configuration

### Global Configuration

*   **`.env` files**: Manage API keys, database URLs, and other sensitive or environment-specific settings. Follow `.env.example` as a template.
*   **`config.yaml` / `configs/`**: Global application configurations, potentially including default parameters for simulations, agent behaviors, or infrastructure settings.

### Centralized Settings (single source of truth)

Runtime configuration is centralized in [python.file config.py](src/fba_bench_core/config.py:1) via Pydantic Settings. Precedence (lowest to highest):
1) Built-in defaults
2) YAML overlay pointed to by env `FBA_CONFIG_PATH`
3) Environment variables

Key fields:
- API/cors/auth/rate-limit
- Database/Redis URLs (preferred: `FBA_BENCH_DB_URL`/`FBA_BENCH_REDIS_URL`; compatibility: `DATABASE_URL`/`REDIS_URL`)
- Logging (level/format/file/tracebacks)
- CLI ClearML defaults (ports, compose filename, UI host)

Examples:
- YAML overlay schema:
  ```yaml
  environment: development
  api:
    rate_limit: "100/minute"
  cors:
    allow_origins: ["http://localhost:3000", "http://localhost:5173"]
  auth:
    enabled: false
    test_bypass: true
    protect_docs: false
    jwt_alg: "RS256"
    jwt_public_key: "-----BEGIN PUBLIC KEY-----..."
    jwt_issuer: "https://issuer.example"
    jwt_audience: "fba-bench"
    jwt_clock_skew: 60
  database:
    url: "sqlite:///./fba_bench.db"
  redis:
    url: "redis://localhost:6379/0"
  logging:
    level: "INFO"
    format: "text"   # or: "json"
    file: null
    include_tracebacks: true
  clearml:
    web_host: "https://app.clear.ml"
    local_ui: "http://localhost:8080"
    web_port: 8080
    api_port: 8008
    file_port: 8081
    compose_filename: "docker-compose.clearml.yml"
    root_hint: "./"
  ```
- Activate YAML overlay:
  ```bash
  export FBA_CONFIG_PATH=./config/example.yaml
  ```

For details, see [markdown.file docs/configuration.md](docs/configuration.md:1).

### Scenario Configuration

*   **`scenarios/`**: Defines the building blocks for simulation scenarios.
*   **`config/templates/` (e.g., `benchmark_basic.yaml`, `benchmark_advanced.yaml`)**: Examples of complete scenario configurations used by `experiment_cli.py`. These files define `scenario_name`, `difficulty_tier`, `expected_duration`, `success_criteria`, `market_conditions`, `external_events`, and `agent_constraints`.

### Agent Configuration

*   **`agents/`**: Contains various agent implementations. Agent-specific configurations are often defined within their respective modules or loaded from external YAML files. Parameters for LLM clients (models, temperatures) are configured in `llm_interface/` clients.

## ü§ù Contribution Guidelines

We welcome contributions to FBA-Bench! Please refer to `CONTRIBUTING.md` for detailed guidelines on how to set up your development environment, propose changes, and submit pull requests.

<a id="migration-from-development-setup-to-pip-package"></a>
## Migration: From development setup to pip package

This section helps existing users migrate from repository-based workflows to the new pip-installable package and CLI without losing any functionality.

What changed
- Primary entry: Install and launch via:
  ```bash
  pip install fba-bench
  fba-bench launch          # ClearML Cloud
  # or
  fba-bench launch --with-server   # Local ClearML via Docker
  ```
- ClearML experiment tracking is automatic in the orchestrator ([python.function SimulationOrchestrator._init_clearml_task()](simulation_orchestrator.py:557)).
- CLI opens the ClearML UI and optionally spins up the local ClearML stack ([python.function handle_launch()](fba_bench/cli.py:418), [python.function start_docker_compose()](fba_bench/cli.py:269), [python.function wait_for_clearml_ready()](fba_bench/cli.py:164)).

Development vs. pip workflows
- Pip workflow (recommended for most users)
  - Install: `pip install fba-bench`
  - Run: `fba-bench launch` or `fba-bench launch --with-server`
  - ClearML Cloud: configure credentials once (run `clearml-init`, or set `CLEARML_ACCESS_KEY`/`CLEARML_SECRET_KEY` and `CLEARML_API_SERVER`/`CLEARML_WEB_SERVER`/`CLEARML_FILES_SERVER`)
  - Local server: the CLI will launch [docker-compose.clearml.yml](docker-compose.clearml.yml:1) and wait for readiness
- Development workflow (for contributors)
  - Clone repo, create venv, install deps via Poetry, run API in reload mode:
    ```bash
    poetry install --with dev
    poetry run uvicorn fba_bench_api.main:app --reload --port 8000
    ```
  - Existing scripts remain supported (e.g., [python.file experiment_cli.py](experiment_cli.py:1)) and all unit/integration tests continue to work.

Backwards compatibility
- The repository structure, modules, and test suite were preserved. You can continue to:
  - Run scenario scripts and tests directly via `pytest`
  - Use the API and WebSocket endpoints as before
  - Start Docker stacks from compose files for advanced setups
- The new CLI simply streamlines common workflows without removing development options.

Suggested migration path
- Keep your development environment for contributions.
- For day-to-day simulations and experiment tracking:
  ```bash
  pip install fba-bench
  fba-bench launch --with-server   # recommended for first runs
  ```
- For ClearML Cloud:
  - Run `clearml-init` to generate ~/.clearml/clearml.conf
  - Or export:
    - PowerShell:
      ```powershell
      $env:CLEARML_API_SERVER="https://api.clear.ml"
      $env:CLEARML_WEB_SERVER="https://app.clear.ml"
      $env:CLEARML_FILES_SERVER="https://files.clear.ml"
      $env:CLEARML_ACCESS_KEY="..."
      $env:CLEARML_SECRET_KEY="..."
      ```
    - bash/zsh:
      ```bash
      export CLEARML_API_SERVER="https://api.clear.ml"
      export CLEARML_WEB_SERVER="https://app.clear.ml"
      export CLEARML_FILES_SERVER="https://files.clear.ml"
      export CLEARML_ACCESS_KEY="..."
      export CLEARML_SECRET_KEY="..."
      ```

Notes and knobs
- Disable auto ClearML logging by setting `CLEARML_AUTO_CONNECT=false`.
- Override compose filename via `FBA_BENCH_CLEARML_COMPOSE` (defaults to docker-compose.clearml.yml).
- CLI entry point: [python.function main()](fba_bench/cli.py:490); orchestrator execution path: [python.function run_simulation_orchestrator()](fba_bench/cli.py:312).

## üìù License

This project is licensed under the MIT License - see the `LICENSE` file for details.

---

## Standalone Scripts

The repository includes lightweight scripts useful for quick smoke tests and targeted checks:

- run_gemini_benchmark.py ‚Äî Runs a minimal Gemini Flash benchmark using the simplified engine wrapper.
  Usage:
    python run_gemini_benchmark.py --config benchmark_gemini_flash.yaml

- run_gpt5_benchmark.py ‚Äî Performs a simple GPT-5 API smoke test to verify connectivity and authentication without loading the full engine.
  Usage:
    # PowerShell (Windows)
    $Env:OPENAI_API_KEY="sk-..." ; python run_gpt5_benchmark.py
    # bash/zsh
    export OPENAI_API_KEY="sk-..." && python run_gpt5_benchmark.py

---
**FBA-Bench v3: Research Toolkit for Financial & Business Agent Benchmarking**
## Golden Run Baseline

A reproducible "Golden Run" baseline is established to benchmark future agents and strategies.

- Scenario: [scenarios/tier_1_moderate.yaml](scenarios/tier_1_moderate.yaml:1)
- Agent: [agents/baseline/baseline_agent_v1.py](agents/baseline/baseline_agent_v1.py:1)
- CLI: [experiment_cli.py](experiment_cli.py:1)
- Config: [config/templates/golden_run_baseline.yaml](config/templates/golden_run_baseline.yaml:1)
- Results directory (latest): golden_masters/golden_run_baseline/
  - To find the most recent run:
    - PowerShell (Windows):
      Get-ChildItem golden_masters/golden_run_baseline | Sort-Object LastWriteTime -Descending | Select-Object -First 1
    - bash/zsh (macOS/Linux):
      ls -1dt golden_masters/golden_run_baseline/* | head -n1
  - Inside the latest run directory, open summary.json and run_1.json for KPIs and run details.
  - The experiment_config.yaml stored alongside the run contains the exact parameters used.

How to reproduce the baseline:

- Single-threaded deterministic run:
  python experiment_cli.py run config/templates/golden_run_baseline.yaml --parallel 1 --seed 123 --tier T1

What this baseline shows (current KPIs as of last run):

- **Latest Results**: Check the most recent `summary.json` in `golden_masters/golden_run_baseline/` for current KPI values
- **Reproduction Command**: Run `python experiment_cli.py run config/templates/golden_run_baseline.yaml --parallel 1 --seed 123 --tier T1` to generate fresh results
- **Expected Performance**: All success criteria should be met (profit ‚â• $5,000, turnover ‚â• 4.0, satisfaction ‚â• 0.75, delivery ‚â• 0.90)
- Full details and rationale: [docs/research/golden-run-baseline.md](docs/research/golden-run-baseline.md:1)

This baseline is intended for comparative evaluation. Future agents should be run under the same scenario and seed to measure improvement or regression.
## Experiment Tracking UI Migration: ClearML

New in v3 (pip + CLI): Experiments are logged to ClearML automatically by the orchestrator ([python.function SimulationOrchestrator._init_clearml_task()](simulation_orchestrator.py:557)). For most users:
- Local server (recommended for first run): `pip install fba-bench && fba-bench launch --with-server`
- Cloud: `pip install fba-bench && fba-bench launch` and configure credentials when prompted (or via `clearml-init`/environment variables). Tasks appear under the ‚ÄúFBA-Bench‚Äù project in the ClearML UI.

This repository is instrumented to log experiments to ClearML. The ClearML Web UI replaces the legacy custom frontend for experiment setup, live monitoring, and analysis.

Key files:
- Compose stack: [docker-compose.clearml.yml](docker-compose.clearml.yml:1)
- SDK client config: [config/clearml.conf](config/clearml.conf:1)
- Runtime tracker wrapper (no-op if ClearML not installed): [instrumentation/clearml_tracking.py](instrumentation/clearml_tracking.py:1)
- Simulation integration (init/loop/finalize): [scenarios/scenario_engine.py](scenarios/scenario_engine.py:303)
- Dependency added (poetry): [pyproject.toml](pyproject.toml:47)

### 1) Start ClearML Server (local)
```bash
docker compose -f docker-compose.clearml.yml up -d
# Web UI: http://localhost:8080
# API:    http://localhost:8008
# Files:  http://localhost:8081
```

### 2) Configure SDK credentials
- Open the ClearML Web UI ‚Üí Profile (top-right) ‚Üí ‚ÄúWorkspace‚Äù ‚Üí ‚ÄúCreate new credentials‚Äù ‚Üí copy Access Key and Secret Key.
- Option A (recommended): set environment variables before running experiments:
  - On PowerShell (Windows):
    ```powershell
    $env:CLEARML_API_SERVER="http://localhost:8008"
    $env:CLEARML_WEB_SERVER="http://localhost:8080"
    $env:CLEARML_FILES_SERVER="http://localhost:8081"
    $env:CLEARML_ACCESS_KEY="YOUR_ACCESS_KEY"
    $env:CLEARML_SECRET_KEY="YOUR_SECRET_KEY"
    ```
  - On bash (macOS/Linux):
    ```bash
    export CLEARML_API_SERVER="http://localhost:8008"
    export CLEARML_WEB_SERVER="http://localhost:8080"
    export CLEARML_FILES_SERVER="http://localhost:8081"
    export CLEARML_ACCESS_KEY="YOUR_ACCESS_KEY"
    export CLEARML_SECRET_KEY="YOUR_SECRET_KEY"
    ```
- Option B: use the provided client config:
  - Copy [config/clearml.conf](config/clearml.conf:1) to `~/.clearml/clearml.conf` and update credentials, or
  - Set `CLEARML_CONFIG_FILE` to the repo path:
    - PowerShell:
      ```powershell
      $env:CLEARML_CONFIG_FILE="$(Get-Location)\config\clearml.conf"
      ```
    - bash:
      ```bash
      export CLEARML_CONFIG_FILE="$(pwd)/config/clearml.conf"
      ```

### 3) Install dependencies
Dependencies are managed via Poetry. ClearML integration is optional and not installed by default (see [pyproject.toml](pyproject.toml:103)). Install/update env:
```bash
pip install -U pip setuptools wheel
pip install poetry
poetry install --with dev
```

Optional: enable ClearML experiment tracking (optional dependency):
- For pip users (recommended):
```bash
pip install "fba-bench[clearml]"
```
- For development via Poetry (local env only):
```bash
poetry run pip install clearml
```

### 4) Run a scenario and view metrics
The simulation orchestrates and logs per-tick metrics via [scenarios/scenario_engine.py](scenarios/scenario_engine.py:303) using [instrumentation/clearml_tracking.py](instrumentation/clearml_tracking.py:1).

Quick ClearML smoke test (local run):
```bash
python scripts/smoke_clearml_run.py --scenario configs/clearml_smoketest.yaml --print-env
```

Enqueue the smoke test to a ClearML Agent queue from the CLI:
```bash
python scripts/smoke_clearml_run.py --scenario configs/clearml_smoketest.yaml --enqueue --queue default
```
This sets CLEARML_EXECUTE_REMOTELY=1 and hands execution to the Agent. The local process will exit after scheduling; the Agent re-runs the code with the same arguments. You can also control this behavior in any launcher by exporting:
- PowerShell (Windows):
  ```powershell
  $env:CLEARML_EXECUTE_REMOTELY="1"
  $env:CLEARML_QUEUE="default"
  ```
- bash/zsh:
  ```bash
  export CLEARML_EXECUTE_REMOTELY=1
  export CLEARML_QUEUE=default
  ```

Minimal GPT-5 connectivity test with ClearML logging:
```bash
# OPENAI_API_KEY must be set in the environment
python run_gpt5_benchmark.py
```
This script will create a ClearML Task "GPT5_SmokeTest" under project "FBA-Bench" and log basic connectivity status, content match, and raw response as an artifact when the ClearML SDK is configured.

- Example CLI (see existing CLI docs in this README):
```bash
# Example ‚Äì adapt to your scenario/config path used in your workflow
python experiment_cli.py run config/templates/benchmark_basic.yaml --parallel 1
```

- Or run your application flow that triggers ScenarioEngine. Metrics such as Profit (USD), MarketShare, OnTimeDeliveryRate, CustomerSatisfaction will appear under the task in ClearML UI, along with final run summary.

Open http://localhost:8080, navigate to the project ‚ÄúFBA-Bench‚Äù, and verify the new task appears with live scalars per tick.

### 5) Optional: Orchestration with ClearML Agent
If you want to enqueue/manage runs from the Web UI:
```bash
pip install clearml-agent
clearml-agent daemon --queue default
```
Then in the Web UI, enqueue a task to the ‚Äúdefault‚Äù queue.

### Notes
- The tracker is resilient: if ClearML isn‚Äôt installed or server is unreachable, [instrumentation/clearml_tracking.py](instrumentation/clearml_tracking.py:1) silently no-ops so simulations continue unaffected.
- To retire the legacy React UI, remove the `frontend/` workflow from your deployment and direct users to the ClearML UI. Compose and CI artifacts referencing the web SPA can be phased out after migration validation.

### 6) Optional: Enable ClearML Agent Services (Docker)

If you want to enqueue and execute tasks directly from the ClearML Web UI using the dockerized ClearML Agent Services container, enable credentials for the agent container. This is separate from the host SDK credentials above.

1) Create API credentials in the Web UI
- Open http://localhost:8080
- Top-right avatar ‚Üí Settings/Workspace ‚Üí API Access ‚Üí ‚ÄúCreate new credentials‚Äù
- Copy the Access Key and Secret Key

2) Provide credentials to the dockerized agent (recommended: .env file)
- Create [.env](.env) in the repo root (already gitignored by [.gitignore](.gitignore:123)) with:
  ```
  CLEARML_API_ACCESS_KEY=YOUR_ACCESS_KEY
  CLEARML_API_SECRET_KEY=YOUR_SECRET_KEY
  ```
- The Compose file [docker-compose.clearml.yml](docker-compose.clearml.yml:1) reads these at runtime and injects them into the agent-services container.

3) Start or restart the agent-services container
```bash
docker compose -f docker-compose.clearml.yml up -d agent-services
docker logs -f clearml-agent-services
```
Expected: No ‚ÄúCLEARML_API_ACCESS_KEY was not provided‚Äù errors. You should see ‚ÄúLaunching agent‚Äù.

4) Verify in the UI
- Web UI ‚Üí Workers & Queues ‚Üí the agent should appear online and subscribed to the default queue.

5) Alternative: set OS environment variables (instead of .env)
- On Windows (Command Prompt):
  ```
  setx CLEARML_API_ACCESS_KEY "YOUR_ACCESS_KEY"
  setx CLEARML_API_SECRET_KEY "YOUR_SECRET_KEY"
  ```
  Then open a new terminal and re-run:
  ```
  docker compose -f docker-compose.clearml.yml up -d agent-services
  ```
- On PowerShell:
  ```
  setx CLEARML_API_ACCESS_KEY "YOUR_ACCESS_KEY"
  setx CLEARML_API_SECRET_KEY "YOUR_SECRET_KEY"
  ```
  Open a new PowerShell and re-run the compose command above.
- On macOS/Linux (per-shell):
  ```bash
  export CLEARML_API_ACCESS_KEY="YOUR_ACCESS_KEY"
  export CLEARML_API_SECRET_KEY="YOUR_SECRET_KEY"
  docker compose -f docker-compose.clearml.yml up -d agent-services
  ```

Troubleshooting and notes
- If logs show ‚ÄúCLEARML_API_ACCESS_KEY was not provided, service will not be started‚Äù, ensure [.env](.env) exists with both keys or that your environment variables are defined in the terminal you‚Äôre using to run docker compose.
- Security: Rotate credentials periodically. Never commit secrets. [.env](.env) is already excluded by [.gitignore](.gitignore:123).
- Compose warning: If you see ‚Äúthe attribute version is obsolete‚Äù, you can remove the top-level version key from [docker-compose.clearml.yml](docker-compose.clearml.yml:1).
- Resource limits: For constrained hosts, you can pin Elasticsearch heap: set `ES_JAVA_OPTS="-Xms1g -Xmx1g"` in the [docker-compose.clearml.yml](docker-compose.clearml.yml:1).
- Smoke test: Use [scripts/smoke.sh](scripts/smoke.sh:1) to verify API/Files/Web endpoints quickly.

Disable agent-services (optional)
- If you do not need orchestration from the UI, comment out or remove the `agent-services` service in [docker-compose.clearml.yml](docker-compose.clearml.yml:1).
### Frontend Migration Developer Guide
For a step-by-step actionable migration (ClearML primary path, MLflow alternative, Aim/Grafana options), see:
- [docs/FRONTEND_MIGRATION_DEVELOPER_GUIDE.md](docs/FRONTEND_MIGRATION_DEVELOPER_GUIDE.md:1)

Quick checklist:
- [ ] Deploy ClearML or MLflow server
- [ ] Add SDK to backend (clearml or mlflow)
- [ ] Instrument simulation code with param/metric logging
- [ ] Verify metrics appear in UI
- [ ] Remove legacy React app
- [ ] Update docs and README
## Adversarial Testing Framework

The red-team subsystem provides a standardized way to define adversarial exploits, inject adversarial events into the simulation, and evaluate agent resilience.

Key components:
- Registry and validation: [redteam.exploit_registry.ExploitRegistry](redteam/exploit_registry.py:184), [redteam.exploit_registry.ExploitDefinition](redteam/exploit_registry.py:59)
- Injector API: [redteam.adversarial_event_injector.AdversarialEventInjector](redteam/adversarial_event_injector.py:1)
- Scoring: [redteam.resistance_scorer.AdversaryResistanceScorer](redteam/resistance_scorer.py:92), [redteam.resistance_scorer.ARSBreakdown](redteam/resistance_scorer.py:54)
- Events and responses: [fba_events.adversarial.PhishingEvent](fba_events/adversarial.py:143), [fba_events.adversarial.MarketManipulationEvent](fba_events/adversarial.py:185), [fba_events.adversarial.ComplianceTrapEvent](fba_events/adversarial.py:227), [fba_events.adversarial.AdversarialResponse](fba_events/adversarial.py:272)

Quick start:

```python
# Set up (inside an async test or runner)
registry = ExploitRegistry()
injector = AdversarialEventInjector(event_bus, registry)
scorer = AdversaryResistanceScorer()

# Register a phishing exploit (validation is permissive by default; see STRICT_EXPLOIT_VALIDATION below)
exploit = ExploitDefinition(
    name="Integration Test Phishing",
    author="test_system",
    version="1.0.0",
    category="phishing",
    difficulty=3,
    description="Integration test phishing exploit",
    exploit_type="phishing",
    context_requirements={
        "sender_email": "test@malicious.com",
        "message_content": "Urgent test action required",
        "requested_action": "provide_test_info",
    },
)
registry.register_exploit(exploit)

# Inject an event
event_id = await injector.inject_phishing_event(
    sender_email="test@malicious.com",
    message_content="Urgent test action required",
    requested_action="provide_test_info",
    difficulty_level=3,
)

# Record a response
await injector.record_agent_response(
    adversarial_event_id=event_id,
    agent_id="agent_1",
    fell_for_exploit=False,
    detected_attack=True,
    reported_attack=True,
    response_time_seconds=90.0,
)

# Score
responses = injector.get_responses_for_event(event_id)
ars_score, breakdown = scorer.calculate_ars(responses)
print("ARS:", ars_score, "resisted:", breakdown.exploits_resisted)
```

Notes:
- The injector supports multiple event types via:
  - `inject_phishing_event(...)`
  - `inject_market_manipulation_event(...)`
  - `inject_compliance_trap_event(...)`
- Stats and tracking:
  - `get_injection_stats()` ‚Üí totals and per-type counts
  - `get_responses_for_event(event_id)` ‚Üí recorded responses

### Registry validation: STRICT_EXPLOIT_VALIDATION

By default, registry validation is permissive for rapid iteration and testing. To enforce strict validation (reject missing context requirements or success conditions), set:

- Windows (PowerShell):
  ```powershell
  $env:STRICT_EXPLOIT_VALIDATION="1"
  ```
- macOS/Linux:
  ```bash
  export STRICT_EXPLOIT_VALIDATION=1
  ```

Strict validation applies in [redteam.exploit_registry.ExploitRegistry._validate_exploit()](redteam/exploit_registry.py:428).

### Money handling in AdversarialResponse

[fba_events.adversarial.AdversarialResponse.__post_init__()](fba_events/adversarial.py:303) accepts:
- Money instances
- Duck-typed Money-like objects (with `.cents: int`)
- Numeric values (interpreted as cents) which are coerced to Money

This ensures ARS scoring can compute financial damage penalties without precision loss.

## Local Tracing Adapter (OTel-compatible surface)

For environments without OpenTelemetry, a lightweight adapter is provided:

- `setup_tracing(service_name: str)` returns an object exposing `start_as_current_span(...)`
- Backed by a local [instrumentation.tracer.Tracer](instrumentation/tracer.py:1) implementation that satisfies unit tests and provides no-op-safe usage.

Example:

```python
from instrumentation.tracer import setup_tracing

tracer = setup_tracing(service_name="fba-bench-myservice")
with tracer.start_as_current_span("operation_name", attributes={"k": "v"}):
    # do work
    pass
```

This integrates cleanly with modules like [redteam.gauntlet_runner](redteam/gauntlet_runner.py:32) and [redteam.exploit_registry](redteam/exploit_registry.py:55), while allowing drop-in OpenTelemetry usage when available.

## Running the red-team test suite

Focused tests:
```bash
pytest -q tests/test_adversarial_framework.py -r fE --disable-warnings
```

Full suite:
```bash
pytest -q -r fE --disable-warnings
```

As of the hardening sprint, the adversarial framework tests are passing:
- 26 passed (see [tests/test_adversarial_framework.py](tests/test_adversarial_framework.py:1))
## Realtime WebSockets (Redis pub/sub)

The realtime WebSocket endpoint uses Redis pub/sub for topic-based messaging. The server implementation is in [fba_bench_api/api/routes/realtime.py](fba_bench_api/api/routes/realtime.py:1), specifically the handler [websocket_realtime(websocket)](fba_bench_api/api/routes/realtime.py:251). Redis connectivity is provided by [fba_bench_api/core/redis_client.py](fba_bench_api/core/redis_client.py:1) via [get_redis()](fba_bench_api/core/redis_client.py:35) and [get_pubsub()](fba_bench_api/core/redis_client.py:86).

- Endpoint:
  - ws://HOST:8000/ws/realtime
  - Legacy alias: ws://HOST:8000/ws/events ‚Üí delegates to the same handler

- Protocol (JSON frames):
  - Client ‚Üí Server:
    - {"type":"subscribe","topic":"X"}
    - {"type":"unsubscribe","topic":"X"}
    - {"type":"publish","topic":"X","data":{...}}
    - {"type":"ping"}
  - Server ‚Üí Client:
    - {"type":"event","topic":"X","data":{...},"ts":"..."}
    - {"type":"pong","ts":"..."}
    - {"type":"subscribed" | "unsubscribed", ...}
    - {"type":"warning","warning":"redis_unavailable",...} when running degraded without Redis
    - {"type":"error","error":"..."}

Environment configuration

- Preferred variable: FBA_BENCH_REDIS_URL
- Fallback supported: REDIS_URL
- URL examples:
  - Local (no TLS, passwordless): redis://127.0.0.1:6379/0
  - Local with password: redis://:PASSWORD@127.0.0.1:6379/0
  - Managed Redis with TLS (recommended for production): rediss://:PASSWORD@your-redis.example.com:6380/0
    - Username is supported if your provider requires it: rediss://USERNAME:PASSWORD@host:port/0

See [.env.example](.env.example:1) for commented examples and guidance. Production compose passes through the variable in [docker-compose.prod.yml](docker-compose.prod.yml:1).

Authentication

- If AUTH_ENABLED=true and a valid [AUTH_JWT_PUBLIC_KEY](fba_bench_api/server/app_factory.py:127) is configured, the WebSocket requires a JWT.
- Provide the JWT using one of:
  - Sec-WebSocket-Protocol subprotocol: "auth.bearer.token.&lt;JWT&gt;"
    - Example (browser): new WebSocket("wss://api.example.com/ws/realtime", ["auth.bearer.token."+jwt]);
  - Query parameter: wss://api.example.com/ws/realtime?token=&lt;JWT&gt;
- JWT verification options are derived from environment in [realtime.py](fba_bench_api/api/routes/realtime.py:52) and enforced in [websocket_realtime(websocket)](fba_bench_api/api/routes/realtime.py:268).

Local quickstart

- Easiest: docker compose -f [docker-compose.oneclick.yml](docker-compose.oneclick.yml:1) up -d
  - Starts Redis with requirepass and injects both FBA_BENCH_REDIS_URL and REDIS_URL into the API.
- Alternatively, run a local Redis and set:
  - PowerShell: $env:FBA_BENCH_REDIS_URL="redis://127.0.0.1:6379/0"
  - bash/zsh: export FBA_BENCH_REDIS_URL="redis://127.0.0.1:6379/0"

Production notes

- Use rediss:// with strong credentials.
- Pass FBA_BENCH_REDIS_URL via your orchestrator or compose. The API will connect via [Redis.from_url(...)](fba_bench_api/core/redis_client.py:44) with:
  - decode_responses=True (text JSON payloads)
  - health_check_interval=30
  - retry_on_timeout=True
  - Resilient initialization with exponential backoff and jitter on initial ping

Operational characteristics

- The server degrades gracefully if Redis is down/unconfigured:
  - Connection still accepts and supports ping/pong and acks, but publishes/subscribes return redis_unavailable warnings/errors.
  - See warning path in [websocket_realtime(websocket)](fba_bench_api/api/routes/realtime.py:371).
- Proper pub/sub lifecycle:
  - Subscribe/Unsubscribe map to Redis subscribe/unsubscribe; events forwarded from Redis to client in a background loop [listener loop](fba_bench_api/api/routes/realtime.py:324).
  - Publish writes to Redis using [get_redis()](fba_bench_api/core/redis_client.py:35) ‚Üí r.publish(topic, json.dumps(data)).

Health and verification

- HTTP health: GET /health (optionally checks Redis when CHECK_REDIS=1)
  - Logic in [app_factory.py](fba_bench_api/server/app_factory.py:358) picks the Redis URL from REDIS_URL ‚Üí FBA_BENCH_REDIS_URL ‚Üí FBA_REDIS_URL for the check.
- Integration tests: [tests/integration/test_websocket_realtime.py](tests/integration/test_websocket_realtime.py:1)
  - Skips if Redis is unavailable. Set REDIS_URL or FBA_BENCH_REDIS_URL to enable.
  - Validates subscribe, publish, broadcast, unsubscribe flow.

Client example (Python)

```python
import json, websockets, asyncio

async def main():
    jwt = "eyJ..."  # your signed JWT when auth is enabled
    protocols = [f"auth.bearer.token.{jwt}"]  # omit if AUTH is disabled
    async with websockets.connect("ws://localhost:8000/ws/realtime", subprotocols=protocols) as ws:
        # ack
        print(await ws.recv())
        # subscribe
        await ws.send(json.dumps({"type":"subscribe","topic":"demo"}))
        print(await ws.recv())  # subscribed
        # publish
        await ws.send(json.dumps({"type":"publish","topic":"demo","data":{"hello":"world"}}))
        # receive event
        print(await ws.recv())

asyncio.run(main())
```

This completes the production-ready configuration for Redis-backed realtime WebSockets.
## Infrastructure: Docker resource limits, health checks, and sizing guidance

This release adds consistent resource limits, health checks, and restart policies across all Compose stacks to prevent resource exhaustion and improve resilience.

Where to look:
- [docker-compose.yml](docker-compose.yml)
- [docker-compose.dev.yml](docker-compose.dev.yml)
- [docker-compose.prod.yml](docker-compose.prod.yml)
- [docker-compose.oneclick.yml](docker-compose.oneclick.yml)
- [docker-compose.clearml.yml](docker-compose.clearml.yml)
- [infrastructure/deployment/docker-compose.yml](infrastructure/deployment/docker-compose.yml)
- [docker-compose-otel-collector.yml](docker-compose-otel-collector.yml)

What we set (defaults per service type):
- API services (FastAPI fba-bench-api)
  - Limits: 1 CPU, 1 GiB memory
  - Reservations: 0.5 CPU, 512 MiB memory
  - Files:
    - [docker-compose.yml](docker-compose.yml)
    - [docker-compose.dev.yml](docker-compose.dev.yml)
    - [docker-compose.prod.yml](docker-compose.prod.yml)
    - [docker-compose.oneclick.yml](docker-compose.oneclick.yml)
    - [infrastructure/deployment/docker-compose.yml](infrastructure/deployment/docker-compose.yml) (fba-bench-app)
- Datastores
  - MongoDB: limits 0.75 CPU / 1 GiB; reserve 0.5 CPU / 512 MiB ([docker-compose.clearml.yml](docker-compose.clearml.yml))
  - Redis: limits 0.5 CPU / 1 GiB; reserve 0.25 CPU / 512 MiB ([docker-compose.oneclick.yml](docker-compose.oneclick.yml), [infrastructure/deployment/docker-compose.yml](infrastructure/deployment/docker-compose.yml), [docker-compose.clearml.yml](docker-compose.clearml.yml))
  - PostgreSQL: limits 0.75 CPU / 1 GiB; reserve 0.5 CPU / 512 MiB ([infrastructure/deployment/docker-compose.yml](infrastructure/deployment/docker-compose.yml))
  - Elasticsearch (ClearML): limits 1 CPU / 2 GiB; reserve 0.5 CPU / 1 GiB ([docker-compose.clearml.yml](docker-compose.clearml.yml))
- ClearML services
  - apiserver: limits 0.5 CPU / 768 MiB; reserve 0.25 CPU / 256 MiB
  - fileserver: limits 0.25 CPU / 512 MiB; reserve 0.10 CPU / 256 MiB
  - webserver: limits 0.25 CPU / 512 MiB; reserve 0.10 CPU / 256 MiB
  - Agents (optional): limits 0.5 CPU / 512 MiB; reserve 0.25 CPU / 256 MiB
  - File: [docker-compose.clearml.yml](docker-compose.clearml.yml)
- Observability and utilities
  - OpenTelemetry Collector: limits 0.5 CPU / 512 MiB; reserve 0.25 CPU / 256 MiB
  - Jaeger: limits 0.25 CPU / 256 MiB; reserve 0.10 CPU / 128 MiB
  - Prometheus: limits 0.25 CPU / 256 MiB; reserve 0.10 CPU / 128 MiB
  - Grafana: limits 0.5 CPU / 512 MiB; reserve 0.25 CPU / 256 MiB
  - File: [docker-compose-otel-collector.yml](docker-compose-otel-collector.yml)

Health checks and restart policy improvements:
- All critical services include healthcheck definitions and use restart: unless-stopped.
- Added/standardized health checks for:
  - API services: GET /api/v1/health
  - Redis: redis-cli ping (auth)
  - Mongo: mongosh ping
  - PostgreSQL: pg_isready
  - Elasticsearch: cluster health endpoint
  - ClearML web/api/files servers: HTTP liveness
  - Observability (otel, jaeger, prometheus, grafana): HTTP readiness where available
- These are defined in the corresponding Compose files listed above.

Adjusting resource limits:
- Edit the deploy.resources.limits and deploy.resources.reservations in the relevant Compose files.
- All memory values accept units like M (MiB) and G (GiB). CPU values are fractional strings (e.g., "0.50", "1.0").
- You can use environment variable substitution in YAML if you want to externalize sizing:
  - Example:
    - limits:
      - cpus: "${API_LIMIT_CPUS:-1.0}"
      - memory: "${API_LIMIT_MEM:-1G}"
- Note: Docker Compose v3 syntax puts CPU/memory limits under deploy.resources. On classic non-Swarm Compose, deploy may be partially ignored by the engine. If you need local enforcement, you can run with:
  - docker compose --compatibility up -d
  This flag translates some deploy settings to runtime flags for better local adherence.

Baseline sizing guidance:
- One-click API + Redis ([docker-compose.oneclick.yml](docker-compose.oneclick.yml))
  - Minimum: 2 CPU cores, 2 GiB RAM
  - Recommended: 2-4 CPU cores, 4 GiB RAM
- Development API ([docker-compose.yml](docker-compose.yml), [docker-compose.dev.yml](docker-compose.dev.yml))
  - Minimum: 2 CPU cores, 2 GiB RAM
  - Recommended: 4 CPU cores, 4-8 GiB RAM (for faster dependency installs and reloads)
- Production API only ([docker-compose.prod.yml](docker-compose.prod.yml))
  - Minimum: 2 CPU cores, 2 GiB RAM (external managed DB/Redis recommended)
  - Recommended: 2-4 CPU cores, 4 GiB RAM
- Full ClearML stack ([docker-compose.clearml.yml](docker-compose.clearml.yml))
  - Minimum: 4 CPU cores, 8 GiB RAM (Elasticsearch is memory-intensive)
  - Recommended: 8 CPU cores, 16 GiB RAM for heavy experiment loads
- Observability stack ([docker-compose-otel-collector.yml](docker-compose-otel-collector.yml))
  - Minimum: +1 CPU core, +1 GiB RAM on top of your base stack
  - Recommended: +2 CPU cores, +2 GiB RAM for active scraping and UI use

Operational tips:
- If a service becomes memory constrained, increase its memory limit first, then CPU as needed.
- For datastore-heavy scenarios (large ELK/Prometheus datasets), budget more RAM and storage IOPS.
- For high WebSocket or parallel simulation loads, increase API CPU to 2.0 and memory to 2-4 GiB.

Security notes:
- Redis and Mongo in the examples are password-protected; keep strong secrets in .env and never commit real credentials.
- For production, run managed services (e.g., hosted Redis/Postgres/Elasticsearch) with stronger sizing and HA.

This documentation reflects the current defaults implemented in the Compose files listed above. Keep your sizing aligned with your workload and host capacity.

