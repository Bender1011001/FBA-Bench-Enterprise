"""
API routes for benchmark operations.

This module provides REST API endpoints for creating, managing, and monitoring
benchmark executions in the FBA Benchmark system.
"""

import logging
from datetime import datetime, timezone
from typing import Any, Dict, List
from uuid import uuid4

from fastapi import APIRouter, BackgroundTasks, HTTPException, status
from fastapi.responses import JSONResponse

from ...core.models import BenchmarkConfig, BenchmarkCreateRequest
from ...core.models import BenchmarkResponse
from ...core.models import BenchmarkResponse as BenchmarkRunResponse
from ...core.models import BenchmarkResult, BenchmarkStatus, HealthResponse

# Import services (these would be implemented in a real system)
# from ...services.benchmark_service import BenchmarkService
# from ...services.experiment_service import ExperimentService
# from ...services.persistence_service import PersistenceService

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/benchmarks", tags=["benchmarks"])

# Placeholder for service dependencies
# In a real implementation, these would be injected via FastAPI's dependency system
# benchmark_service: BenchmarkService = Depends(get_benchmark_service)
# experiment_service: ExperimentService = Depends(get_experiment_service)
# persistence_service: PersistenceService = Depends(get_persistence_service)


class BenchmarkService:
    """Placeholder for the actual BenchmarkService implementation."""

    def __init__(self):
        self.active_benchmarks: Dict[str, Dict[str, Any]] = {}
        self.completed_benchmarks: Dict[str, Dict[str, Any]] = {}

    async def create_benchmark(self, config: BenchmarkConfig) -> str:
        """Create a new benchmark and return its ID."""
        benchmark_id = str(uuid4())
        self.active_benchmarks[benchmark_id] = {
            "id": benchmark_id,
            "config": config.dict(),
            "status": BenchmarkStatus.CREATED,
            "created_at": datetime.now(timezone.utc),
            "updated_at": datetime.now(timezone.utc),
        }
        return benchmark_id

    async def run_benchmark(self, benchmark_id: str) -> BenchmarkResult:
        """Run a benchmark and return its results."""
        if benchmark_id not in self.active_benchmarks:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND, detail=f"Benchmark {benchmark_id} not found"
            )

        benchmark = self.active_benchmarks[benchmark_id]
        benchmark["status"] = BenchmarkStatus.RUNNING
        benchmark["updated_at"] = datetime.now(timezone.utc)

        # Simulate benchmark execution
        # In a real implementation, this would orchestrate the actual benchmark run
        try:
            # Placeholder for actual benchmark execution logic
            result = BenchmarkResult(
                benchmark_id=benchmark_id,
                run_id=str(uuid4()),
                status=BenchmarkStatus.COMPLETED,
                config=benchmark["config"],
                scenario_results=[],
                start_time=benchmark["created_at"],
                end_time=datetime.now(timezone.utc),
                duration_seconds=30.0,  # Placeholder
                overall_score=0.85,  # Placeholder
            )

            # Move from active to completed
            self.completed_benchmarks[benchmark_id] = {
                **benchmark,
                "result": result.dict(),
                "completed_at": datetime.now(timezone.utc),
            }
            del self.active_benchmarks[benchmark_id]

            return result

        except Exception as e:
            benchmark["status"] = BenchmarkStatus.FAILED
            benchmark["updated_at"] = datetime.now(timezone.utc)
            benchmark["error"] = str(e)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Benchmark execution failed: {e!s}",
            )

    async def get_benchmark_status(self, benchmark_id: str) -> Dict[str, Any]:
        """Get the status of a benchmark."""
        if benchmark_id in self.active_benchmarks:
            return self.active_benchmarks[benchmark_id]
        elif benchmark_id in self.completed_benchmarks:
            return self.completed_benchmarks[benchmark_id]
        else:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND, detail=f"Benchmark {benchmark_id} not found"
            )

    async def list_benchmarks(self) -> List[Dict[str, Any]]:
        """List all benchmarks."""
        benchmarks = []
        benchmarks.extend(list(self.active_benchmarks.values()))
        benchmarks.extend(list(self.completed_benchmarks.values()))
        return benchmarks

    async def stop_benchmark(self, benchmark_id: str) -> bool:
        """Stop a running benchmark."""
        if benchmark_id in self.active_benchmarks:
            benchmark = self.active_benchmarks[benchmark_id]
            benchmark["status"] = BenchmarkStatus.STOPPED
            benchmark["updated_at"] = datetime.now(timezone.utc)

            # Move to completed
            self.completed_benchmarks[benchmark_id] = benchmark
            del self.active_benchmarks[benchmark_id]
            return True
        return False


# Initialize placeholder service
benchmark_service = BenchmarkService()


@router.post("/", response_model=BenchmarkResponse, status_code=status.HTTP_201_CREATED)
async def create_benchmark(request: BenchmarkCreateRequest):
    """
    Create a new benchmark.

    This endpoint creates a new benchmark configuration but does not execute it.
    Use the run endpoint to start the benchmark execution.
    """
    try:
        benchmark_id = await benchmark_service.create_benchmark(request.config)

        return BenchmarkResponse(
            benchmark_id=benchmark_id,
            status=BenchmarkStatus.CREATED,
            message="Benchmark created successfully",
        )
    except Exception as e:
        logger.error(f"Failed to create benchmark: {e!s}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to create benchmark: {e!s}",
        )


@router.post("/{benchmark_id}/run", response_model=BenchmarkRunResponse)
async def run_benchmark(benchmark_id: str, background_tasks: BackgroundTasks):
    """
    Run a benchmark.

    This endpoint starts the execution of a previously created benchmark.
    The benchmark runs in the background and results can be retrieved using the status endpoint.
    """
    try:
        # In a real implementation, this would run in the background
        # For now, we'll run it synchronously for simplicity
        result = await benchmark_service.run_benchmark(benchmark_id)

        return BenchmarkRunResponse(
            benchmark_id=benchmark_id,
            status=result.status,
            message="Benchmark completed successfully",
            result=result,
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to run benchmark {benchmark_id}: {e!s}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to run benchmark: {e!s}",
        )


@router.get("/{benchmark_id}/status")
async def get_benchmark_status(benchmark_id: str):
    """
    Get the status of a benchmark.

    This endpoint returns the current status and metadata of a benchmark.
    If the benchmark has completed, it will include the results.
    """
    try:
        status = await benchmark_service.get_benchmark_status(benchmark_id)
        return JSONResponse(content=status)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get status for benchmark {benchmark_id}: {e!s}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get benchmark status: {e!s}",
        )


@router.get("/", response_model=List[Dict[str, Any]])
async def list_benchmarks():
    """
    List all benchmarks.

    This endpoint returns a list of all benchmarks, both active and completed.
    """
    try:
        benchmarks = await benchmark_service.list_benchmarks()
        return benchmarks
    except Exception as e:
        logger.error(f"Failed to list benchmarks: {e!s}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to list benchmarks: {e!s}",
        )


@router.post("/{benchmark_id}/stop")
async def stop_benchmark(benchmark_id: str):
    """
    Stop a running benchmark.

    This endpoint attempts to stop a currently running benchmark.
    """
    try:
        stopped = await benchmark_service.stop_benchmark(benchmark_id)
        if stopped:
            return {"message": f"Benchmark {benchmark_id} stopped successfully"}
        else:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Benchmark {benchmark_id} not found or not running",
            )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to stop benchmark {benchmark_id}: {e!s}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to stop benchmark: {e!s}",
        )


@router.get("/health", response_model=HealthResponse)
async def health_check():
    """
    Health check endpoint.

    This endpoint returns the health status of the benchmark service.
    """
    return HealthResponse(
        status="healthy",
        version="1.0.0",
        services={
            "benchmark_service": "healthy",
            "database": "healthy",
            "message_queue": "healthy",
        },
    )


@router.delete("/{benchmark_id}")
async def delete_benchmark(benchmark_id: str):
    """
    Delete a benchmark.

    This endpoint deletes a benchmark and all associated data.
    """
    try:
        # Check if benchmark exists
        if benchmark_id in benchmark_service.active_benchmarks:
            del benchmark_service.active_benchmarks[benchmark_id]
        elif benchmark_id in benchmark_service.completed_benchmarks:
            del benchmark_service.completed_benchmarks[benchmark_id]
        else:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND, detail=f"Benchmark {benchmark_id} not found"
            )

        return {"message": f"Benchmark {benchmark_id} deleted successfully"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete benchmark {benchmark_id}: {e!s}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete benchmark: {e!s}",
        )


# Note: Exception handlers are registered at the application level in app_factory.add_exception_handlers.
# Router-level exception decorators are not supported by FastAPI's APIRouter and were removed.
