........................................................................ [  5%]
............F........................................................... [ 10%]
.......................................E.......E......EFEEE............. [ 15%]
.........FFEEEE.....F......EEEEE..............................EEEEFF.... [ 21%]
F.F.F.FFFFFFFFF.FFF......FFF...................FF.........F...FF......FF [ 26%]
.FFFEF..FFEEEEE...EEEEEE.EE.......EF.FFFF.FF.F..EF.F.F.................. [ 31%]
..FF.................................................................... [ 36%]
..FFFFF..............FFFFFFFFFFFFFFFFFFFFF.....................F..FFFFFF [ 42%]
..FFFFFFFFF..FFFFFFFFF..FF.F.F..FFF..F....FFFFF......F.................F [ 47%]
..F.FF..FF............F.F............................FFF......FFFFFFFFFF [ 52%]
.FF....................................................F.....FFFFFF..... [ 57%]
...................................F....FF.F...F..F..F..F.F............. [ 63%]
......FFF.F........FF.......FF.........F....F.FF.FF.FFF.F....F.......... [ 68%]
.........................FFFFFFFFFFFFFFFFFFFFFFFFFFFF................... [ 73%]
...................................................FFFFFFFFFFFFFFFFFFFFF [ 78%]
FFFFF.F..........................................F..s.........F.....FF.. [ 84%]
..F.F.....FFFFFFFFFFFFFFFFFFFFFFFFFFFF...FF...........F................. [ 89%]
..............................................F...............FFF...F.F. [ 94%]
.F.F.FFFFF.FFFFFFFFFFFFF.FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF..FFFFFFFFFFF [ 99%]
..                                                                       [100%]
=================================== ERRORS ====================================
__ ERROR at setup of TestAgentRunnerInterface.test_simulation_state_methods ___

    @pytest.fixture
    def sample_product():
        """Create a sample product for testing."""
>       return Product(
            asin="B0TEST123",
            category="electronics",
            cost=Money.from_dollars(10.0),
            price=Money.from_dollars(20.0),
            base_demand=100.0,
            inventory_units=50,
        )
E       pydantic_core._pydantic_core.ValidationError: 5 validation errors for Canonical Product Model
E       sku
E         Field required [type=missing, input_value={'asin': 'B0TEST123', 'ca..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       name
E         Field required [type=missing, input_value={'asin': 'B0TEST123', 'ca..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       price
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=2000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       cost
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=1000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       weight_kg
E         Field required [type=missing, input_value={'asin': 'B0TEST123', 'ca..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_agent_runners.py:57: ValidationError
______ ERROR at setup of TestAgentManager.test_agent_decision_processing ______

    @pytest.fixture
    def sample_product():
        """Create a sample product for testing."""
>       return Product(
            asin="B0TEST123",
            category="electronics",
            cost=Money.from_dollars(10.0),
            price=Money.from_dollars(20.0),
            base_demand=100.0,
            inventory_units=50,
        )
E       pydantic_core._pydantic_core.ValidationError: 5 validation errors for Canonical Product Model
E       sku
E         Field required [type=missing, input_value={'asin': 'B0TEST123', 'ca..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       name
E         Field required [type=missing, input_value={'asin': 'B0TEST123', 'ca..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       price
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=2000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       cost
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=1000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       weight_kg
E         Field required [type=missing, input_value={'asin': 'B0TEST123', 'ca..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_agent_runners.py:57: ValidationError
_________________ ERROR at setup of test_end_to_end_workflow __________________

    @pytest.fixture
    def sample_product():
        """Create a sample product for testing."""
>       return Product(
            asin="B0TEST123",
            category="electronics",
            cost=Money.from_dollars(10.0),
            price=Money.from_dollars(20.0),
            base_demand=100.0,
            inventory_units=50,
        )
E       pydantic_core._pydantic_core.ValidationError: 5 validation errors for Canonical Product Model
E       sku
E         Field required [type=missing, input_value={'asin': 'B0TEST123', 'ca..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       name
E         Field required [type=missing, input_value={'asin': 'B0TEST123', 'ca..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       price
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=2000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       cost
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=1000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       weight_kg
E         Field required [type=missing, input_value={'asin': 'B0TEST123', 'ca..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_agent_runners.py:57: ValidationError
_ ERROR at setup of TestCompetitorEventFlow.test_sales_service_receives_competitor_data _

    @pytest.fixture
    def sales_service():
        """Create a test sales service."""
        config = {
            "demand_model": "exponential",
            "base_conversion_rate": 0.15,
            "price_elasticity": -1.5,
            "trust_score_impact": 0.3,
            "inventory_impact_threshold": 10,
            "demand_volatility": 0.1,
            "conversion_volatility": 0.05,
            "max_history_size": 100,
        }
    
        # Mock fee service
        fee_service = MagicMock(spec=FeeCalculationService)
        fee_service.calculate_fees = AsyncMock(
            return_value=(
                Money.from_dollars(5.00),  # total_fees
                {  # fee_breakdown
                    "referral_fee": Money.from_dollars(3.00),
                    "fulfillment_fee": Money.from_dollars(2.00),
                },
            )
        )
    
>       return SalesService(config, fee_service)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: SalesService.__init__() takes 2 positional arguments but 3 were given

tests\test_competitor_integration.py:74: TypeError
__ ERROR at setup of TestCompetitorEventFlow.test_end_to_end_competitor_flow __

    @pytest.fixture
    def sales_service():
        """Create a test sales service."""
        config = {
            "demand_model": "exponential",
            "base_conversion_rate": 0.15,
            "price_elasticity": -1.5,
            "trust_score_impact": 0.3,
            "inventory_impact_threshold": 10,
            "demand_volatility": 0.1,
            "conversion_volatility": 0.05,
            "max_history_size": 100,
        }
    
        # Mock fee service
        fee_service = MagicMock(spec=FeeCalculationService)
        fee_service.calculate_fees = AsyncMock(
            return_value=(
                Money.from_dollars(5.00),  # total_fees
                {  # fee_breakdown
                    "referral_fee": Money.from_dollars(3.00),
                    "fulfillment_fee": Money.from_dollars(2.00),
                },
            )
        )
    
>       return SalesService(config, fee_service)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: SalesService.__init__() takes 2 positional arguments but 3 were given

tests\test_competitor_integration.py:74: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:26:21.344 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized.
2025-09-16 01:26:21.345 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
2025-09-16 01:26:21.345 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager initialized
----------------------------- Captured log setup ------------------------------
INFO     fba_bench_core.services.world_store:world_store.py:90 InMemoryStorageBackend initialized.
INFO     fba_bench_core.services.world_store:world_store.py:353 WorldStore initialized - ready for multi-agent command processing
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:101 CompetitorManager initialized
_ ERROR at setup of TestCompetitorEventFlow.test_competitor_price_changes_affect_demand _

    @pytest.fixture
    def sales_service():
        """Create a test sales service."""
        config = {
            "demand_model": "exponential",
            "base_conversion_rate": 0.15,
            "price_elasticity": -1.5,
            "trust_score_impact": 0.3,
            "inventory_impact_threshold": 10,
            "demand_volatility": 0.1,
            "conversion_volatility": 0.05,
            "max_history_size": 100,
        }
    
        # Mock fee service
        fee_service = MagicMock(spec=FeeCalculationService)
        fee_service.calculate_fees = AsyncMock(
            return_value=(
                Money.from_dollars(5.00),  # total_fees
                {  # fee_breakdown
                    "referral_fee": Money.from_dollars(3.00),
                    "fulfillment_fee": Money.from_dollars(2.00),
                },
            )
        )
    
>       return SalesService(config, fee_service)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: SalesService.__init__() takes 2 positional arguments but 3 were given

tests\test_competitor_integration.py:74: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:26:21.381 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized.
2025-09-16 01:26:21.382 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
2025-09-16 01:26:21.382 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager initialized
----------------------------- Captured log setup ------------------------------
INFO     fba_bench_core.services.world_store:world_store.py:90 InMemoryStorageBackend initialized.
INFO     fba_bench_core.services.world_store:world_store.py:353 WorldStore initialized - ready for multi-agent command processing
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:101 CompetitorManager initialized
___ ERROR at setup of TestEventDrivenCore.test_sales_service_event_handling ___

fee_service = <fba_bench_core.services.fee_calculation_service.FeeCalculationService object at 0x0000016F8E0E04D0>

    @pytest.fixture
    def sales_service(fee_service):
        """Create a sales service for testing."""
        config = {"demand_model": "exponential", "base_conversion_rate": 0.15, "price_elasticity": -1.5}
>       return SalesService(config, fee_service)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: SalesService.__init__() takes 2 positional arguments but 3 were given

tests\test_event_integration.py:82: TypeError
_ ERROR at setup of TestEventDrivenCore.test_trust_service_sale_event_handling _

    @pytest.fixture
    def sample_product():
        """Create a sample product for testing."""
>       return Product(
            asin="B12345TEST",
            category="electronics",
            cost=Money.from_dollars(10.0),
            price=Money.from_dollars(25.0),
            base_demand=50.0,
            bsr=100000,
            trust_score=0.8,
            inventory_units=100,
            size="small",
            weight=1.0,
        )
E       pydantic_core._pydantic_core.ValidationError: 5 validation errors for Canonical Product Model
E       sku
E         Field required [type=missing, input_value={'asin': 'B12345TEST', 'c... 'small', 'weight': 1.0}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       name
E         Field required [type=missing, input_value={'asin': 'B12345TEST', 'c... 'small', 'weight': 1.0}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       price
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=2500, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       cost
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=1000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       weight_kg
E         Field required [type=missing, input_value={'asin': 'B12345TEST', 'c... 'small', 'weight': 1.0}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_event_integration.py:55: ValidationError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:26:27.313 | INFO | fba_bench_core.services.trust_score_service | [req=-] | TrustScoreService initialized with config: {'base_trust_score': 0.7, 'trust_event_weights': {'sale': 0.02, 'stockout': -0.03}}
----------------------------- Captured log setup ------------------------------
INFO     fba_bench_core.services.trust_score_service:trust_score_service.py:27 TrustScoreService initialized with config: {'base_trust_score': 0.7, 'trust_event_weights': {'sale': 0.02, 'stockout': -0.03}}
___ ERROR at setup of TestEndToEndIntegration.test_complete_simulation_flow ___

fee_service = <fba_bench_core.services.fee_calculation_service.FeeCalculationService object at 0x0000016F8E0B2510>

    @pytest.fixture
    def sales_service(fee_service):
        """Create a sales service for testing."""
        config = {"demand_model": "exponential", "base_conversion_rate": 0.15, "price_elasticity": -1.5}
>       return SalesService(config, fee_service)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: SalesService.__init__() takes 2 positional arguments but 3 were given

tests\test_event_integration.py:82: TypeError
_ ERROR at setup of TestEndToEndIntegration.test_money_type_strict_enforcement _

    @pytest.fixture
    def sample_product():
        """Create a sample product for testing."""
>       return Product(
            asin="B12345TEST",
            category="electronics",
            cost=Money.from_dollars(10.0),
            price=Money.from_dollars(25.0),
            base_demand=50.0,
            bsr=100000,
            trust_score=0.8,
            inventory_units=100,
            size="small",
            weight=1.0,
        )
E       pydantic_core._pydantic_core.ValidationError: 5 validation errors for Canonical Product Model
E       sku
E         Field required [type=missing, input_value={'asin': 'B12345TEST', 'c... 'small', 'weight': 1.0}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       name
E         Field required [type=missing, input_value={'asin': 'B12345TEST', 'c... 'small', 'weight': 1.0}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       price
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=2500, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       cost
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=1000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       weight_kg
E         Field required [type=missing, input_value={'asin': 'B12345TEST', 'c... 'small', 'weight': 1.0}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_event_integration.py:55: ValidationError
______ ERROR at setup of TestGreedyScriptBot.test_decide_price_matching _______

    @pytest.fixture
    def sample_product_state_basic():
>       return Product(
            asin="B0TEST00A",
            category="electronics",
            cost=Money.from_dollars(10.0),
            price=Money.from_dollars(20.0),
            base_demand=100.0,
            inventory_units=20,
            metadata={
                "competitor_prices": [
                    ("B0COMP01", Money.from_dollars(19.0)),
                    ("B0COMP02", Money.from_dollars(21.0)),
                ]
            },
        )
E       pydantic_core._pydantic_core.ValidationError: 5 validation errors for Canonical Product Model
E       sku
E         Field required [type=missing, input_value={'asin': 'B0TEST00A', 'ca...100, currency='USD'))]}}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       name
E         Field required [type=missing, input_value={'asin': 'B0TEST00A', 'ca...100, currency='USD'))]}}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       price
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=2000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       cost
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=1000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       weight_kg
E         Field required [type=missing, input_value={'asin': 'B0TEST00A', 'ca...100, currency='USD'))]}}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_greedy_script_bot.py:13: ValidationError
_ ERROR at setup of TestGreedyScriptBot.test_decide_price_below_cost_prevention _

    @pytest.fixture
    def sample_product_state_basic():
>       return Product(
            asin="B0TEST00A",
            category="electronics",
            cost=Money.from_dollars(10.0),
            price=Money.from_dollars(20.0),
            base_demand=100.0,
            inventory_units=20,
            metadata={
                "competitor_prices": [
                    ("B0COMP01", Money.from_dollars(19.0)),
                    ("B0COMP02", Money.from_dollars(21.0)),
                ]
            },
        )
E       pydantic_core._pydantic_core.ValidationError: 5 validation errors for Canonical Product Model
E       sku
E         Field required [type=missing, input_value={'asin': 'B0TEST00A', 'ca...100, currency='USD'))]}}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       name
E         Field required [type=missing, input_value={'asin': 'B0TEST00A', 'ca...100, currency='USD'))]}}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       price
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=2000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       cost
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=1000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       weight_kg
E         Field required [type=missing, input_value={'asin': 'B0TEST00A', 'ca...100, currency='USD'))]}}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_greedy_script_bot.py:13: ValidationError
______ ERROR at setup of TestGreedyScriptBot.test_decide_no_competitors _______

    @pytest.fixture
    def sample_product_state_basic():
>       return Product(
            asin="B0TEST00A",
            category="electronics",
            cost=Money.from_dollars(10.0),
            price=Money.from_dollars(20.0),
            base_demand=100.0,
            inventory_units=20,
            metadata={
                "competitor_prices": [
                    ("B0COMP01", Money.from_dollars(19.0)),
                    ("B0COMP02", Money.from_dollars(21.0)),
                ]
            },
        )
E       pydantic_core._pydantic_core.ValidationError: 5 validation errors for Canonical Product Model
E       sku
E         Field required [type=missing, input_value={'asin': 'B0TEST00A', 'ca...100, currency='USD'))]}}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       name
E         Field required [type=missing, input_value={'asin': 'B0TEST00A', 'ca...100, currency='USD'))]}}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       price
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=2000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       cost
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=1000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       weight_kg
E         Field required [type=missing, input_value={'asin': 'B0TEST00A', 'ca...100, currency='USD'))]}}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_greedy_script_bot.py:13: ValidationError
_ ERROR at setup of TestGreedyScriptBot.test_decide_inventory_management_logging _

    @pytest.fixture
    def sample_product_state_basic():
>       return Product(
            asin="B0TEST00A",
            category="electronics",
            cost=Money.from_dollars(10.0),
            price=Money.from_dollars(20.0),
            base_demand=100.0,
            inventory_units=20,
            metadata={
                "competitor_prices": [
                    ("B0COMP01", Money.from_dollars(19.0)),
                    ("B0COMP02", Money.from_dollars(21.0)),
                ]
            },
        )
E       pydantic_core._pydantic_core.ValidationError: 5 validation errors for Canonical Product Model
E       sku
E         Field required [type=missing, input_value={'asin': 'B0TEST00A', 'ca...100, currency='USD'))]}}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       name
E         Field required [type=missing, input_value={'asin': 'B0TEST00A', 'ca...100, currency='USD'))]}}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       price
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=2000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       cost
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=1000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       weight_kg
E         Field required [type=missing, input_value={'asin': 'B0TEST00A', 'ca...100, currency='USD'))]}}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_greedy_script_bot.py:13: ValidationError
_ ERROR at setup of TestGreedyScriptBot.test_decide_price_insignificant_change _

    @pytest.fixture
    def sample_product_state_basic():
>       return Product(
            asin="B0TEST00A",
            category="electronics",
            cost=Money.from_dollars(10.0),
            price=Money.from_dollars(20.0),
            base_demand=100.0,
            inventory_units=20,
            metadata={
                "competitor_prices": [
                    ("B0COMP01", Money.from_dollars(19.0)),
                    ("B0COMP02", Money.from_dollars(21.0)),
                ]
            },
        )
E       pydantic_core._pydantic_core.ValidationError: 5 validation errors for Canonical Product Model
E       sku
E         Field required [type=missing, input_value={'asin': 'B0TEST00A', 'ca...100, currency='USD'))]}}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       name
E         Field required [type=missing, input_value={'asin': 'B0TEST00A', 'ca...100, currency='USD'))]}}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       price
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=2000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       cost
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=1000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       weight_kg
E         Field required [type=missing, input_value={'asin': 'B0TEST00A', 'ca...100, currency='USD'))]}}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_greedy_script_bot.py:13: ValidationError
_________ ERROR at setup of TestLLMBots.test_gpt35_bot_decide_success _________

    @pytest.fixture
    def sample_llm_simulation_state():
        # A realistic-ish product for LLM consumption
>       product1 = Product(
            asin="B0EXAMPLE01",
            category="electronics",
            cost=Money.from_dollars(10.0),
            price=Money.from_dollars(20.0),
            base_demand=100.0,
            inventory_units=50,
            # competitor_prices not directly in Product; simulate its presence if needed by prompt adapter
            # Assuming prompt adapter extracts this from broader simulation state or world store
        )
E       pydantic_core._pydantic_core.ValidationError: 5 validation errors for Canonical Product Model
E       sku
E         Field required [type=missing, input_value={'asin': 'B0EXAMPLE01', '..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       name
E         Field required [type=missing, input_value={'asin': 'B0EXAMPLE01', '..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       price
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=2000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       cost
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=1000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       weight_kg
E         Field required [type=missing, input_value={'asin': 'B0EXAMPLE01', '..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_llm_bots.py:127: ValidationError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:27:01.792 | INFO | constraints.token_counter | [req=-] | Initialized tiktoken encoding for model: gpt-3.5-turbo
----------------------------- Captured log setup ------------------------------
INFO     constraints.token_counter:token_counter.py:194 Initialized tiktoken encoding for model: gpt-3.5-turbo
_ ERROR at setup of TestLLMBots.test_gpt4o_mini_bot_decide_no_actions_on_parsing_error _

    @pytest.fixture
    def sample_llm_simulation_state():
        # A realistic-ish product for LLM consumption
>       product1 = Product(
            asin="B0EXAMPLE01",
            category="electronics",
            cost=Money.from_dollars(10.0),
            price=Money.from_dollars(20.0),
            base_demand=100.0,
            inventory_units=50,
            # competitor_prices not directly in Product; simulate its presence if needed by prompt adapter
            # Assuming prompt adapter extracts this from broader simulation state or world store
        )
E       pydantic_core._pydantic_core.ValidationError: 5 validation errors for Canonical Product Model
E       sku
E         Field required [type=missing, input_value={'asin': 'B0EXAMPLE01', '..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       name
E         Field required [type=missing, input_value={'asin': 'B0EXAMPLE01', '..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       price
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=2000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       cost
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=1000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       weight_kg
E         Field required [type=missing, input_value={'asin': 'B0EXAMPLE01', '..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_llm_bots.py:127: ValidationError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:27:01.805 | INFO | constraints.token_counter | [req=-] | Initialized tiktoken encoding for model: gpt-3.5-turbo
----------------------------- Captured log setup ------------------------------
INFO     constraints.token_counter:token_counter.py:194 Initialized tiktoken encoding for model: gpt-3.5-turbo
_ ERROR at setup of TestLLMBots.test_grok4_bot_decide_budget_exceeded_before_llm_call _

    @pytest.fixture
    def sample_llm_simulation_state():
        # A realistic-ish product for LLM consumption
>       product1 = Product(
            asin="B0EXAMPLE01",
            category="electronics",
            cost=Money.from_dollars(10.0),
            price=Money.from_dollars(20.0),
            base_demand=100.0,
            inventory_units=50,
            # competitor_prices not directly in Product; simulate its presence if needed by prompt adapter
            # Assuming prompt adapter extracts this from broader simulation state or world store
        )
E       pydantic_core._pydantic_core.ValidationError: 5 validation errors for Canonical Product Model
E       sku
E         Field required [type=missing, input_value={'asin': 'B0EXAMPLE01', '..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       name
E         Field required [type=missing, input_value={'asin': 'B0EXAMPLE01', '..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       price
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=2000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       cost
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=1000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       weight_kg
E         Field required [type=missing, input_value={'asin': 'B0EXAMPLE01', '..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_llm_bots.py:127: ValidationError
_ ERROR at setup of TestLLMBots.test_claude_sonnet_bot_decide_empty_response __

    @pytest.fixture
    def sample_llm_simulation_state():
        # A realistic-ish product for LLM consumption
>       product1 = Product(
            asin="B0EXAMPLE01",
            category="electronics",
            cost=Money.from_dollars(10.0),
            price=Money.from_dollars(20.0),
            base_demand=100.0,
            inventory_units=50,
            # competitor_prices not directly in Product; simulate its presence if needed by prompt adapter
            # Assuming prompt adapter extracts this from broader simulation state or world store
        )
E       pydantic_core._pydantic_core.ValidationError: 5 validation errors for Canonical Product Model
E       sku
E         Field required [type=missing, input_value={'asin': 'B0EXAMPLE01', '..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       name
E         Field required [type=missing, input_value={'asin': 'B0EXAMPLE01', '..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing
E       price
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=2000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       cost
E         Input should be an instance of Money [type=is_instance_of, input_value=Money(cents=1000, currency='USD'), input_type=Money]
E           For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
E       weight_kg
E         Field required [type=missing, input_value={'asin': 'B0EXAMPLE01', '..., 'inventory_units': 50}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_llm_bots.py:127: ValidationError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:27:01.828 | INFO | constraints.token_counter | [req=-] | Initialized tiktoken encoding for model: gpt-3.5-turbo
----------------------------- Captured log setup ------------------------------
INFO     constraints.token_counter:token_counter.py:194 Initialized tiktoken encoding for model: gpt-3.5-turbo
________________ ERROR at setup of test_cli_generate_scenario _________________
file C:\Users\admin\Downloads\fba\tests\test_scenario_system.py, line 341
  @pytest.mark.asyncio
  @patch(
      "sys.argv",
      [
          "experiment_cli.py",
          "run",
          "--generate-scenario",
          "tier_0_baseline",
          "--dynamic-randomization-config",
          os.path.join(TEST_SCENARIO_DIR, "dynamic_rand_config.yaml"),
          "--dynamic-scenario-output",
          os.path.join(TEST_RESULTS_DIR, "generated_scenario_cli.yaml"),
      ],
  )
  async def test_cli_generate_scenario(mock_cli_parse_args):
      """Test CLI integration for dynamic scenario generation."""
      # The actual cli_main will sys.exit(0) after generating.
      # We need to mock sys.exit to prevent test runner from stopping.
      with patch("sys.exit") as mock_sys_exit:
          await cli_main()
          mock_sys_exit.assert_called_once_with(0)  # Ensure it exits as expected

      output_file_path = Path(TEST_RESULTS_DIR) / "generated_scenario_cli.yaml"
      assert output_file_path.exists()

      generated_config = ScenarioConfig.from_yaml(str(output_file_path))
      assert generated_config.config_data["scenario_name"].startswith("Test Tier 0 Baseline")
      assert isinstance(generated_config.config_data["agent_constraints"]["initial_capital"], int)
      assert generated_config.validate_scenario_consistency() is True
E       fixture 'mock_cli_parse_args' not found
>       available fixtures: _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, basic_simulation_seed_factory, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, config, connector, cov, data_regression, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, monkeypatch, no_cover, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, set_consistent_seed, setup_test_environment, sim_factory, tests/test_scenario_system.py::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

C:\Users\admin\Downloads\fba\tests\test_scenario_system.py:341
__ ERROR at setup of TestSystemIntegration.test_agent_manager_initialization __

self = <tests.test_system_integration.TestSystemIntegration object at 0x0000016F8B229850>

    @pytest.fixture
    def test_config(self) -> PydanticConfig:
        """
        Create a test configuration.
    
        Returns:
            Test configuration
        """
        return PydanticConfig(
>           llm_config=LLMConfig(
                model="gpt-3.5-turbo", temperature=0.7, max_tokens=1000, api_key="test-api-key"
            ),
            agent_configs=[
                AgentConfig(
                    name="test-agent",
                    type="diy",
                    config={
                        "llm_config": {
                            "model": "gpt-3.5-turbo",
                            "temperature": 0.7,
                            "api_key": "test-api-key",
                        },
                        "system_prompt": "You are a helpful assistant.",
                    },
                )
            ],
            benchmark_config={
                "max_ticks": 100,
                "time_limit": 60.0,
                "metrics": ["revenue", "profit", "costs"],
            },
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for LLMConfig
E       name
E         Field required [type=missing, input_value={'model': 'gpt-3.5-turbo'...pi_key': 'test-api-key'}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_system_integration.py:57: ValidationError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:27:08.169 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized.
2025-09-16 01:27:08.169 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
----------------------------- Captured log setup ------------------------------
INFO     fba_bench_core.services.world_store:world_store.py:90 InMemoryStorageBackend initialized.
INFO     fba_bench_core.services.world_store:world_store.py:353 WorldStore initialized - ready for multi-agent command processing
___ ERROR at setup of TestSystemIntegration.test_agent_manager_create_agent ___

self = <tests.test_system_integration.TestSystemIntegration object at 0x0000016F8B229A90>

    @pytest.fixture
    def test_config(self) -> PydanticConfig:
        """
        Create a test configuration.
    
        Returns:
            Test configuration
        """
        return PydanticConfig(
>           llm_config=LLMConfig(
                model="gpt-3.5-turbo", temperature=0.7, max_tokens=1000, api_key="test-api-key"
            ),
            agent_configs=[
                AgentConfig(
                    name="test-agent",
                    type="diy",
                    config={
                        "llm_config": {
                            "model": "gpt-3.5-turbo",
                            "temperature": 0.7,
                            "api_key": "test-api-key",
                        },
                        "system_prompt": "You are a helpful assistant.",
                    },
                )
            ],
            benchmark_config={
                "max_ticks": 100,
                "time_limit": 60.0,
                "metrics": ["revenue", "profit", "costs"],
            },
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for LLMConfig
E       name
E         Field required [type=missing, input_value={'model': 'gpt-3.5-turbo'...pi_key': 'test-api-key'}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_system_integration.py:57: ValidationError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:27:08.183 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized.
2025-09-16 01:27:08.184 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
----------------------------- Captured log setup ------------------------------
INFO     fba_bench_core.services.world_store:world_store.py:90 InMemoryStorageBackend initialized.
INFO     fba_bench_core.services.world_store:world_store.py:353 WorldStore initialized - ready for multi-agent command processing
__ ERROR at setup of TestSystemIntegration.test_agent_manager_decision_cycle __

self = <tests.test_system_integration.TestSystemIntegration object at 0x0000016F8B229CD0>

    @pytest.fixture
    def test_config(self) -> PydanticConfig:
        """
        Create a test configuration.
    
        Returns:
            Test configuration
        """
        return PydanticConfig(
>           llm_config=LLMConfig(
                model="gpt-3.5-turbo", temperature=0.7, max_tokens=1000, api_key="test-api-key"
            ),
            agent_configs=[
                AgentConfig(
                    name="test-agent",
                    type="diy",
                    config={
                        "llm_config": {
                            "model": "gpt-3.5-turbo",
                            "temperature": 0.7,
                            "api_key": "test-api-key",
                        },
                        "system_prompt": "You are a helpful assistant.",
                    },
                )
            ],
            benchmark_config={
                "max_ticks": 100,
                "time_limit": 60.0,
                "metrics": ["revenue", "profit", "costs"],
            },
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for LLMConfig
E       name
E         Field required [type=missing, input_value={'model': 'gpt-3.5-turbo'...pi_key': 'test-api-key'}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_system_integration.py:57: ValidationError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:27:08.198 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized.
2025-09-16 01:27:08.198 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
----------------------------- Captured log setup ------------------------------
INFO     fba_bench_core.services.world_store:world_store.py:90 InMemoryStorageBackend initialized.
INFO     fba_bench_core.services.world_store:world_store.py:353 WorldStore initialized - ready for multi-agent command processing
_ ERROR at setup of TestSystemIntegration.test_benchmark_engine_initialization _

self = <tests.test_system_integration.TestSystemIntegration object at 0x0000016F8B229F40>

    @pytest.fixture
    def test_config(self) -> PydanticConfig:
        """
        Create a test configuration.
    
        Returns:
            Test configuration
        """
        return PydanticConfig(
>           llm_config=LLMConfig(
                model="gpt-3.5-turbo", temperature=0.7, max_tokens=1000, api_key="test-api-key"
            ),
            agent_configs=[
                AgentConfig(
                    name="test-agent",
                    type="diy",
                    config={
                        "llm_config": {
                            "model": "gpt-3.5-turbo",
                            "temperature": 0.7,
                            "api_key": "test-api-key",
                        },
                        "system_prompt": "You are a helpful assistant.",
                    },
                )
            ],
            benchmark_config={
                "max_ticks": 100,
                "time_limit": 60.0,
                "metrics": ["revenue", "profit", "costs"],
            },
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for LLMConfig
E       name
E         Field required [type=missing, input_value={'model': 'gpt-3.5-turbo'...pi_key': 'test-api-key'}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_system_integration.py:57: ValidationError
_ ERROR at setup of TestSystemIntegration.test_benchmark_engine_run_benchmark _

self = <tests.test_system_integration.TestSystemIntegration object at 0x0000016F8B22A1E0>

    @pytest.fixture
    def test_config(self) -> PydanticConfig:
        """
        Create a test configuration.
    
        Returns:
            Test configuration
        """
        return PydanticConfig(
>           llm_config=LLMConfig(
                model="gpt-3.5-turbo", temperature=0.7, max_tokens=1000, api_key="test-api-key"
            ),
            agent_configs=[
                AgentConfig(
                    name="test-agent",
                    type="diy",
                    config={
                        "llm_config": {
                            "model": "gpt-3.5-turbo",
                            "temperature": 0.7,
                            "api_key": "test-api-key",
                        },
                        "system_prompt": "You are a helpful assistant.",
                    },
                )
            ],
            benchmark_config={
                "max_ticks": 100,
                "time_limit": 60.0,
                "metrics": ["revenue", "profit", "costs"],
            },
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for LLMConfig
E       name
E         Field required [type=missing, input_value={'model': 'gpt-3.5-turbo'...pi_key': 'test-api-key'}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_system_integration.py:57: ValidationError
_ ERROR at setup of TestSystemIntegration.test_pricing_scenario_initialization _

self = <tests.test_system_integration.TestSystemIntegration object at 0x0000016F8B22A690>

    @pytest.fixture
    def pricing_scenario(self) -> PricingScenario:
        """
        Create a test PricingScenario.
    
        Returns:
            Test PricingScenario
        """
        config = ScenarioConfig(
            name="test-pricing",
            description="Test pricing scenario",
            difficulty=ScenarioDifficulty.EASY,
            scenario_type=ScenarioType.PRICING,
            max_ticks=50,
            time_limit=30.0,
            initial_state={
                "base_price": 10.0,
                "cost_per_unit": 5.0,
                "max_price": 20.0,
                "min_price": 5.0,
                "price_elasticity": -1.5,
                "competitor_price": 10.0,
                "competitor_price_volatility": 0.1,
            },
            success_criteria={"min_profit": 100.0},
            failure_criteria={"max_loss": -50.0},
        )
>       return PricingScenario(config)
               ^^^^^^^^^^^^^^^^^^^^^^^

tests\test_system_integration.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\scenarios\refined_scenarios.py:318: in __init__
    super().__init__(config)
benchmarking\scenarios\refined_scenarios.py:193: in __init__
    self.context = ScenarioContext.from_dict(config.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'benchmarking.scenarios.refined_scenarios.ScenarioContext'>
data = {'base_price': 10.0, 'competitor_price': 10.0, 'competitor_price_volatility': 0.1, 'cost_per_unit': 5.0, ...}

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ScenarioContext":
        """Create context from dictionary."""
>       return cls(**data)
               ^^^^^^^^^^^
E       TypeError: ScenarioContext.__init__() got an unexpected keyword argument 'base_price'

benchmarking\scenarios\refined_scenarios.py:122: TypeError
___ ERROR at setup of TestSystemIntegration.test_pricing_scenario_execution ___

self = <tests.test_system_integration.TestSystemIntegration object at 0x0000016F8B229D00>

    @pytest.fixture
    def pricing_scenario(self) -> PricingScenario:
        """
        Create a test PricingScenario.
    
        Returns:
            Test PricingScenario
        """
        config = ScenarioConfig(
            name="test-pricing",
            description="Test pricing scenario",
            difficulty=ScenarioDifficulty.EASY,
            scenario_type=ScenarioType.PRICING,
            max_ticks=50,
            time_limit=30.0,
            initial_state={
                "base_price": 10.0,
                "cost_per_unit": 5.0,
                "max_price": 20.0,
                "min_price": 5.0,
                "price_elasticity": -1.5,
                "competitor_price": 10.0,
                "competitor_price_volatility": 0.1,
            },
            success_criteria={"min_profit": 100.0},
            failure_criteria={"max_loss": -50.0},
        )
>       return PricingScenario(config)
               ^^^^^^^^^^^^^^^^^^^^^^^

tests\test_system_integration.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\scenarios\refined_scenarios.py:318: in __init__
    super().__init__(config)
benchmarking\scenarios\refined_scenarios.py:193: in __init__
    self.context = ScenarioContext.from_dict(config.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'benchmarking.scenarios.refined_scenarios.ScenarioContext'>
data = {'base_price': 10.0, 'competitor_price': 10.0, 'competitor_price_volatility': 0.1, 'cost_per_unit': 5.0, ...}

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ScenarioContext":
        """Create context from dictionary."""
>       return cls(**data)
               ^^^^^^^^^^^
E       TypeError: ScenarioContext.__init__() got an unexpected keyword argument 'base_price'

benchmarking\scenarios\refined_scenarios.py:122: TypeError
_ ERROR at setup of TestSystemIntegration.test_inventory_scenario_initialization _

self = <tests.test_system_integration.TestSystemIntegration object at 0x0000016F8B229760>

    @pytest.fixture
    def inventory_scenario(self) -> InventoryScenario:
        """
        Create a test InventoryScenario.
    
        Returns:
            Test InventoryScenario
        """
        config = ScenarioConfig(
            name="test-inventory",
            description="Test inventory scenario",
            difficulty=ScenarioDifficulty.MEDIUM,
            scenario_type=ScenarioType.INVENTORY,
            max_ticks=50,
            time_limit=30.0,
            initial_state={
                "initial_inventory": 100,
                "holding_cost_per_unit": 0.1,
                "stockout_cost_per_unit": 2.0,
                "order_cost_per_order": 10.0,
                "lead_time": 3,
                "max_order_quantity": 200,
                "demand_mean": 20,
                "demand_std": 5,
            },
            success_criteria={"max_cost": 500.0},
            failure_criteria={"min_fulfillment_rate": 0.5},
        )
>       return InventoryScenario(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^

tests\test_system_integration.py:192: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\scenarios\refined_scenarios.py:521: in __init__
    super().__init__(config)
benchmarking\scenarios\refined_scenarios.py:193: in __init__
    self.context = ScenarioContext.from_dict(config.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'benchmarking.scenarios.refined_scenarios.ScenarioContext'>
data = {'demand_mean': 20, 'demand_std': 5, 'holding_cost_per_unit': 0.1, 'initial_inventory': 100, ...}

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ScenarioContext":
        """Create context from dictionary."""
>       return cls(**data)
               ^^^^^^^^^^^
E       TypeError: ScenarioContext.__init__() got an unexpected keyword argument 'initial_inventory'

benchmarking\scenarios\refined_scenarios.py:122: TypeError
__ ERROR at setup of TestSystemIntegration.test_inventory_scenario_execution __

self = <tests.test_system_integration.TestSystemIntegration object at 0x0000016F8B22A8D0>

    @pytest.fixture
    def inventory_scenario(self) -> InventoryScenario:
        """
        Create a test InventoryScenario.
    
        Returns:
            Test InventoryScenario
        """
        config = ScenarioConfig(
            name="test-inventory",
            description="Test inventory scenario",
            difficulty=ScenarioDifficulty.MEDIUM,
            scenario_type=ScenarioType.INVENTORY,
            max_ticks=50,
            time_limit=30.0,
            initial_state={
                "initial_inventory": 100,
                "holding_cost_per_unit": 0.1,
                "stockout_cost_per_unit": 2.0,
                "order_cost_per_order": 10.0,
                "lead_time": 3,
                "max_order_quantity": 200,
                "demand_mean": 20,
                "demand_std": 5,
            },
            success_criteria={"max_cost": 500.0},
            failure_criteria={"min_fulfillment_rate": 0.5},
        )
>       return InventoryScenario(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^

tests\test_system_integration.py:192: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\scenarios\refined_scenarios.py:521: in __init__
    super().__init__(config)
benchmarking\scenarios\refined_scenarios.py:193: in __init__
    self.context = ScenarioContext.from_dict(config.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'benchmarking.scenarios.refined_scenarios.ScenarioContext'>
data = {'demand_mean': 20, 'demand_std': 5, 'holding_cost_per_unit': 0.1, 'initial_inventory': 100, ...}

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ScenarioContext":
        """Create context from dictionary."""
>       return cls(**data)
               ^^^^^^^^^^^
E       TypeError: ScenarioContext.__init__() got an unexpected keyword argument 'initial_inventory'

benchmarking\scenarios\refined_scenarios.py:122: TypeError
_ ERROR at setup of TestSystemIntegration.test_competitive_scenario_initialization _

self = <tests.test_system_integration.TestSystemIntegration object at 0x0000016F8B22AAB0>

    @pytest.fixture
    def competitive_scenario(self) -> CompetitiveScenario:
        """
        Create a test CompetitiveScenario.
    
        Returns:
            Test CompetitiveScenario
        """
        config = ScenarioConfig(
            name="test-competitive",
            description="Test competitive scenario",
            difficulty=ScenarioDifficulty.HARD,
            scenario_type=ScenarioType.COMPETITIVE,
            max_ticks=50,
            time_limit=30.0,
            initial_state={
                "num_competitors": 3,
                "market_size": 1000,
                "competitor_strategies": ["aggressive", "moderate", "conservative"],
                "price_sensitivity": 1.0,
                "quality_sensitivity": 0.8,
                "marketing_sensitivity": 0.5,
                "agent_price": 10.0,
                "agent_quality": 0.7,
                "agent_marketing": 0.5,
            },
            success_criteria={"min_market_share": 0.3},
            failure_criteria={"max_market_share": 0.1},
        )
>       return CompetitiveScenario(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\test_system_integration.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\scenarios\refined_scenarios.py:753: in __init__
    super().__init__(config)
benchmarking\scenarios\refined_scenarios.py:193: in __init__
    self.context = ScenarioContext.from_dict(config.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'benchmarking.scenarios.refined_scenarios.ScenarioContext'>
data = {'agent_marketing': 0.5, 'agent_price': 10.0, 'agent_quality': 0.7, 'competitor_strategies': ['aggressive', 'moderate', 'conservative'], ...}

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ScenarioContext":
        """Create context from dictionary."""
>       return cls(**data)
               ^^^^^^^^^^^
E       TypeError: ScenarioContext.__init__() got an unexpected keyword argument 'num_competitors'

benchmarking\scenarios\refined_scenarios.py:122: TypeError
_ ERROR at setup of TestSystemIntegration.test_competitive_scenario_execution _

self = <tests.test_system_integration.TestSystemIntegration object at 0x0000016F8B22AC90>

    @pytest.fixture
    def competitive_scenario(self) -> CompetitiveScenario:
        """
        Create a test CompetitiveScenario.
    
        Returns:
            Test CompetitiveScenario
        """
        config = ScenarioConfig(
            name="test-competitive",
            description="Test competitive scenario",
            difficulty=ScenarioDifficulty.HARD,
            scenario_type=ScenarioType.COMPETITIVE,
            max_ticks=50,
            time_limit=30.0,
            initial_state={
                "num_competitors": 3,
                "market_size": 1000,
                "competitor_strategies": ["aggressive", "moderate", "conservative"],
                "price_sensitivity": 1.0,
                "quality_sensitivity": 0.8,
                "marketing_sensitivity": 0.5,
                "agent_price": 10.0,
                "agent_quality": 0.7,
                "agent_marketing": 0.5,
            },
            success_criteria={"min_market_share": 0.3},
            failure_criteria={"max_market_share": 0.1},
        )
>       return CompetitiveScenario(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\test_system_integration.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\scenarios\refined_scenarios.py:753: in __init__
    super().__init__(config)
benchmarking\scenarios\refined_scenarios.py:193: in __init__
    self.context = ScenarioContext.from_dict(config.initial_state)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'benchmarking.scenarios.refined_scenarios.ScenarioContext'>
data = {'agent_marketing': 0.5, 'agent_price': 10.0, 'agent_quality': 0.7, 'competitor_strategies': ['aggressive', 'moderate', 'conservative'], ...}

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ScenarioContext":
        """Create context from dictionary."""
>       return cls(**data)
               ^^^^^^^^^^^
E       TypeError: ScenarioContext.__init__() got an unexpected keyword argument 'num_competitors'

benchmarking\scenarios\refined_scenarios.py:122: TypeError
____ ERROR at setup of TestSystemIntegration.test_full_system_integration _____

self = <tests.test_system_integration.TestSystemIntegration object at 0x0000016F8B22B110>

    @pytest.fixture
    def test_config(self) -> PydanticConfig:
        """
        Create a test configuration.
    
        Returns:
            Test configuration
        """
        return PydanticConfig(
>           llm_config=LLMConfig(
                model="gpt-3.5-turbo", temperature=0.7, max_tokens=1000, api_key="test-api-key"
            ),
            agent_configs=[
                AgentConfig(
                    name="test-agent",
                    type="diy",
                    config={
                        "llm_config": {
                            "model": "gpt-3.5-turbo",
                            "temperature": 0.7,
                            "api_key": "test-api-key",
                        },
                        "system_prompt": "You are a helpful assistant.",
                    },
                )
            ],
            benchmark_config={
                "max_ticks": 100,
                "time_limit": 60.0,
                "metrics": ["revenue", "profit", "costs"],
            },
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for LLMConfig
E       name
E         Field required [type=missing, input_value={'model': 'gpt-3.5-turbo'...pi_key': 'test-api-key'}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_system_integration.py:57: ValidationError
______ ERROR at setup of TestSystemIntegration.test_api_server_lifespan _______

self = <tests.test_system_integration.TestSystemIntegration object at 0x0000016F8B22B2F0>

    @pytest.fixture
    def test_config(self) -> PydanticConfig:
        """
        Create a test configuration.
    
        Returns:
            Test configuration
        """
        return PydanticConfig(
>           llm_config=LLMConfig(
                model="gpt-3.5-turbo", temperature=0.7, max_tokens=1000, api_key="test-api-key"
            ),
            agent_configs=[
                AgentConfig(
                    name="test-agent",
                    type="diy",
                    config={
                        "llm_config": {
                            "model": "gpt-3.5-turbo",
                            "temperature": 0.7,
                            "api_key": "test-api-key",
                        },
                        "system_prompt": "You are a helpful assistant.",
                    },
                )
            ],
            benchmark_config={
                "max_ticks": 100,
                "time_limit": 60.0,
                "metrics": ["revenue", "profit", "costs"],
            },
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for LLMConfig
E       name
E         Field required [type=missing, input_value={'model': 'gpt-3.5-turbo'...pi_key': 'test-api-key'}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.11/v/missing

tests\test_system_integration.py:57: ValidationError
_________ ERROR at setup of TestBenchmarkEngine.test_create_benchmark _________

self = <tests.unit.benchmarking.test_engine_new_api.TestBenchmarkEngine object at 0x0000016F8B2820C0>

    @pytest.fixture
    def mock_config(self):
        """Create a mock benchmark configuration."""
        return BenchmarkConfig(
            name="test_benchmark",
            description="Test benchmark for unit testing",
            scenarios=[
>               ScenarioConfig(
                    name="test_scenario",
                    description="Test scenario",
                    domain="test",
                    duration_ticks=10,
                )
            ],
            agents=[AgentConfig(agent_id="test_agent", framework="test", config={})],
            services={},
            metric_weights={},
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for ScenarioConfig
E       domain
E         Extra inputs are not permitted [type=extra_forbidden, input_value='test', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden

tests\unit\benchmarking\test_engine_new_api.py:37: ValidationError
_ ERROR at setup of TestBenchmarkEngine.test_calculate_scenario_kpis_with_results _

self = <tests.unit.benchmarking.test_engine_new_api.TestBenchmarkEngine object at 0x0000016F8B2A03B0>

    @pytest.fixture
    def mock_scenario_result(self):
        """Create a mock scenario result."""
>       return ScenarioResult(
            scenario_name="test_scenario",
            success=True,
            message="Test completed successfully",
            metrics={"test_agent": {"score": 0.85}},
            start_time=1000.0,
            end_time=1030.0,
        )
E       TypeError: ScenarioResult.__init__() got an unexpected keyword argument 'message'

tests\unit\benchmarking\test_engine_new_api.py:70: TypeError
================================== FAILURES ===================================
___________ test_config_validation_parallelism_and_empty_scenarios ____________

    @pytest.mark.unit
    def test_config_validation_parallelism_and_empty_scenarios():
        # Bad parallelism
        with pytest.raises(ValidationError):
            EngineConfig(
                scenarios=[ScenarioSpec(key="x")],
                runners=[RunnerSpec(key="diy", config={"agent_id": "a"})],
                parallelism=0,
            )
        # Empty scenarios fails
>       with pytest.raises(ValidationError):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       Failed: DID NOT RAISE <class 'pydantic_core._pydantic_core.ValidationError'>

tests\benchmarking\test_engine_unit.py:33: Failed
______ TestCompetitorEventFlow.test_competitor_manager_publishes_on_tick ______

self = <tests.test_competitor_integration.TestCompetitorEventFlow object at 0x0000016F8A6F46E0>
event_bus = <fba_bench_core.event_bus.EventBus object at 0x0000016F8BF19C40>
competitor_manager = <fba_bench_core.services.competitor_manager.CompetitorManager object at 0x0000016F8C145DC0>
test_competitors = [<fba_bench_core.models.competitor.Competitor object at 0x0000016F8C145220>, <fba_bench_core.models.competitor.Competitor object at 0x0000016F8C145280>, <fba_bench_core.models.competitor.Competitor object at 0x0000016F8C145520>]

    @pytest.mark.asyncio
    async def test_competitor_manager_publishes_on_tick(
        self, event_bus, competitor_manager, test_competitors
    ):
        """Test that CompetitorManager publishes CompetitorPricesUpdated on TickEvent."""
        # Setup competitors
        for competitor in test_competitors:
            # Add required attributes for event-driven flow
            competitor.competitor_id = competitor.asin
            competitor.is_active = True
            competitor.last_price_change = 0
            competitor_manager.add_competitor(competitor)
    
        # Start competitor manager
        competitor_manager.event_bus = event_bus
        await competitor_manager.start()
    
        # Create a collector for published events
        published_events = []
    
        async def event_collector(event):
            published_events.append(event)
    
        await event_bus.subscribe("CompetitorPricesUpdated", event_collector)
    
        # Publish TickEvent
        tick_event = TickEvent(
            event_id="tick_001",
            timestamp=datetime.now(),
            tick_number=1,
            simulation_time=datetime.now(),
            metadata={
                "market_conditions": {
                    "our_price": Money.from_dollars(20.00),
                    "sales_velocity": 1.0,
                    "market_trend": "stable",
                }
            },
        )
    
        await event_bus.publish(tick_event)
    
        # Wait for event processing
        await asyncio.sleep(0.1)
    
        # Verify CompetitorPricesUpdated was published
>       assert len(published_events) == 1
E       assert 0 == 1
E        +  where 0 = len([])

tests\test_competitor_integration.py:175: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:26:21.199 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized.
2025-09-16 01:26:21.200 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
2025-09-16 01:26:21.200 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager initialized
----------------------------- Captured log setup ------------------------------
INFO     fba_bench_core.services.world_store:world_store.py:90 InMemoryStorageBackend initialized.
INFO     fba_bench_core.services.world_store:world_store.py:353 WorldStore initialized - ready for multi-agent command processing
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:101 CompetitorManager initialized
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:26:21.201 | INFO | fba_bench_core.services.competitor_manager | [req=-] | Competitor B001 added.
2025-09-16 01:26:21.201 | INFO | fba_bench_core.services.competitor_manager | [req=-] | Competitor B002 added.
2025-09-16 01:26:21.201 | INFO | fba_bench_core.services.competitor_manager | [req=-] | Competitor B003 added.
2025-09-16 01:26:21.201 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager started and subscribed to TickEvent
2025-09-16 01:26:21.202 | INFO | fba_events.bus | [req=-] | Event published
2025-09-16 01:26:21.202 | ERROR | fba_bench_core.services.competitor_manager | [req=-] | Error processing TickEvent in CompetitorManager: 'dict' object has no attribute 'tick_number'
Traceback (most recent call last):
  File "C:\Users\admin\Downloads\fba\src\fba_bench_core\services\competitor_manager.py", line 216, in _handle_tick_event
    f"CompetitorManager received TickEvent for tick {event.tick_number}. Updating competitors..."
                                                     ^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'tick_number'
------------------------------ Captured log call ------------------------------
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:175 Competitor B001 added.
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:175 Competitor B002 added.
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:175 Competitor B003 added.
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:120 CompetitorManager started and subscribed to TickEvent
INFO     fba_events.bus:bus.py:290 Event published
ERROR    fba_bench_core.services.competitor_manager:competitor_manager.py:289 Error processing TickEvent in CompetitorManager: 'dict' object has no attribute 'tick_number'
Traceback (most recent call last):
  File "C:\Users\admin\Downloads\fba\src\fba_bench_core\services\competitor_manager.py", line 216, in _handle_tick_event
    f"CompetitorManager received TickEvent for tick {event.tick_number}. Updating competitors..."
                                                     ^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'tick_number'
____________ TestEventDrivenCore.test_event_bus_publish_subscribe _____________

self = <tests.test_event_integration.TestEventDrivenCore object at 0x0000016F8A708560>
event_bus = <fba_bench_core.event_bus.EventBus object at 0x0000016F8E0B13D0>

    @pytest.mark.asyncio
    async def test_event_bus_publish_subscribe(self, event_bus):
        """Test basic EventBus publish/subscribe functionality."""
        collector = EventCollector()
    
        # Subscribe to TickEvent
        await event_bus.subscribe(TickEvent, collector.collect_event)
    
        # Create and publish a test event
        test_event = TickEvent(
            event_id="test_tick_1",
            timestamp=datetime.now(),
            tick_number=1,
            simulation_time=datetime.now(),
            metadata={"test": True},
        )
    
        await event_bus.publish(test_event)
    
        # Wait a bit for event processing
        await asyncio.sleep(0.1)
    
        # Verify event was received
        assert len(collector.collected_events) == 1
>       assert collector.event_counts["TickEvent"] == 1
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'TickEvent'

tests\test_event_integration.py:128: KeyError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:26:26.149 | INFO | fba_events.bus | [req=-] | Event published
------------------------------ Captured log call ------------------------------
INFO     fba_events.bus:bus.py:290 Event published
______ TestEventDrivenCore.test_simulation_orchestrator_tick_generation _______

self = <tests.test_event_integration.TestEventDrivenCore object at 0x0000016F8A70B260>
event_bus = <fba_bench_core.event_bus.EventBus object at 0x0000016F8C144920>
orchestrator = <fba_bench_core.simulation_orchestrator.SimulationOrchestrator object at 0x0000016F8E0B1460>

    @pytest.mark.asyncio
    async def test_simulation_orchestrator_tick_generation(self, event_bus, orchestrator):
        """Test that SimulationOrchestrator generates TickEvents."""
        collector = EventCollector()
    
        # Subscribe to TickEvents
        await event_bus.subscribe(TickEvent, collector.collect_event)
    
        # Start orchestrator
        await orchestrator.start(event_bus)
    
        # Wait for simulation to complete (5 ticks at 0.1s intervals)
        await asyncio.sleep(1.0)
    
        # Stop orchestrator
        await orchestrator.stop()
    
        # Verify TickEvents were generated
        tick_events = collector.get_events_of_type(TickEvent)
>       assert len(tick_events) == 5
E       assert 0 == 5
E        +  where 0 = len([])

tests\test_event_integration.py:152: AssertionError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:26:26.266 | INFO | fba_events.bus | [req=-] | Event published
2025-09-16 01:26:26.377 | INFO | fba_events.bus | [req=-] | Event published
2025-09-16 01:26:26.485 | INFO | fba_events.bus | [req=-] | Event published
2025-09-16 01:26:26.595 | INFO | fba_events.bus | [req=-] | Event published
2025-09-16 01:26:26.703 | INFO | fba_events.bus | [req=-] | Event published
------------------------------ Captured log call ------------------------------
INFO     fba_events.bus:bus.py:290 Event published
INFO     fba_events.bus:bus.py:290 Event published
INFO     fba_events.bus:bus.py:290 Event published
INFO     fba_events.bus:bus.py:290 Event published
INFO     fba_events.bus:bus.py:290 Event published
_____________________ test_experiment_cli_sequential_run ______________________

tmp_path = WindowsPath('C:/Users/admin/AppData/Local/Temp/pytest-of-admin/pytest-49/test_experiment_cli_sequential0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x0000016F8E0AB410>

    def test_experiment_cli_sequential_run(tmp_path, monkeypatch):
        # Prepare a minimal sweep.yaml with a single run
        config = {
            "experiment_name": "prod_cli_test",
            "description": "unit test sweep for production CLI",
            "base_parameters": {"scenario_file": "dummy_scenario.yaml"},
            "parameter_sweep": {},
            "output": {},
        }
        config_path = Path(tmp_path) / "sweep.yaml"
        _safe_dump(config, config_path)
    
        # Patch ScenarioEngine.run_simulation to avoid real environment
        def mock_run(self, scenario_file, agents):
            return {
                "final_state": {"ok": True},
                "simulation_duration": 1,
                "metrics": {"total_profit": 123.45},
            }
    
        monkeypatch.setattr(ec.ScenarioEngine, "run_simulation", mock_run, raising=True)
    
        exp_config, run_items_iter = ec._load_config_and_expand_run_params(str(config_path))
        run_items = list(run_items_iter)
        assert len(run_items) == 1  # ensure one run
    
        results_dir = Path(tmp_path) / "results"
        results_dir.mkdir()
        run_numbers = run_items
    
        successes = ec._parallel_run(str(config_path), run_numbers, str(results_dir), parallel=1)
>       assert successes == 1
E       assert None == 1

tests\test_experiment_cli_production.py:45: AssertionError
___________ TestLLMBots.test_all_llm_bots_use_correct_model_params ____________

self = <tests.test_llm_bots.TestLLMBots object at 0x0000016F8A8EFDA0>

    @pytest.mark.asyncio
    async def test_all_llm_bots_use_correct_model_params(self):
        # This test ensures that different bots pass their specific model_params correctly.
    
        # Setup common mocks
        mock_llm_client_generic = AsyncMock(spec=BaseLLMClient)
        mock_llm_client_generic.generate_response.return_value = {
            "choices": [
                {"message": {"content": '{"actions": [], "reasoning": "", "confidence": 0.5}'}}
            ]
        }
        mock_llm_client_generic.get_token_count.return_value = 10
    
>       mock_prompt_adapter_generic = MockPromptAdapter(MockWorldStore(), MockBudgetEnforcer())
                                      ^^^^^^^^^^^^^^^^^
E       NameError: name 'MockPromptAdapter' is not defined

tests\test_llm_bots.py:318: NameError
______________________ test_metrics_endpoints_end_to_end ______________________

    @pytest.mark.asyncio
    async def test_metrics_endpoints_end_to_end():
        # Setup EventBus and services
        event_bus = EventBus()
        await event_bus.start()
    
        audit = FinancialAuditService(config={"halt_on_violation": False})
        ledger = DoubleEntryLedgerService(config={})
        bsr = BsrEngineV3Service()
        fee_agg = FeeMetricsAggregatorService()
    
        # Wire audit to ledger for position sync
        audit.ledger_service = ledger
    
        # Start services (subscription wiring)
        await audit.start(event_bus)
        await ledger.start(event_bus)
        await bsr.start(event_bus)
        await fee_agg.start(event_bus)
    
        # Publish competitor update to seed BSR market EMA
        asin_a = "B00METRICS1"
        asin_b = "B00METRICS2"
        comp_event = CompetitorPricesUpdated(
            event_id="cmp-1",
            timestamp=datetime.now(timezone.utc),
            tick_number=1,
            competitors=[
                CompetitorState(
                    asin=asin_a, price=Money.from_dollars(24.99), bsr=1200, sales_velocity=2.5
                ),
                CompetitorState(
                    asin=asin_b, price=Money.from_dollars(29.99), bsr=800, sales_velocity=3.0
                ),
            ],
            market_summary={"note": "seed snapshot"},
        )
        await event_bus.publish(comp_event)
    
        # Publish a SaleOccurred with fee breakdown to seed fee aggregator and audit
        sale_asin = "B00METRICSX"
        unit_price = Money.from_dollars(25.00)
        units_sold = 2
        total_revenue = unit_price * units_sold  # $50.00
        referral_fee = Money.from_dollars(3.00)
        fba_fee = Money.from_dollars(4.00)
        fee_breakdown = {"referral": referral_fee, "fba": fba_fee}
        total_fees = referral_fee + fba_fee  # $7.00
        cost_basis = Money.from_dollars(20.00)  # $20.00
        total_profit = total_revenue - total_fees - cost_basis  # $23.00
    
        sale_event = SaleOccurred(
            event_id="sale-1",
            timestamp=datetime.now(timezone.utc),
            asin=sale_asin,
            units_sold=units_sold,
            units_demanded=3,
            unit_price=unit_price,
            total_revenue=total_revenue,
            total_fees=total_fees,
            total_profit=total_profit,
            cost_basis=cost_basis,
            trust_score_at_sale=0.92,
            bsr_at_sale=900,
            conversion_rate=2 / 3,
            fee_breakdown=fee_breakdown,
        )
        await event_bus.publish(sale_event)
    
        # Allow async handlers to process
        await asyncio.sleep(0.1)
    
        # Build ASGI app from extended DashboardAPIService
        dashboard = DashboardAPIService(
            event_bus=event_bus,
            audit_service=audit,
            ledger_service=ledger,
            bsr_service=bsr,
            fee_aggregator=fee_agg,
        )
        app = dashboard.build_app()
    
        async with httpx.AsyncClient(app=app, base_url="http://test") as client:
            # Audit endpoint
            r = await client.get("/api/metrics/audit")
            assert r.status_code == 200
            audit_json = r.json()
            for key in [
                "processed_transactions",
                "total_violations",
                "critical_violations",
                "total_revenue_audited",
                "total_fees_audited",
                "total_profit_audited",
                "current_position",
                "audit_enabled",
                "halt_on_violation",
                "tolerance_cents",
            ]:
                assert key in audit_json
            assert isinstance(audit_json["current_position"]["accounting_identity_valid"], bool)
            # Money fields are strings with "$" prefix
            assert isinstance(audit_json["total_revenue_audited"], str) and audit_json[
                "total_revenue_audited"
            ].startswith("$")
            assert isinstance(audit_json["total_fees_audited"], str) and audit_json[
                "total_fees_audited"
            ].startswith("$")
            assert isinstance(audit_json["total_profit_audited"], str) and audit_json[
                "total_profit_audited"
            ].startswith("$")
    
            # Ledger endpoint
            r = await client.get("/api/metrics/ledger")
            assert r.status_code == 200
            ledger_json = r.json()
            for key in [
                "cash",
                "inventory_value",
                "accounts_receivable",
                "accounts_payable",
                "accrued_liabilities",
                "total_assets",
                "total_liabilities",
                "total_equity",
                "accounting_identity_valid",
                "identity_difference",
                "timestamp",
            ]:
                assert key in ledger_json
            assert isinstance(ledger_json["cash"], str) and ledger_json["cash"].startswith("$")
            # timestamp is ISO
            try:
                datetime.fromisoformat(ledger_json["timestamp"].replace("Z", "+00:00"))
            except Exception:
                pytest.fail("Ledger timestamp is not ISO formatted")
    
            # BSR endpoint
            r = await client.get("/api/metrics/bsr")
            assert r.status_code == 200
            bsr_json = r.json()
            assert "products" in bsr_json and isinstance(bsr_json["products"], list)
            # Indices are numbers or null
            for p in bsr_json["products"]:
                for idx_key in ["velocity_index", "conversion_index", "composite_index"]:
                    v = p.get(idx_key)
                    assert v is None or isinstance(v, (int, float))
            # Market EMA fields (if present) serialized as strings
            if "market_ema_velocity" in bsr_json:
                assert isinstance(bsr_json["market_ema_velocity"], str)
            if "market_ema_conversion" in bsr_json:
                assert isinstance(bsr_json["market_ema_conversion"], str)
            if "competitor_count" in bsr_json:
                assert isinstance(bsr_json["competitor_count"], int)
    
            # Fees endpoint
            r = await client.get("/api/metrics/fees")
            assert r.status_code == 200
            fees_json = r.json()
            # Should include the fee types we published
>           assert "referral" in fees_json and "fba" in fees_json
E           AssertionError: assert ('referral' in {})

tests\test_metrics_endpoints.py:176: AssertionError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:01.850 | INFO | financial_audit | [req=-] | FinancialAuditService initialized with starting position: Cash=$10,000.00 USD, Inventory=$5,000.00 USD, Equity=$15,000.00 USD
2025-09-16 01:27:01.850 | INFO | fba_bench_core.services.double_entry_ledger_service | [req=-] | Initialized chart of accounts with 17 accounts
2025-09-16 01:27:01.850 | INFO | fba_bench_core.services.double_entry_ledger_service | [req=-] | DoubleEntryLedgerService initialized with chart of accounts
2025-09-16 01:27:01.850 | INFO | financial_audit | [req=-] | FinancialAuditService started and subscribed to SaleOccurred events
2025-09-16 01:27:01.851 | INFO | fba_bench_core.services.double_entry_ledger_service | [req=-] | DoubleEntryLedgerService started and subscribed to events
2025-09-16 01:27:01.851 | INFO | fba_events.bus | [req=-] | Event published
2025-09-16 01:27:01.852 | INFO | fba_events.bus | [req=-] | Event published
------------------------------ Captured log call ------------------------------
INFO     financial_audit:financial_audit.py:130 FinancialAuditService initialized with starting position: Cash=$10,000.00 USD, Inventory=$5,000.00 USD, Equity=$15,000.00 USD
INFO     fba_bench_core.services.double_entry_ledger_service:double_entry_ledger_service.py:321 Initialized chart of accounts with 17 accounts
INFO     fba_bench_core.services.double_entry_ledger_service:double_entry_ledger_service.py:159 DoubleEntryLedgerService initialized with chart of accounts
INFO     financial_audit:financial_audit.py:139 FinancialAuditService started and subscribed to SaleOccurred events
INFO     fba_bench_core.services.double_entry_ledger_service:double_entry_ledger_service.py:354 DoubleEntryLedgerService started and subscribed to events
INFO     fba_events.bus:bus.py:290 Event published
INFO     fba_events.bus:bus.py:290 Event published
___________________________ test_multi_skill_agent ____________________________

    @pytest.mark.asyncio
    async def test_multi_skill_agent():
        """Test the enhanced AdvancedAgent with multi-skill capabilities."""
    
        # Create agent configuration with multi-skill system enabled
>       agent_config = AgentConfig(
            agent_id="test_agent_001",
            target_asin="B123TEST",
            strategy="profit_maximizer",
            enable_multi_skill_system=True,
            skill_config_template="balanced",
            enable_supply_management=True,
            enable_marketing_management=True,
            enable_customer_service=True,
            enable_financial_analysis=True,
        )
E       TypeError: AgentConfig.__init__() got an unexpected keyword argument 'strategy'

tests\test_multi_skill_agent.py:33: TypeError
___________ TestObservabilityEnhancements.test_agent_error_handler ____________

self = <tests.test_observability_enhancements.TestObservabilityEnhancements testMethod=test_agent_error_handler>

    def test_agent_error_handler(self):
        # Test validate_command_syntax
        valid_json_cmd = '{"tool_name": "test_tool", "parameters": {"param1": "value"}}'
>       is_valid, msg = self.error_handler.validate_command_syntax(valid_json_cmd)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'AgentErrorHandler' object has no attribute 'validate_command_syntax'

tests\test_observability_enhancements.py:103: AttributeError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:02.627 | INFO | root | [req=-] | ObservabilityAlertSystem initialized.
------------------------------ Captured log call ------------------------------
INFO     root:alert_system.py:57 ObservabilityAlertSystem initialized.
________ TestObservabilityEnhancements.test_observability_alert_system ________

self = <tests.test_observability_enhancements.TestObservabilityEnhancements testMethod=test_observability_alert_system>

    def test_observability_alert_system(self):
        # Test performance monitoring
        self.alert_system.monitor_performance_metrics({"latency_ms": 150}, {"latency_ms": 100})
        self.assertEqual(len(self.mock_alerts), 1)
        self.assertEqual(self.mock_alerts[0]["type"], "performance_alert")
>       self.assertEqual(self.mock_alerts[0]["severity"], "Critical")
E       AssertionError: 'CRITICAL' != 'Critical'
E       - CRITICAL
E       + Critical

tests\test_observability_enhancements.py:272: AssertionError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:02.639 | INFO | root | [req=-] | ObservabilityAlertSystem initialized.
2025-09-16 01:27:02.639 | CRITICAL | root | [req=-] | ALERT TRIGGERED - Rule: performance_alert, Severity: CRITICAL, Details: {"metric": "latency_ms", "current_value": 150, "threshold": 100, "message": "Performance for 'latency_ms' (150.00) exceeded threshold (100.00)."}
2025-09-16 01:27:02.639 | INFO | root | [req=-] | Alert dispatched via callback.
------------------------------ Captured log call ------------------------------
INFO     root:alert_system.py:57 ObservabilityAlertSystem initialized.
CRITICAL root:alert_system.py:228 ALERT TRIGGERED - Rule: performance_alert, Severity: CRITICAL, Details: {"metric": "latency_ms", "current_value": 150, "threshold": 100, "message": "Performance for 'latency_ms' (150.00) exceeded threshold (100.00)."}
INFO     root:alert_system.py:241 Alert dispatched via callback.
_________ TestObservabilityEnhancements.test_smart_command_processor __________

self = <tests.test_observability_enhancements.TestObservabilityEnhancements testMethod=test_smart_command_processor>

    def test_smart_command_processor(self):
        clean_command = '{"tool_name": "create_product", "parameters": {"name": "WidgetA"}}'
>       processed = self.command_processor.process_agent_command(clean_command, {})
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'SmartCommandProcessor' object has no attribute 'process_agent_command'

tests\test_observability_enhancements.py:226: AttributeError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:02.650 | INFO | root | [req=-] | ObservabilityAlertSystem initialized.
------------------------------ Captured log call ------------------------------
INFO     root:alert_system.py:57 ObservabilityAlertSystem initialized.
______________ TestObservabilityEnhancements.test_trace_analyzer ______________

self = <tests.test_observability_enhancements.TestObservabilityEnhancements testMethod=test_trace_analyzer>

    def test_trace_analyzer(self):
        # Mock trace data for testing
        trace_data = [
            {"event_type": "info", "timestamp": 100, "details": "sim started"},
            {
                "event_type": "agent_action",
                "timestamp": 105,
                "details": {"agent_id": "A1", "action": "observe"},
            },
            {"event_type": "warning", "timestamp": 108, "details": "low inventory alert"},
            {"event_type": "error", "timestamp": 110, "details": "tool_call_failed: invalid param"},
            {
                "event_type": "agent_action",
                "timestamp": 112,
                "details": {"agent_id": "A1", "action": "think"},
            },
        ]
        failure_point = {
            "event_type": "simulation_crash",
            "timestamp": 111,
            "reason": "unhandled exception",
        }
    
>       analysis = self.trace_analyzer.analyze_simulation_failure(trace_data, failure_point)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TraceAnalyzer' object has no attribute 'analyze_simulation_failure'

tests\test_observability_enhancements.py:168: AttributeError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:02.659 | INFO | root | [req=-] | ObservabilityAlertSystem initialized.
------------------------------ Captured log call ------------------------------
INFO     root:alert_system.py:57 ObservabilityAlertSystem initialized.
_______________ TestPersonaIntegration.test_persona_assignment ________________

self = <tests.test_persona_integration.TestPersonaIntegration object at 0x0000016F8A998560>
competitor_manager = <fba_bench_core.services.competitor_manager.CompetitorManager object at 0x0000016F8C73B200>
mock_competitors = [<tests.test_persona_integration.MockCompetitor object at 0x0000016F8C73B020>, <tests.test_persona_integration.MockCompetitor object at 0x0000016F8C73AED0>, <tests.test_persona_integration.MockCompetitor object at 0x0000016F8C73AC90>]

    @pytest.mark.asyncio
    async def test_persona_assignment(self, competitor_manager, mock_competitors):
        """Test that personas are correctly assigned to competitors."""
        # Add competitors with specific personas
        irrational_slasher = IrrationalSlasher("comp_1", Money.from_dollars(14.00))
        slow_follower = SlowFollower("comp_2", Money.from_dollars(15.00))
    
        competitor_manager.add_competitor(mock_competitors[0], irrational_slasher)
        competitor_manager.add_competitor(mock_competitors[1], slow_follower)
        competitor_manager.add_competitor(mock_competitors[2])  # Auto-assign
    
        # Verify persona assignments
>       assert isinstance(competitor_manager.get_competitor_persona("comp_1"), IrrationalSlasher)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CompetitorManager' object has no attribute 'get_competitor_persona'

tests\test_persona_integration.py:90: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:27:02.671 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized.
2025-09-16 01:27:02.671 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
2025-09-16 01:27:02.671 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager initialized
2025-09-16 01:27:02.671 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager started and subscribed to TickEvent
----------------------------- Captured log setup ------------------------------
INFO     fba_bench_core.services.world_store:world_store.py:90 InMemoryStorageBackend initialized.
INFO     fba_bench_core.services.world_store:world_store.py:353 WorldStore initialized - ready for multi-agent command processing
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:101 CompetitorManager initialized
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:120 CompetitorManager started and subscribed to TickEvent
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:02.672 | INFO | fba_bench_core.services.competitor_manager | [req=-] | Competitor ASIN001 added.
2025-09-16 01:27:02.672 | INFO | fba_bench_core.services.competitor_manager | [req=-] | Competitor ASIN002 added.
2025-09-16 01:27:02.672 | INFO | fba_bench_core.services.competitor_manager | [req=-] | Competitor ASIN003 added.
------------------------------ Captured log call ------------------------------
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:175 Competitor ASIN001 added.
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:175 Competitor ASIN002 added.
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:175 Competitor ASIN003 added.
-------------------------- Captured stdout teardown ---------------------------
2025-09-16 01:27:02.689 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager stopped
---------------------------- Captured log teardown ----------------------------
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:124 CompetitorManager stopped
___________ TestPersonaIntegration.test_irrational_slasher_behavior ___________

self = <tests.test_persona_integration.TestPersonaIntegration object at 0x0000016F8A998890>
competitor_manager = <fba_bench_core.services.competitor_manager.CompetitorManager object at 0x0000016F8C738680>
mock_competitors = [<tests.test_persona_integration.MockCompetitor object at 0x0000016F8C792B70>, <tests.test_persona_integration.MockCompetitor object at 0x0000016F8C793B30>, <tests.test_persona_integration.MockCompetitor object at 0x0000016F8C791700>]

    @pytest.mark.asyncio
    async def test_irrational_slasher_behavior(self, competitor_manager, mock_competitors):
        """Test IrrationalSlasher persona behavior."""
        # Create slasher with high slash probability for testing
        slasher = IrrationalSlasher("comp_1", Money.from_dollars(14.00))
        slasher.slash_probability = 1.0  # Force slashing for test
    
        competitor_manager.add_competitor(mock_competitors[0], slasher)
    
        # Create market conditions that trigger slashing
>       tick_event = TickEvent(
            event_id="test_tick_1",
            timestamp=datetime.now(),
            tick_number=1,
            metadata={
                "market_conditions": {
                    "our_price": Money.from_dollars(19.00),
                    "sales_velocity": 0.3,  # Low sales velocity
                    "market_trend": "falling",
                }
            },
        )
E       TypeError: TickEvent.__init__() missing 1 required positional argument: 'simulation_time'

tests\test_persona_integration.py:110: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:27:02.692 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized.
2025-09-16 01:27:02.692 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
2025-09-16 01:27:02.692 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager initialized
2025-09-16 01:27:02.692 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager started and subscribed to TickEvent
----------------------------- Captured log setup ------------------------------
INFO     fba_bench_core.services.world_store:world_store.py:90 InMemoryStorageBackend initialized.
INFO     fba_bench_core.services.world_store:world_store.py:353 WorldStore initialized - ready for multi-agent command processing
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:101 CompetitorManager initialized
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:120 CompetitorManager started and subscribed to TickEvent
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:02.693 | INFO | fba_bench_core.services.competitor_manager | [req=-] | Competitor ASIN001 added.
------------------------------ Captured log call ------------------------------
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:175 Competitor ASIN001 added.
-------------------------- Captured stdout teardown ---------------------------
2025-09-16 01:27:02.705 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager stopped
---------------------------- Captured log teardown ----------------------------
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:124 CompetitorManager stopped
_____________ TestPersonaIntegration.test_slow_follower_behavior ______________

self = <tests.test_persona_integration.TestPersonaIntegration object at 0x0000016F8A998BC0>
competitor_manager = <fba_bench_core.services.competitor_manager.CompetitorManager object at 0x0000016F8C791CD0>
mock_competitors = [<tests.test_persona_integration.MockCompetitor object at 0x0000016F8C793E30>, <tests.test_persona_integration.MockCompetitor object at 0x0000016F8C7931D0>, <tests.test_persona_integration.MockCompetitor object at 0x0000016F8C792C60>]

    @pytest.mark.asyncio
    async def test_slow_follower_behavior(self, competitor_manager, mock_competitors):
        """Test SlowFollower persona delayed response behavior."""
        slow_follower = SlowFollower("comp_2", Money.from_dollars(15.00))
        competitor_manager.add_competitor(mock_competitors[1], slow_follower)
    
        initial_price = mock_competitors[1].price
    
        # Process multiple ticks
        for tick_num in range(1, 6):
>           tick_event = TickEvent(
                event_id=f"test_tick_{tick_num}",
                timestamp=datetime.now(),
                tick_number=tick_num,
                metadata={
                    "market_conditions": {
                        "our_price": Money.from_dollars(19.00),
                        "sales_velocity": 1.2,
                        "market_trend": "rising",
                    }
                },
            )
E           TypeError: TickEvent.__init__() missing 1 required positional argument: 'simulation_time'

tests\test_persona_integration.py:146: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:27:02.707 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized.
2025-09-16 01:27:02.707 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
2025-09-16 01:27:02.707 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager initialized
2025-09-16 01:27:02.708 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager started and subscribed to TickEvent
----------------------------- Captured log setup ------------------------------
INFO     fba_bench_core.services.world_store:world_store.py:90 InMemoryStorageBackend initialized.
INFO     fba_bench_core.services.world_store:world_store.py:353 WorldStore initialized - ready for multi-agent command processing
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:101 CompetitorManager initialized
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:120 CompetitorManager started and subscribed to TickEvent
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:02.708 | INFO | fba_bench_core.services.competitor_manager | [req=-] | Competitor ASIN002 added.
------------------------------ Captured log call ------------------------------
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:175 Competitor ASIN002 added.
-------------------------- Captured stdout teardown ---------------------------
2025-09-16 01:27:02.720 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager stopped
---------------------------- Captured log teardown ----------------------------
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:124 CompetitorManager stopped
_________ TestPersonaIntegration.test_persona_event_flow_integration __________

self = <tests.test_persona_integration.TestPersonaIntegration object at 0x0000016F8A998EF0>
event_bus = <fba_bench_core.event_bus.EventBus object at 0x0000016F8C7907A0>
competitor_manager = <fba_bench_core.services.competitor_manager.CompetitorManager object at 0x0000016F8C790B90>
mock_competitors = [<tests.test_persona_integration.MockCompetitor object at 0x0000016F8C792510>, <tests.test_persona_integration.MockCompetitor object at 0x0000016F8C793B00>, <tests.test_persona_integration.MockCompetitor object at 0x0000016F8C790DA0>]

    @pytest.mark.asyncio
    async def test_persona_event_flow_integration(
        self, event_bus, competitor_manager, mock_competitors
    ):
        """Test complete event flow with personas: TickEvent \u2192 CompetitorManager \u2192 CompetitorPricesUpdated."""
        # Add competitors with different personas
        irrational_slasher = IrrationalSlasher("comp_1", Money.from_dollars(14.00))
        slow_follower = SlowFollower("comp_2", Money.from_dollars(15.00))
    
        competitor_manager.add_competitor(mock_competitors[0], irrational_slasher)
        competitor_manager.add_competitor(mock_competitors[1], slow_follower)
    
        # Set up event capture
        captured_events = []
    
        async def capture_competitor_updates(event: CompetitorPricesUpdated):
            captured_events.append(event)
    
        await event_bus.subscribe("CompetitorPricesUpdated", capture_competitor_updates)
    
        # Send tick event
>       tick_event = TickEvent(
            event_id="test_tick_integration",
            timestamp=datetime.now(),
            tick_number=1,
            metadata={
                "market_conditions": {
                    "our_price": Money.from_dollars(19.00),
                    "sales_velocity": 1.0,
                    "market_trend": "stable",
                }
            },
        )
E       TypeError: TickEvent.__init__() missing 1 required positional argument: 'simulation_time'

tests\test_persona_integration.py:190: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:27:02.723 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized.
2025-09-16 01:27:02.723 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
2025-09-16 01:27:02.723 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager initialized
2025-09-16 01:27:02.723 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager started and subscribed to TickEvent
----------------------------- Captured log setup ------------------------------
INFO     fba_bench_core.services.world_store:world_store.py:90 InMemoryStorageBackend initialized.
INFO     fba_bench_core.services.world_store:world_store.py:353 WorldStore initialized - ready for multi-agent command processing
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:101 CompetitorManager initialized
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:120 CompetitorManager started and subscribed to TickEvent
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:02.724 | INFO | fba_bench_core.services.competitor_manager | [req=-] | Competitor ASIN001 added.
2025-09-16 01:27:02.724 | INFO | fba_bench_core.services.competitor_manager | [req=-] | Competitor ASIN002 added.
------------------------------ Captured log call ------------------------------
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:175 Competitor ASIN001 added.
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:175 Competitor ASIN002 added.
-------------------------- Captured stdout teardown ---------------------------
2025-09-16 01:27:02.736 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager stopped
---------------------------- Captured log teardown ----------------------------
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:124 CompetitorManager stopped
_____ TestPersonaIntegration.test_persona_with_sales_service_integration ______

self = <tests.test_persona_integration.TestPersonaIntegration object at 0x0000016F8A999220>
event_bus = <fba_bench_core.event_bus.EventBus object at 0x0000016F8C791400>
competitor_manager = <fba_bench_core.services.competitor_manager.CompetitorManager object at 0x0000016F8C7909B0>
mock_competitors = [<tests.test_persona_integration.MockCompetitor object at 0x0000016F8C7901D0>, <tests.test_persona_integration.MockCompetitor object at 0x0000016F8C792600>, <tests.test_persona_integration.MockCompetitor object at 0x0000016F8C792060>]

    @pytest.mark.asyncio
    async def test_persona_with_sales_service_integration(
        self, event_bus, competitor_manager, mock_competitors
    ):
        """Test full integration: Personas \u2192 CompetitorPricesUpdated \u2192 SalesService."""
        # Set up SalesService to receive competitor events
        sales_config = {
            "base_demand": 100,
            "price_elasticity": 2.0,
            "competitor_effect_strength": 0.5,
        }
        sales_service = SalesService(sales_config)
        sales_service.event_bus = event_bus
>       await sales_service.start()
              ^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'SalesService' object has no attribute 'start'

tests\test_persona_integration.py:235: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:27:02.739 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized.
2025-09-16 01:27:02.739 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
2025-09-16 01:27:02.739 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager initialized
2025-09-16 01:27:02.739 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager started and subscribed to TickEvent
----------------------------- Captured log setup ------------------------------
INFO     fba_bench_core.services.world_store:world_store.py:90 InMemoryStorageBackend initialized.
INFO     fba_bench_core.services.world_store:world_store.py:353 WorldStore initialized - ready for multi-agent command processing
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:101 CompetitorManager initialized
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:120 CompetitorManager started and subscribed to TickEvent
-------------------------- Captured stdout teardown ---------------------------
2025-09-16 01:27:02.751 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager stopped
---------------------------- Captured log teardown ----------------------------
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:124 CompetitorManager stopped
_____________ TestPersonaIntegration.test_market_chaos_generation _____________

self = <tests.test_persona_integration.TestPersonaIntegration object at 0x0000016F8A999520>
competitor_manager = <fba_bench_core.services.competitor_manager.CompetitorManager object at 0x0000016F8C792F60>

    @pytest.mark.asyncio
    async def test_market_chaos_generation(self, competitor_manager):
        """Test that personas generate realistic market chaos and unpredictability."""
        # Create a diverse set of competitors with different personas
        competitors = []
        for i in range(6):
            competitor = MockCompetitor(f"comp_{i}", f"ASIN{i:03d}", Money.from_dollars(20.0 + i))
            competitors.append(competitor)
    
        # Add with specific persona distribution
        personas = [
            IrrationalSlasher("comp_0", Money.from_dollars(14.00)),
            IrrationalSlasher("comp_1", Money.from_dollars(14.50)),
            SlowFollower("comp_2", Money.from_dollars(15.00)),
            SlowFollower("comp_3", Money.from_dollars(15.50)),
            SlowFollower("comp_4", Money.from_dollars(16.00)),
            SlowFollower("comp_5", Money.from_dollars(16.50)),
        ]
    
        for i, (competitor, persona) in enumerate(zip(competitors, personas)):
            competitor_manager.add_competitor(competitor, persona)
    
        # Track price changes over multiple ticks
        price_history = []
    
        for tick_num in range(1, 15):  # Run for 14 ticks
>           tick_event = TickEvent(
                event_id=f"chaos_tick_{tick_num}",
                timestamp=datetime.now(),
                tick_number=tick_num,
                metadata={
                    "market_conditions": {
                        "our_price": Money.from_dollars(19.00),
                        "sales_velocity": 0.8 + (tick_num % 3) * 0.2,  # Varying conditions
                        "market_trend": ["stable", "rising", "falling"][tick_num % 3],
                    }
                },
            )
E           TypeError: TickEvent.__init__() missing 1 required positional argument: 'simulation_time'

tests\test_persona_integration.py:304: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:27:02.754 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized.
2025-09-16 01:27:02.754 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
2025-09-16 01:27:02.754 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager initialized
2025-09-16 01:27:02.754 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager started and subscribed to TickEvent
----------------------------- Captured log setup ------------------------------
INFO     fba_bench_core.services.world_store:world_store.py:90 InMemoryStorageBackend initialized.
INFO     fba_bench_core.services.world_store:world_store.py:353 WorldStore initialized - ready for multi-agent command processing
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:101 CompetitorManager initialized
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:120 CompetitorManager started and subscribed to TickEvent
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:02.755 | INFO | fba_bench_core.services.competitor_manager | [req=-] | Competitor ASIN000 added.
2025-09-16 01:27:02.755 | INFO | fba_bench_core.services.competitor_manager | [req=-] | Competitor ASIN001 added.
2025-09-16 01:27:02.755 | INFO | fba_bench_core.services.competitor_manager | [req=-] | Competitor ASIN002 added.
2025-09-16 01:27:02.756 | INFO | fba_bench_core.services.competitor_manager | [req=-] | Competitor ASIN003 added.
2025-09-16 01:27:02.756 | INFO | fba_bench_core.services.competitor_manager | [req=-] | Competitor ASIN004 added.
2025-09-16 01:27:02.756 | INFO | fba_bench_core.services.competitor_manager | [req=-] | Competitor ASIN005 added.
------------------------------ Captured log call ------------------------------
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:175 Competitor ASIN000 added.
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:175 Competitor ASIN001 added.
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:175 Competitor ASIN002 added.
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:175 Competitor ASIN003 added.
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:175 Competitor ASIN004 added.
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:175 Competitor ASIN005 added.
-------------------------- Captured stdout teardown ---------------------------
2025-09-16 01:27:02.768 | INFO | fba_bench_core.services.competitor_manager | [req=-] | CompetitorManager stopped
---------------------------- Captured log teardown ----------------------------
INFO     fba_bench_core.services.competitor_manager:competitor_manager.py:124 CompetitorManager stopped
___________________________ test_irrational_slasher ___________________________

    @pytest.mark.asyncio
    async def test_irrational_slasher():
        """Test IrrationalSlasher persona behavior."""
        print("Testing IrrationalSlasher...")
    
        slasher = IrrationalSlasher("comp_1", Money.from_dollars(14.00))
    
        # Create test state
        current_state = CompetitorState(
            competitor_id="comp_1",
            price=Money.from_dollars(20.00),
            bsr=50000,
            sales_velocity=Decimal("0.3"),  # Low sales to trigger slashing
            inventory_level=100,
            last_updated=0,
        )
    
        # Create market conditions
        market_conditions = MarketConditions(
            current_tick=1,
            current_state=current_state,
            market_competitors=[],
            market_average_price=Money.from_dollars(19.00),
            market_min_price=Money.from_dollars(18.00),
            market_max_price=Money.from_dollars(22.00),
            own_sales_velocity=0.3,
            market_trend="falling",
        )
    
        # Test multiple ticks
        for tick in range(1, 10):
            market_conditions.current_tick = tick
>           result = await slasher.act(market_conditions)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\test_personas_simple.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\test_personas_simple.py:125: in act
    return await self._slash_pricing(market_conditions)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.test_personas_simple.IrrationalSlasher object at 0x0000016F8C7917C0>
market_conditions = MarketConditions(current_tick=1, current_state=CompetitorState(competitor_id='comp_1', price=<tests.test_personas_simp..._price=<tests.test_personas_simple.Money object at 0x0000016F8C793410>, own_sales_velocity=0.3, market_trend='falling')

    async def _slash_pricing(self, market_conditions: MarketConditions) -> CompetitorState:
        slash_price = self._calculate_minimum_price()
        current_state = market_conditions.current_state
    
>       return CompetitorState(
            asin=current_state.competitor_id,
            price=slash_price,
            bsr=current_state.bsr,
            sales_velocity=float(current_state.sales_velocity * Decimal("1.5")),
            inventory_level=current_state.inventory_level,
            last_updated=market_conditions.current_tick,
        )
E       TypeError: CompetitorState.__init__() got an unexpected keyword argument 'asin'

tests\test_personas_simple.py:149: TypeError
---------------------------- Captured stdout call -----------------------------
Testing IrrationalSlasher...
___________________________ test_persona_diversity ____________________________

    @pytest.mark.asyncio
    async def test_persona_diversity():
        """Test that different personas behave differently."""
        print("Testing persona diversity...")
    
        # Create personas
        slasher = IrrationalSlasher("slasher", Money.from_dollars(14.00))
        follower = SlowFollower("follower", Money.from_dollars(15.40))
    
        # Force slasher to slash for comparison
        slasher.slash_probability = 1.0
    
        personas = [slasher, follower]
        results = {}
    
        for persona in personas:
            # Create test state
            current_state = CompetitorState(
                competitor_id=persona.competitor_id,
                price=Money.from_dollars(20.00),
                bsr=50000,
                sales_velocity=Decimal("0.4"),  # Low sales
                inventory_level=100,
                last_updated=0,
            )
    
            market_conditions = MarketConditions(
                current_tick=1,
                current_state=current_state,
                market_competitors=[],
                market_average_price=Money.from_dollars(19.00),
                market_min_price=Money.from_dollars(18.00),
                market_max_price=Money.from_dollars(22.00),
                own_sales_velocity=0.4,
                market_trend="falling",
            )
    
            # Test single action
>           result = await persona.act(market_conditions)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\test_personas_simple.py:384: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\test_personas_simple.py:127: in act
    return await self._rational_pricing(market_conditions)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.test_personas_simple.IrrationalSlasher object at 0x0000016F8C736930>
market_conditions = MarketConditions(current_tick=1, current_state=CompetitorState(competitor_id='slasher', price=<tests.test_personas_sim..._price=<tests.test_personas_simple.Money object at 0x0000016F8C735F40>, own_sales_velocity=0.4, market_trend='falling')

    async def _rational_pricing(self, market_conditions: MarketConditions) -> CompetitorState:
        current_state = market_conditions.current_state
        market_avg = market_conditions.market_average_price
    
        rational_price = market_avg * Decimal("0.95")
        final_price = Money(max(rational_price.cents, self._calculate_minimum_price().cents))
    
>       return CompetitorState(
            asin=current_state.competitor_id,
            price=final_price,
            bsr=current_state.bsr,
            sales_velocity=float(current_state.sales_velocity),
        )
E       TypeError: CompetitorState.__init__() got an unexpected keyword argument 'asin'

tests\test_personas_simple.py:165: TypeError
---------------------------- Captured stdout call -----------------------------
Testing persona diversity...
______________________ test_golden_run_matches_snapshot _______________________

sim_factory = <function sim_factory.<locals>._sim_factory at 0x0000016F8C7CEB60>
data_regression = <tests.conftest.data_regression.<locals>._DataRegression object at 0x0000016F8C7390A0>

    @pytest.mark.golden
    def test_golden_run_matches_snapshot(sim_factory, data_regression):
        """Test that a deterministic run produces identical results."""
        sim = sim_factory(seed=42, days=30)
>       audit = sim.run_and_audit(days=30)
                ^^^^^^^^^^^^^^^^^
E       AttributeError: 'coroutine' object has no attribute 'run_and_audit'

tests\test_reproducibility.py:52: AttributeError
______________________ test_golden_run_365_days_snapshot ______________________

sim_factory = <function sim_factory.<locals>._sim_factory at 0x0000016F8C7CD800>
data_regression = <tests.conftest.data_regression.<locals>._DataRegression object at 0x0000016F8C73B950>

    @pytest.mark.golden
    def test_golden_run_365_days_snapshot(sim_factory, data_regression):
        """Test longer run for comprehensive drift detection."""
        sim = sim_factory(seed=42, days=365)
>       audit = sim.run_and_audit(days=365)
                ^^^^^^^^^^^^^^^^^
E       AttributeError: 'coroutine' object has no attribute 'run_and_audit'

tests\test_reproducibility.py:65: AttributeError
__________________________ test_minimal_run_snapshot __________________________

sim_factory = <function sim_factory.<locals>._sim_factory at 0x0000016F8C7CFD80>
data_regression = <tests.conftest.data_regression.<locals>._DataRegression object at 0x0000016F8C782060>

    @pytest.mark.golden
    def test_minimal_run_snapshot(sim_factory, data_regression):
        """Test minimal 1-day run for quick verification."""
        sim = sim_factory(seed=42, days=1)
>       audit = sim.run_and_audit(days=1)
                ^^^^^^^^^^^^^^^^^
E       AttributeError: 'coroutine' object has no attribute 'run_and_audit'

tests\test_reproducibility.py:172: AttributeError
________________ test_golden_event_stream_matches_snapshot[42] ________________

sim_factory = <function sim_factory.<locals>._sim_factory at 0x0000016F8C7CF2E0>
data_regression = <tests.conftest.data_regression.<locals>._DataRegression object at 0x0000016F8C7820C0>
seed = 42

    @pytest.mark.golden
    @pytest.mark.parametrize("seed", [42])  # Use a fixed seed for predictable event streams
    async def test_golden_event_stream_matches_snapshot(sim_factory, data_regression, seed):
        """
        Test that a deterministic run produces identical event streams (golden snapshot).
        This test captures the full sequence of events and compares it to a baseline.
        """
        event_bus = get_event_bus()
        event_bus.start_recording()  # Start recording events
    
        sim = await sim_factory(
            seed=seed, days=5
        )  # Run for a shorter period for event stream manageability
        audit = sim.run_and_audit(days=5)
    
        recorded_events = event_bus.get_recorded_events()
>       event_bus.stop_recording()  # Stop recording events
        ^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'InMemoryEventBus' object has no attribute 'stop_recording'. Did you mean: 'start_recording'?

tests\test_reproducibility.py:215: AttributeError
______________________ test_ci_reproducibility_check[42] ______________________

sim_factory = <function sim_factory.<locals>._sim_factory at 0x0000016F8C659620>
seed = 42

    @pytest.mark.golden
    @pytest.mark.parametrize("seed", [42])
    async def test_ci_reproducibility_check(sim_factory, seed):
        """
        Simulates a CI-like reproducibility check by comparing an event stream
        against a (mock) baseline snapshot.
        """
        event_bus = get_event_bus()
        event_bus.start_recording()
    
        sim = await sim_factory(seed=seed, days=5)
        sim.run_and_audit(days=5)
    
        current_events = event_bus.get_recorded_events()
>       event_bus.stop_recording()
        ^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'InMemoryEventBus' object has no attribute 'stop_recording'. Did you mean: 'start_recording'?

tests\test_reproducibility.py:244: AttributeError
_______________ TestGoldenMaster.test_golden_master_comparison ________________

self = <tests.test_reproducibility_enhancements.TestGoldenMaster object at 0x0000016F8AC1BE00>
golden_master_tester = <reproducibility.golden_master.GoldenMasterTester object at 0x0000016F8C8E09B0>
sample_simulation_data = {'events': [{'data': {'tick': 0}, 'event_type': 'start', 'timestamp': '2024-01-01T00:00:00Z'}, {'data': {'price': 1000...tate': {'inventory': 100, 'profit': 1000, 'revenue': 6000}, 'metadata': {'agent_count': 3, 'simulation_duration': 120}}

    def test_golden_master_comparison(self, golden_master_tester, sample_simulation_data):
        """Test comparing runs against golden master."""
        label = "comparison_test"
    
        # Record baseline
        golden_master_tester.record_golden_master(sample_simulation_data, label)
    
        # Compare identical data (should pass)
        result = golden_master_tester.compare_against_golden(sample_simulation_data, label)
        assert result.is_identical
        assert result.is_within_tolerance
        assert len(result.critical_differences) == 0
    
        # Compare modified data (should detect differences)
        modified_data = sample_simulation_data.copy()
        modified_data["final_state"]["revenue"] = 6000
    
        result2 = golden_master_tester.compare_against_golden(modified_data, label)
>       assert not result2.is_identical
E       AssertionError: assert not True
E        +  where True = ComparisonResult(is_identical=True, is_within_tolerance=True, differences=[], critical_differences=[], warnings=[], st...erance_ms': 1.0, 'floating_point_epsilon': 1e-12, 'ignore_fields': [], 'ignore_patterns': []}}, comparison_time_ms=0.0).is_identical

tests\test_reproducibility_enhancements.py:411: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:27:03.108 | INFO | reproducibility.golden_master | [req=-] | Golden Master Tester initialized: C:\Users\admin\AppData\Local\Temp\tmpph3j6qo0
----------------------------- Captured log setup ------------------------------
INFO     reproducibility.golden_master:golden_master.py:135 Golden Master Tester initialized: C:\Users\admin\AppData\Local\Temp\tmpph3j6qo0
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:03.110 | INFO | reproducibility.golden_master | [req=-] | Golden master 'comparison_test' recorded: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpph3j6qo0\\comparison_test.golden\n2025-09-16 01:27:03.110 | INFO | reproducibility.golden_master | [req=-] | Comparison complete: \u2705 Runs are bit-perfect identical\n2025-09-16 01:27:03.111 | INFO | reproducibility.golden_master | [req=-] | Comparison complete: \u2705 Runs are bit-perfect identical
------------------------------ Captured log call ------------------------------
INFO     reproducibility.golden_master:golden_master.py:186 Golden master 'comparison_test' recorded: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpph3j6qo0\\comparison_test.golden\nINFO     reproducibility.golden_master:golden_master.py:254 Comparison complete: \u2705 Runs are bit-perfect identical\nINFO     reproducibility.golden_master:golden_master.py:254 Comparison complete: \u2705 Runs are bit-perfect identical
________________ TestGoldenMaster.test_tolerance_configuration ________________

self = <tests.test_reproducibility_enhancements.TestGoldenMaster object at 0x0000016F8AC44080>
golden_master_tester = <reproducibility.golden_master.GoldenMasterTester object at 0x0000016F8E097350>
sample_simulation_data = {'events': [{'data': {'tick': 0}, 'event_type': 'start', 'timestamp': '2024-01-01T00:00:00Z'}, {'data': {'price': 1000...te': {'inventory': 100, 'profit': 1000, 'revenue': 5000.1}, 'metadata': {'agent_count': 3, 'simulation_duration': 120}}

    def test_tolerance_configuration(self, golden_master_tester, sample_simulation_data):
        """Test tolerance configuration for comparisons."""
        label = "tolerance_test"
        golden_master_tester.record_golden_master(sample_simulation_data, label)
    
        # Modify numeric value slightly
        modified_data = sample_simulation_data.copy()
        modified_data["final_state"]["revenue"] = 5000.1  # Small difference
    
        # Should fail with strict tolerance
        strict_tolerance = ToleranceConfig(numeric_tolerance=1e-10)
        result1 = golden_master_tester.compare_against_golden(
            modified_data, label, strict_tolerance
        )
>       assert not result1.is_within_tolerance
E       AssertionError: assert not True
E        +  where True = ComparisonResult(is_identical=True, is_within_tolerance=True, differences=[], critical_differences=[], warnings=[], st...erance_ms': 1.0, 'floating_point_epsilon': 1e-12, 'ignore_fields': [], 'ignore_patterns': []}}, comparison_time_ms=0.0).is_within_tolerance

tests\test_reproducibility_enhancements.py:428: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:27:03.128 | INFO | reproducibility.golden_master | [req=-] | Golden Master Tester initialized: C:\Users\admin\AppData\Local\Temp\tmp18xze6bm
----------------------------- Captured log setup ------------------------------
INFO     reproducibility.golden_master:golden_master.py:135 Golden Master Tester initialized: C:\Users\admin\AppData\Local\Temp\tmp18xze6bm
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:03.131 | INFO | reproducibility.golden_master | [req=-] | Golden master 'tolerance_test' recorded: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp18xze6bm\\tolerance_test.golden\n2025-09-16 01:27:03.131 | INFO | reproducibility.golden_master | [req=-] | Comparison complete: \u2705 Runs are bit-perfect identical
------------------------------ Captured log call ------------------------------
INFO     reproducibility.golden_master:golden_master.py:186 Golden master 'tolerance_test' recorded: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp18xze6bm\\tolerance_test.golden\nINFO     reproducibility.golden_master:golden_master.py:254 Comparison complete: \u2705 Runs are bit-perfect identical
_____________ TestReproducibilityConfig.test_config_serialization _____________

self = <tests.test_reproducibility_enhancements.TestReproducibilityConfig object at 0x0000016F8AC1B4A0>
temp_config_dir = 'C:\\Users\\admin\\AppData\\Local\\Temp\\tmp29mj73rr'

    def test_config_serialization(self, temp_config_dir):
        """Test configuration file save/load functionality."""
        config = create_deterministic_config(master_seed=12345)
    
        config_file = Path(temp_config_dir) / "test_config.json"
    
        # Save configuration
        success = config.save_to_file(config_file)
>       assert success
E       assert False

tests\test_reproducibility_enhancements.py:578: AssertionError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:03.174 | ERROR | reproducibility.reproducibility_config | [req=-] | Failed to save configuration to C:\Users\admin\AppData\Local\Temp\tmp29mj73rr\test_config.json: asdict() should be called on dataclass instances
------------------------------ Captured log call ------------------------------
ERROR    reproducibility.reproducibility_config:reproducibility_config.py:368 Failed to save configuration to C:\Users\admin\AppData\Local\Temp\tmp29mj73rr\test_config.json: asdict() should be called on dataclass instances
_____________ TestEventSnapshots.test_enhanced_snapshot_creation ______________

self = <tests.test_reproducibility_enhancements.TestEventSnapshots object at 0x0000016F8AC45820>
sample_events = [{'data': {'tick': 0}, 'event_type': 'start', 'timestamp': '2024-01-01T00:00:00Z'}, {'data': {'action': 'price_change'... 'timestamp': '2024-01-01T00:01:00Z'}, {'data': {'tick': 2}, 'event_type': 'end', 'timestamp': '2024-01-01T00:02:00Z'}]
tmp_path = WindowsPath('C:/Users/admin/AppData/Local/Temp/pytest-of-admin/pytest-49/test_enhanced_snapshot_creatio0')

    def test_enhanced_snapshot_creation(self, sample_events, tmp_path):
        """Test enhanced snapshot creation with metadata."""
        # Set up metadata
        EventSnapshot.set_snapshot_metadata(
            simulation_mode="deterministic",
            master_seed=42,
            llm_cache_status={"cache_size": 100, "hit_ratio": 0.95},
        )
    
        # Create enhanced snapshot
        EventSnapshot.clear_llm_interactions()
        EventSnapshot.log_llm_interaction("hash1", "model1", 0.0, True, "resp1", True, True, 100.0)
    
        # Change to temp artifacts directory
        original_artifacts_dir = EventSnapshot.ARTIFACTS_DIR
        EventSnapshot.ARTIFACTS_DIR = tmp_path
    
        try:
            snapshot_path = EventSnapshot.dump_events_with_metadata(
                events=sample_events, git_sha="test123", run_id="run456"
            )
    
            assert snapshot_path is not None
            assert snapshot_path.exists()
    
            # Load and verify enhanced snapshot
            snapshot_data = EventSnapshot.load_enhanced_snapshot(snapshot_path)
>           assert "events" in snapshot_data
E           AssertionError: assert 'events' in [{'data': {'tick': 0}, 'event_type': 'start', 'timestamp': '2024-01-01T00:00:00Z'}, {'data': {'action': 'price_change'... 'timestamp': '2024-01-01T00:01:00Z'}, {'data': {'tick': 2}, 'event_type': 'end', 'timestamp': '2024-01-01T00:02:00Z'}]

tests\test_reproducibility_enhancements.py:676: AssertionError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:03.201 | ERROR | reproducibility.event_snapshots | [req=-] | Failed to save enhanced snapshot: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.
A suitable version of pyarrow or fastparquet is required for parquet support.
Trying to import the above resulted in these errors:
 - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.
 - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.
2025-09-16 01:27:03.207 | WARNING | reproducibility.event_snapshots | [req=-] | Parquet unavailable (Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.
A suitable version of pyarrow or fastparquet is required for parquet support.
Trying to import the above resulted in these errors:
 - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.
 - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.); wrote JSON snapshot to: artifacts\test123_run456.json
2025-09-16 01:27:03.207 | INFO | reproducibility.event_snapshots | [req=-] | Loaded enhanced snapshot: artifacts\test123_run456.json
------------------------------ Captured log call ------------------------------
ERROR    reproducibility.event_snapshots:event_snapshots.py:326 Failed to save enhanced snapshot: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.
A suitable version of pyarrow or fastparquet is required for parquet support.
Trying to import the above resulted in these errors:
 - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.
 - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.
WARNING  reproducibility.event_snapshots:event_snapshots.py:107 Parquet unavailable (Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.
A suitable version of pyarrow or fastparquet is required for parquet support.
Trying to import the above resulted in these errors:
 - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.
 - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.); wrote JSON snapshot to: artifacts\test123_run456.json
INFO     reproducibility.event_snapshots:event_snapshots.py:348 Loaded enhanced snapshot: artifacts\test123_run456.json
_________ TestEventSnapshots.test_snapshot_reproducibility_validation _________

self = <tests.test_reproducibility_enhancements.TestEventSnapshots object at 0x0000016F8AC45A60>
sample_events = [{'data': {'tick': 0}, 'event_type': 'start', 'timestamp': '2024-01-01T00:00:00Z'}, {'data': {'action': 'price_change'... 'timestamp': '2024-01-01T00:01:00Z'}, {'data': {'tick': 2}, 'event_type': 'end', 'timestamp': '2024-01-01T00:02:00Z'}]
tmp_path = WindowsPath('C:/Users/admin/AppData/Local/Temp/pytest-of-admin/pytest-49/test_snapshot_reproducibility_0')

    def test_snapshot_reproducibility_validation(self, sample_events, tmp_path):
        """Test snapshot reproducibility validation."""
        EventSnapshot.ARTIFACTS_DIR = tmp_path
    
        try:
            # Create two identical snapshots
            snapshot1_path = EventSnapshot.dump_events_with_metadata(sample_events, "sha1", "run1")
            snapshot2_path = EventSnapshot.dump_events_with_metadata(sample_events, "sha2", "run2")
    
            # Validate reproducibility
            validation_result = EventSnapshot.validate_snapshot_reproducibility(
                snapshot1_path, snapshot2_path
            )
    
>           assert validation_result["is_reproducible"]
E           assert False

tests\test_reproducibility_enhancements.py:699: AssertionError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:03.229 | ERROR | reproducibility.event_snapshots | [req=-] | Failed to save enhanced snapshot: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.
A suitable version of pyarrow or fastparquet is required for parquet support.
Trying to import the above resulted in these errors:
 - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.
 - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.
2025-09-16 01:27:03.234 | WARNING | reproducibility.event_snapshots | [req=-] | Parquet unavailable (Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.
A suitable version of pyarrow or fastparquet is required for parquet support.
Trying to import the above resulted in these errors:
 - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.
 - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.); wrote JSON snapshot to: artifacts\sha1_run1.json
2025-09-16 01:27:03.238 | ERROR | reproducibility.event_snapshots | [req=-] | Failed to save enhanced snapshot: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.
A suitable version of pyarrow or fastparquet is required for parquet support.
Trying to import the above resulted in these errors:
 - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.
 - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.
2025-09-16 01:27:03.242 | WARNING | reproducibility.event_snapshots | [req=-] | Parquet unavailable (Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.
A suitable version of pyarrow or fastparquet is required for parquet support.
Trying to import the above resulted in these errors:
 - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.
 - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.); wrote JSON snapshot to: artifacts\sha2_run2.json
2025-09-16 01:27:03.243 | INFO | reproducibility.event_snapshots | [req=-] | Loaded enhanced snapshot: artifacts\sha1_run1.json
2025-09-16 01:27:03.243 | INFO | reproducibility.event_snapshots | [req=-] | Loaded enhanced snapshot: artifacts\sha2_run2.json
2025-09-16 01:27:03.243 | ERROR | reproducibility.event_snapshots | [req=-] | Snapshot validation failed: 'list' object has no attribute 'get'
------------------------------ Captured log call ------------------------------
ERROR    reproducibility.event_snapshots:event_snapshots.py:326 Failed to save enhanced snapshot: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.
A suitable version of pyarrow or fastparquet is required for parquet support.
Trying to import the above resulted in these errors:
 - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.
 - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.
WARNING  reproducibility.event_snapshots:event_snapshots.py:107 Parquet unavailable (Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.
A suitable version of pyarrow or fastparquet is required for parquet support.
Trying to import the above resulted in these errors:
 - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.
 - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.); wrote JSON snapshot to: artifacts\sha1_run1.json
ERROR    reproducibility.event_snapshots:event_snapshots.py:326 Failed to save enhanced snapshot: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.
A suitable version of pyarrow or fastparquet is required for parquet support.
Trying to import the above resulted in these errors:
 - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.
 - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.
WARNING  reproducibility.event_snapshots:event_snapshots.py:107 Parquet unavailable (Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.
A suitable version of pyarrow or fastparquet is required for parquet support.
Trying to import the above resulted in these errors:
 - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.
 - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.); wrote JSON snapshot to: artifacts\sha2_run2.json
INFO     reproducibility.event_snapshots:event_snapshots.py:348 Loaded enhanced snapshot: artifacts\sha1_run1.json
INFO     reproducibility.event_snapshots:event_snapshots.py:348 Loaded enhanced snapshot: artifacts\sha2_run2.json
ERROR    reproducibility.event_snapshots:event_snapshots.py:441 Snapshot validation failed: 'list' object has no attribute 'get'
____________________ test_scenario_loading_and_validation _____________________

    @pytest.mark.asyncio
    async def test_scenario_loading_and_validation():
        """Test scenario loading and internal consistency validation."""
        engine = ScenarioEngine()
    
        # Test loading a valid scenario
        scenario = engine.load_scenario(os.path.join(TEST_SCENARIO_DIR, "tier_0_baseline.yaml"))
        assert scenario.config_data["scenario_name"] == "Test Tier 0 Baseline"
        assert scenario.config_data["difficulty_tier"] == 0
>       assert scenario.validate_scenario_consistency() is True
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ScenarioConfig' object has no attribute 'validate_scenario_consistency'

tests\test_scenario_system.py:137: AttributeError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:07.144 | INFO | root | [req=-] | ScenarioEngine initialized.
2025-09-16 01:27:07.145 | INFO | root | [req=-] | Scenario 'Test Tier 0 Baseline' loaded successfully.
------------------------------ Captured log call ------------------------------
INFO     root:scenario_engine.py:59 ScenarioEngine initialized.
INFO     root:scenario_engine.py:80 Scenario 'Test Tier 0 Baseline' loaded successfully.
_____________________ test_scenario_engine_run_simulation _____________________

    @pytest.mark.asyncio
    async def test_scenario_engine_run_simulation():
        """Test the scenario engine's ability to run a simulation."""
        engine = ScenarioEngine()
        mock_agent = MockAgent()
    
        # Patch the BotFactory.create_bot if it's used in cli_main logic
        with patch("baseline_bots.bot_factory.BotFactory.create_bot", return_value=mock_agent):
>           results = await engine.run_simulation(
                os.path.join(TEST_SCENARIO_DIR, "tier_0_baseline.yaml"), {"MockAgent": mock_agent}
            )
E           TypeError: object dict can't be used in 'await' expression

tests\test_scenario_system.py:168: TypeError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:07.158 | INFO | root | [req=-] | ScenarioEngine initialized.
2025-09-16 01:27:07.163 | INFO | root | [req=-] | 
--- Starting simulation for scenario: test_scenarios_temp\tier_0_baseline.yaml ---
2025-09-16 01:27:07.165 | INFO | root | [req=-] | Scenario 'Test Tier 0 Baseline' loaded successfully.
2025-09-16 01:27:07.165 | WARNING | root | [req=-] | Using raw market_conditions due to error in generate_market_conditions: 'ScenarioConfig' object has no attribute '_validate_market_params'
2025-09-16 01:27:07.165 | INFO | root | [req=-] | Initializing market conditions: {'economic_cycles': 'stable'}
2025-09-16 01:27:07.165 | WARNING | root | [req=-] | Falling back to YAML-defined product_catalog due to error in define_product_catalog: 'ScenarioConfig' object has no attribute '_generate_default_products'
2025-09-16 01:27:07.165 | INFO | root | [req=-] | Defining product catalog with: 0 products.
2025-09-16 01:27:07.165 | WARNING | root | [req=-] | Using raw agent_constraints due to error in configure_agent_constraints: 'ScenarioConfig' object has no attribute 'configure_agent_constraints'
2025-09-16 01:27:07.165 | INFO | root | [req=-] | Configuring constraints for MockAgent: {'initial_capital': 10000}
2025-09-16 01:27:07.166 | INFO | root | [req=-] | Scenario environment for 'Test Tier 0 Baseline' initialized.
2025-09-16 01:27:07.166 | INFO | root | [req=-] | Objective 'profit_target' met: 1050.0 >= 1000.0
2025-09-16 01:27:07.166 | INFO | root | [req=-] | Scenario 'Test Tier 0 Baseline' completed successfully!
2025-09-16 01:27:07.166 | INFO | root | [req=-] | Scenario analysis complete: success
2025-09-16 01:27:07.166 | INFO | root | [req=-] | --- Simulation for scenario Test Tier 0 Baseline finished (Duration: 5 ticks) ---
------------------------------ Captured log call ------------------------------
INFO     root:scenario_engine.py:59 ScenarioEngine initialized.
INFO     root:scenario_engine.py:361 
--- Starting simulation for scenario: test_scenarios_temp\tier_0_baseline.yaml ---
INFO     root:scenario_engine.py:80 Scenario 'Test Tier 0 Baseline' loaded successfully.
WARNING  root:scenario_engine.py:111 Using raw market_conditions due to error in generate_market_conditions: 'ScenarioConfig' object has no attribute '_validate_market_params'
INFO     root:scenario_engine.py:114 Initializing market conditions: {'economic_cycles': 'stable'}
WARNING  root:scenario_engine.py:145 Falling back to YAML-defined product_catalog due to error in define_product_catalog: 'ScenarioConfig' object has no attribute '_generate_default_products'
INFO     root:scenario_engine.py:150 Defining product catalog with: 0 products.
WARNING  root:scenario_engine.py:173 Using raw agent_constraints due to error in configure_agent_constraints: 'ScenarioConfig' object has no attribute 'configure_agent_constraints'
INFO     root:scenario_engine.py:178 Configuring constraints for MockAgent: {'initial_capital': 10000}
INFO     root:scenario_engine.py:181 Scenario environment for 'Test Tier 0 Baseline' initialized.
INFO     root:scenario_engine.py:260 Objective 'profit_target' met: 1050.0 >= 1000.0
INFO     root:scenario_engine.py:279 Scenario 'Test Tier 0 Baseline' completed successfully!
INFO     root:scenario_engine.py:353 Scenario analysis complete: success
INFO     root:scenario_engine.py:566 --- Simulation for scenario Test Tier 0 Baseline finished (Duration: 5 ticks) ---
______________________ test_dynamic_scenario_generation _______________________

    @pytest.mark.asyncio
    async def test_dynamic_scenario_generation():
        """Test dynamic scenario generation and scaling."""
        generator = DynamicScenarioGenerator(template_dir=TEST_SCENARIO_DIR + os.sep)
    
        rand_config_path = os.path.join(TEST_SCENARIO_DIR, "dynamic_rand_config.yaml")
        with open(rand_config_path) as f:
>           dynamic_rand_config = yaml.safe_load(f)
                                  ^^^^
E           NameError: name 'yaml' is not defined

tests\test_scenario_system.py:248: NameError
____________________________ test_cli_run_scenario ____________________________

mock_create_bot = <MagicMock name='create_bot' id='1578636293376'>
mock_run_simulation = <AsyncMock name='run_simulation' id='1578603532400'>

    @pytest.mark.asyncio
    @patch(
        "sys.argv",
        ["experiment_cli.py", "run", "--scenario", "Test Tier 0 Baseline", "--agents", "MockAgent"],
    )
    @patch("scenarios.scenario_engine.ScenarioEngine.run_simulation", new_callable=AsyncMock)
    @patch("baseline_bots.bot_factory.BotFactory.create_bot", return_value=MockAgent())
    async def test_cli_run_scenario(mock_create_bot, mock_run_simulation):
        """Test CLI integration for running a single scenario."""
        # Mock the return value of run_simulation
        mock_run_simulation.return_value = {
            "scenario_name": "Test Tier 0 Baseline",
            "tier": 0,
            "success_status": "success",
            "profit_target": 1200.0,
            "simulation_duration": 5,
        }
    
        await cli_main()
>       mock_run_simulation.assert_called_once()

tests\test_scenario_system.py:287: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <AsyncMock name='run_simulation' id='1578603532400'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'run_simulation' to have been called once. Called 2 times.
E           Calls: [call('C:\\Users\\admin\\Downloads\\fba\\test_scenarios_temp\\tier_0_baseline.yaml', agent_models={'MockAgent': <tests.test_scenario_system.MockAgent object at 0x0000016F8AC47EC0>}),
E            call('C:\\Users\\admin\\Downloads\\fba\\test_scenarios_temp\\tier_0_baseline.yaml', agent_models={'MockAgent': <tests.test_scenario_system.MockAgent object at 0x0000016F8AC47EC0>})].

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:928: AssertionError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:07.214 | INFO | root | [req=-] | ScenarioEngine initialized.
2025-09-16 01:27:07.224 | INFO | root | [req=-] | ScenarioEngine initialized.
------------------------------ Captured log call ------------------------------
INFO     root:scenario_engine.py:59 ScenarioEngine initialized.
INFO     root:scenario_engine.py:59 ScenarioEngine initialized.
__________________ test_cli_run_tier_and_validate_curriculum __________________

mock_create_bot = <MagicMock name='create_bot' id='1578603606960'>
mock_run_simulation = <AsyncMock name='run_simulation' id='1578609753072'>

    @pytest.mark.asyncio
    @patch(
        "sys.argv",
        ["experiment_cli.py", "run", "--tier", "0", "--agents", "MockAgent", "--validate-curriculum"],
    )
    @patch("scenarios.scenario_engine.ScenarioEngine.run_simulation", new_callable=AsyncMock)
    @patch("baseline_bots.bot_factory.BotFactory.create_bot", return_value=MockAgent())
    async def test_cli_run_tier_and_validate_curriculum(mock_create_bot, mock_run_simulation):
        """Test CLI integration for running scenarios in a tier and curriculum validation."""
        mock_run_simulation.side_effect = [
            {
                "scenario_name": "Test Tier 0 Baseline",
                "tier": 0,
                "success_status": "success",
                "profit_target": 1500,
                "simulation_duration": 5,
            },
            # Add more mock results if tier 0 had other scenarios
        ]
    
>       await cli_main()

tests\test_scenario_system.py:324: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
experiment_cli.py:640: in main
    await _cmd_run(args)
experiment_cli.py:573: in _cmd_run
    report = validator.generate_curriculum_report()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
scenarios\curriculum_validator.py:140: in generate_curriculum_report
    report["tier_progression_validation"] = self.validate_tier_progression()
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
scenarios\curriculum_validator.py:65: in validate_tier_progression
    .agg(
.venv-new\Lib\site-packages\pandas\core\groupby\generic.py:1432: in aggregate
    result = op.agg()
             ^^^^^^^^
.venv-new\Lib\site-packages\pandas\core\apply.py:190: in agg
    return self.agg_dict_like()
           ^^^^^^^^^^^^^^^^^^^^
.venv-new\Lib\site-packages\pandas\core\apply.py:423: in agg_dict_like
    return self.agg_or_apply_dict_like(op_name="agg")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv-new\Lib\site-packages\pandas\core\apply.py:1603: in agg_or_apply_dict_like
    result_index, result_data = self.compute_dict_like(
.venv-new\Lib\site-packages\pandas\core\apply.py:462: in compute_dict_like
    func = self.normalize_dictlike_arg(op_name, selected_obj, func)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <pandas.core.apply.GroupByApply object at 0x0000016F8C735EE0>
how = 'agg'
obj =   agent_model  tier  ... simulation_duration agent_name
0   MockAgent     0  ...                   5  MockAgent

[1 rows x 7 columns]
func = defaultdict(None, {'profit': ['mean'], 'success_status': [<function CurriculumValidator.validate_tier_progression.<locals>.<lambda> at 0x0000016F8C68F100>], 'simulation_duration': ['mean']})

    def normalize_dictlike_arg(
        self, how: str, obj: DataFrame | Series, func: AggFuncTypeDict
    ) -> AggFuncTypeDict:
        """
        Handler for dict-like argument.
    
        Ensures that necessary columns exist if obj is a DataFrame, and
        that a nested renamer is not passed. Also normalizes to all lists
        when values consists of a mix of list and non-lists.
        """
        assert how in ("apply", "agg", "transform")
    
        # Can't use func.values(); wouldn't work for a Series
        if (
            how == "agg"
            and isinstance(obj, ABCSeries)
            and any(is_list_like(v) for _, v in func.items())
        ) or (any(is_dict_like(v) for _, v in func.items())):
            # GH 15931 - deprecation of renaming keys
            raise SpecificationError("nested renamer is not supported")
    
        if obj.ndim != 1:
            # Check for missing columns on a frame
            from pandas import Index
    
            cols = Index(list(func.keys())).difference(obj.columns, sort=True)
            if len(cols) > 0:
>               raise KeyError(f"Column(s) {list(cols)} do not exist")
E               KeyError: "Column(s) ['profit'] do not exist"

.venv-new\Lib\site-packages\pandas\core\apply.py:663: KeyError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:07.308 | INFO | root | [req=-] | ScenarioEngine initialized.
2025-09-16 01:27:07.317 | INFO | root | [req=-] | ScenarioEngine initialized.
Benchmark recorded for agent 'MockAgent' on scenario 'Test Tier 0 Baseline' (Tier 0).

--- Generating Comprehensive Curriculum Report ---
------------------------------ Captured log call ------------------------------
INFO     root:scenario_engine.py:59 ScenarioEngine initialized.
INFO     root:scenario_engine.py:59 ScenarioEngine initialized.
________________________ test_cli_benchmark_scenarios _________________________

mock_create_bot = <MagicMock name='create_bot' id='1578635984144'>
mock_run_simulation = <AsyncMock name='run_simulation' id='1578636284496'>

    @pytest.mark.asyncio
    @patch("sys.argv", ["experiment_cli.py", "run", "--benchmark-scenarios", "--agents", "MockAgent"])
    @patch("scenarios.scenario_engine.ScenarioEngine.run_simulation", new_callable=AsyncMock)
    @patch("baseline_bots.bot_factory.BotFactory.create_bot", return_value=MockAgent())
    async def test_cli_benchmark_scenarios(mock_create_bot, mock_run_simulation):
        """Test CLI integration for benchmarking all scenarios."""
        mock_run_simulation.side_effect = [
            {
                "scenario_name": "Test Tier 0 Baseline",
                "tier": 0,
                "success_status": "success",
                "profit_target": 1500,
                "simulation_duration": 5,
            },
            {
                "scenario_name": "Test Tier 1 Moderate",
                "tier": 1,
                "success_status": "fail",
                "profit_target": 3000,
                "simulation_duration": 10,
            },
            {
                "scenario_name": "International Expansion Dummy",
                "tier": 3,
                "success_status": "fail",
                "profit_target": 5000,
                "simulation_duration": 15,
            },
            {
                "scenario_name": "Cooperative Joint Venture Dummy",
                "tier": 2,
                "success_status": "success",
                "joint_profit_target": 12000,
                "simulation_duration": 10,
            },
        ]
    
        await cli_main()
    
        # There are 4 dummy scenarios, so simulation should be called 4 times.
>       assert mock_run_simulation.call_count == 4
E       AssertionError: assert 8 == 4
E        +  where 8 = <AsyncMock name='run_simulation' id='1578636284496'>.call_count

tests\test_scenario_system.py:412: AssertionError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:07.516 | INFO | root | [req=-] | ScenarioEngine initialized.
2025-09-16 01:27:07.525 | INFO | root | [req=-] | ScenarioEngine initialized.
2025-09-16 01:27:07.529 | INFO | root | [req=-] | ScenarioEngine initialized.
2025-09-16 01:27:07.532 | INFO | root | [req=-] | ScenarioEngine initialized.
2025-09-16 01:27:07.534 | INFO | root | [req=-] | ScenarioEngine initialized.
------------------------------ Captured log call ------------------------------
INFO     root:scenario_engine.py:59 ScenarioEngine initialized.
INFO     root:scenario_engine.py:59 ScenarioEngine initialized.
INFO     root:scenario_engine.py:59 ScenarioEngine initialized.
INFO     root:scenario_engine.py:59 ScenarioEngine initialized.
INFO     root:scenario_engine.py:59 ScenarioEngine initialized.
____________ TestSystemIntegration.test_world_store_product_state _____________

self = <tests.test_system_integration.TestSystemIntegration object at 0x0000016F8B229430>
world_store = <fba_bench_core.services.world_store.WorldStore object at 0x0000016F8AC65670>

    def test_world_store_product_state(self, world_store: WorldStore):
        """
        Test WorldStore product state management.
    
        Args:
            world_store: Test WorldStore
        """
        # Create a product state
        product_id = "test-product"
>       product_state = ProductState(product_id=product_id, price=10.0, inventory=100, quality=0.8)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: ProductState.__init__() got an unexpected keyword argument 'product_id'

tests\test_system_integration.py:246: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:27:08.144 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized.
2025-09-16 01:27:08.144 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
----------------------------- Captured log setup ------------------------------
INFO     fba_bench_core.services.world_store:world_store.py:90 InMemoryStorageBackend initialized.
INFO     fba_bench_core.services.world_store:world_store.py:353 WorldStore initialized - ready for multi-agent command processing
_________ TestSystemIntegration.test_world_store_command_arbitration __________

self = <tests.test_system_integration.TestSystemIntegration object at 0x0000016F8B229610>
world_store = <fba_bench_core.services.world_store.WorldStore object at 0x0000016F8C7826F0>

    def test_world_store_command_arbitration(self, world_store: WorldStore):
        """
        Test WorldStore command arbitration.
    
        Args:
            world_store: Test WorldStore
        """
        # Create a product state
        product_id = "test-product"
>       product_state = ProductState(product_id=product_id, price=10.0, inventory=100, quality=0.8)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: ProductState.__init__() got an unexpected keyword argument 'product_id'

tests\test_system_integration.py:270: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:27:08.157 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized.
2025-09-16 01:27:08.157 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
----------------------------- Captured log setup ------------------------------
INFO     fba_bench_core.services.world_store:world_store.py:90 InMemoryStorageBackend initialized.
INFO     fba_bench_core.services.world_store:world_store.py:353 WorldStore initialized - ready for multi-agent command processing
____________ TestBenchmarkEngine.test_validate_configuration_valid ____________

self = <tests.unit.benchmarking.test_engine_new_api.TestBenchmarkEngine object at 0x0000016F8B282450>
benchmark_engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8C7B8770>

    @pytest.mark.asyncio
    async def test_validate_configuration_valid(self, benchmark_engine):
        """Test configuration validation with valid config."""
        config = {
            "benchmark_id": "test_benchmark",
            "name": "test_benchmark",
            "scenarios": [
                {
                    "name": "test_scenario",
                    "description": "Test scenario",
                    "domain": "test",
                    "duration_ticks": 10,
                }
            ],
            "agents": [{"agent_id": "test_agent", "framework": "test"}],
            "metrics": {},
        }
    
        is_valid, errors = benchmark_engine._validate_configuration(config)
    
>       assert is_valid is True
E       assert False is True

tests\unit\benchmarking\test_engine_new_api.py:163: AssertionError
_______________ TestBenchmarkEngine.test_run_benchmark_success ________________

self = <tests.unit.benchmarking.test_engine_new_api.TestBenchmarkEngine object at 0x0000016F8B2829F0>
benchmark_engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8E1A0B90>

    @pytest.mark.asyncio
    async def test_run_benchmark_success(self, benchmark_engine):
        """Test successful benchmark execution."""
        config = {
            "benchmark_id": "test_benchmark",
            "name": "test_benchmark",
            "scenarios": [
                {
                    "name": "test_scenario",
                    "description": "Test scenario",
                    "domain": "test",
                    "duration_ticks": 10,
                }
            ],
            "agents": [{"agent_id": "test_agent", "framework": "test"}],
            "metrics": {},
        }
    
>       with patch.object(
            benchmark_engine, "_execute_benchmark", new_callable=AsyncMock
        ) as mock_execute:

tests\unit\benchmarking\test_engine_new_api.py:198: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x0000016F8E0B2D20>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8E1A0B90> does not have the attribute '_execute_benchmark'

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1437: AttributeError
_______________ TestBenchmarkEngine.test_run_benchmark_failure ________________

self = <tests.unit.benchmarking.test_engine_new_api.TestBenchmarkEngine object at 0x0000016F8B282CC0>
benchmark_engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8E18DA30>

    @pytest.mark.asyncio
    async def test_run_benchmark_failure(self, benchmark_engine):
        """Test benchmark execution failure."""
        config = {
            "benchmark_id": "test_benchmark",
            "name": "test_benchmark",
            "scenarios": [
                {
                    "name": "test_scenario",
                    "description": "Test scenario",
                    "domain": "test",
                    "duration_ticks": 10,
                }
            ],
            "agents": [{"agent_id": "test_agent", "framework": "test"}],
            "metrics": {},
        }
    
>       with patch.object(
            benchmark_engine, "_execute_benchmark", new_callable=AsyncMock
        ) as mock_execute:

tests\unit\benchmarking\test_engine_new_api.py:234: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x0000016F8C1CFF20>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8E18DA30> does not have the attribute '_execute_benchmark'

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1437: AttributeError
____________ TestBenchmarkEngine.test_get_benchmark_status_active _____________

self = <tests.unit.benchmarking.test_engine_new_api.TestBenchmarkEngine object at 0x0000016F8B2827E0>
benchmark_engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8E1359D0>

    @pytest.mark.asyncio
    async def test_get_benchmark_status_active(self, benchmark_engine):
        """Test getting status of an active benchmark."""
        benchmark_id = "test_benchmark"
        benchmark_run = PydanticBenchmarkRun(
            run_id=benchmark_id,
            status=BenchmarkStatus.running,
            config={},
            created_at=datetime.utcnow(),
        )
        benchmark_engine.active_runs[benchmark_id] = benchmark_run
    
>       status = benchmark_engine.get_benchmark_status(benchmark_id)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\benchmarking\test_engine_new_api.py:254: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\core\engine.py:1912: in get_benchmark_status
    "start_time": run.start_time,
                  ^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = PydanticBenchmarkRun(run_id='test_benchmark', status=<RunStatus.running: 'running'>, config={}, created_at=datetime.datetime(2025, 9, 16, 1, 27, 9, 226086), updated_at=None)
item = 'start_time'

    def __getattr__(self, item: str) -> Any:
        private_attributes = object.__getattribute__(self, '__private_attributes__')
        if item in private_attributes:
            attribute = private_attributes[item]
            if hasattr(attribute, '__get__'):
                return attribute.__get__(self, type(self))  # type: ignore
    
            try:
                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items
                return self.__pydantic_private__[item]  # type: ignore
            except KeyError as exc:
                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
        else:
            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.
            # See `BaseModel.__repr_args__` for more details
            try:
                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')
            except AttributeError:
                pydantic_extra = None
    
            if pydantic_extra:
                try:
                    return pydantic_extra[item]
                except KeyError as exc:
                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
            else:
                if hasattr(self.__class__, item):
                    return super().__getattribute__(item)  # Raises AttributeError if appropriate
                else:
                    # this is the current error
>                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E                   AttributeError: 'PydanticBenchmarkRun' object has no attribute 'start_time'

.venv-new\Lib\site-packages\pydantic\main.py:991: AttributeError
___________ TestBenchmarkEngine.test_get_benchmark_status_completed ___________

self = <tests.unit.benchmarking.test_engine_new_api.TestBenchmarkEngine object at 0x0000016F8B281FD0>
benchmark_engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8E0E3710>

    @pytest.mark.asyncio
    async def test_get_benchmark_status_completed(self, benchmark_engine):
        """Test getting status of a completed benchmark."""
        benchmark_id = "test_benchmark"
        benchmark_run = PydanticBenchmarkRun(
            run_id=benchmark_id,
            status=BenchmarkStatus.completed,
            config={},
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow(),
        )
        benchmark_engine.completed_runs.append(benchmark_run)
    
>       status = benchmark_engine.get_benchmark_status(benchmark_id)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\benchmarking\test_engine_new_api.py:272: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\core\engine.py:1916: in get_benchmark_status
    if r.benchmark_id == benchmark_id:
       ^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = PydanticBenchmarkRun(run_id='test_benchmark', status=<RunStatus.completed: 'completed'>, config={}, created_at=datetime.datetime(2025, 9, 16, 1, 27, 9, 303147), updated_at=datetime.datetime(2025, 9, 16, 1, 27, 9, 303147))
item = 'benchmark_id'

    def __getattr__(self, item: str) -> Any:
        private_attributes = object.__getattribute__(self, '__private_attributes__')
        if item in private_attributes:
            attribute = private_attributes[item]
            if hasattr(attribute, '__get__'):
                return attribute.__get__(self, type(self))  # type: ignore
    
            try:
                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items
                return self.__pydantic_private__[item]  # type: ignore
            except KeyError as exc:
                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
        else:
            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.
            # See `BaseModel.__repr_args__` for more details
            try:
                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')
            except AttributeError:
                pydantic_extra = None
    
            if pydantic_extra:
                try:
                    return pydantic_extra[item]
                except KeyError as exc:
                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
            else:
                if hasattr(self.__class__, item):
                    return super().__getattribute__(item)  # Raises AttributeError if appropriate
                else:
                    # this is the current error
>                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E                   AttributeError: 'PydanticBenchmarkRun' object has no attribute 'benchmark_id'

.venv-new\Lib\site-packages\pydantic\main.py:991: AttributeError
__________________ TestBenchmarkEngine.test_list_benchmarks ___________________

self = <tests.unit.benchmarking.test_engine_new_api.TestBenchmarkEngine object at 0x0000016F8B283260>
benchmark_engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8E18F050>

    @pytest.mark.asyncio
    async def test_list_benchmarks(self, benchmark_engine):
        """Test listing all benchmarks."""
        # Add active benchmark
        active_run = PydanticBenchmarkRun(
            run_id="active_benchmark",
            status=BenchmarkStatus.running,
            config={},
            created_at=datetime.utcnow(),
        )
        benchmark_engine.active_runs["active_benchmark"] = active_run
    
        # Add completed benchmark
        completed_run = PydanticBenchmarkRun(
            run_id="completed_benchmark",
            status=BenchmarkStatus.completed,
            config={},
            created_at=datetime.utcnow() - timedelta(minutes=30),
            updated_at=datetime.utcnow() - timedelta(minutes=5),
        )
        benchmark_engine.completed_runs.append(completed_run)
    
>       benchmarks = benchmark_engine.list_benchmarks()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\benchmarking\test_engine_new_api.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\core\engine.py:1930: in list_benchmarks
    "benchmark_id": r.benchmark_id,
                    ^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = PydanticBenchmarkRun(run_id='completed_benchmark', status=<RunStatus.completed: 'completed'>, config={}, created_at=datetime.datetime(2025, 9, 16, 0, 57, 9, 384367), updated_at=datetime.datetime(2025, 9, 16, 1, 22, 9, 384367))
item = 'benchmark_id'

    def __getattr__(self, item: str) -> Any:
        private_attributes = object.__getattribute__(self, '__private_attributes__')
        if item in private_attributes:
            attribute = private_attributes[item]
            if hasattr(attribute, '__get__'):
                return attribute.__get__(self, type(self))  # type: ignore
    
            try:
                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items
                return self.__pydantic_private__[item]  # type: ignore
            except KeyError as exc:
                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
        else:
            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.
            # See `BaseModel.__repr_args__` for more details
            try:
                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')
            except AttributeError:
                pydantic_extra = None
    
            if pydantic_extra:
                try:
                    return pydantic_extra[item]
                except KeyError as exc:
                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
            else:
                if hasattr(self.__class__, item):
                    return super().__getattribute__(item)  # Raises AttributeError if appropriate
                else:
                    # this is the current error
>                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E                   AttributeError: 'PydanticBenchmarkRun' object has no attribute 'benchmark_id'

.venv-new\Lib\site-packages\pydantic\main.py:991: AttributeError
___________________ TestBenchmarkEngine.test_stop_benchmark ___________________

self = <tests.unit.benchmarking.test_engine_new_api.TestBenchmarkEngine object at 0x0000016F8B283530>
benchmark_engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8E1A08F0>

    @pytest.mark.asyncio
    async def test_stop_benchmark(self, benchmark_engine):
        """Test stopping a running benchmark."""
        benchmark_id = "test_benchmark"
        benchmark_run = PydanticBenchmarkRun(
            run_id=benchmark_id,
            status=BenchmarkStatus.running,
            config={},
            created_at=datetime.utcnow(),
        )
        benchmark_engine.active_runs[benchmark_id] = benchmark_run
    
>       stopped = benchmark_engine.stop_benchmark(benchmark_id)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\benchmarking\test_engine_new_api.py:324: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\core\engine.py:1945: in stop_benchmark
    run.end_time = _dt.utcnow()
    ^^^^^^^^^^^^
.venv-new\Lib\site-packages\pydantic\main.py:997: in __setattr__
    elif (setattr_handler := self._setattr_handler(name, value)) is not None:
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = PydanticBenchmarkRun(run_id='test_benchmark', status=<BenchmarkStatus.STOPPED: 'stopped'>, config={}, created_at=datetime.datetime(2025, 9, 16, 1, 27, 9, 460570), updated_at=None)
name = 'end_time', value = datetime.datetime(2025, 9, 16, 1, 27, 9, 460570)

    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:
        """Get a handler for setting an attribute on the model instance.
    
        Returns:
            A handler for setting an attribute on the model instance. Used for memoization of the handler.
            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`
            Returns `None` when memoization is not safe, then the attribute is set directly.
        """
        cls = self.__class__
        if name in cls.__class_vars__:
            raise AttributeError(
                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '
                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'
            )
        elif not _fields.is_valid_field_name(name):
            if (attribute := cls.__private_attributes__.get(name)) is not None:
                if hasattr(attribute, '__set__'):
                    return lambda model, _name, val: attribute.__set__(model, val)
                else:
                    return _SIMPLE_SETATTR_HANDLERS['private']
            else:
                _object_setattr(self, name, value)
                return None  # Can not return memoized handler with possibly freeform attr names
    
        attr = getattr(cls, name, None)
        # NOTE: We currently special case properties and `cached_property`, but we might need
        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors
        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value
        # to the instance's `__dict__`, but other non-data descriptors might do things differently.
        if isinstance(attr, cached_property):
            return _SIMPLE_SETATTR_HANDLERS['cached_property']
    
        _check_frozen(cls, name, value)
    
        # We allow properties to be set only on non frozen models for now (to match dataclasses).
        # This can be changed if it ever gets requested.
        if isinstance(attr, property):
            return lambda model, _name, val: attr.__set__(model, val)
        elif cls.model_config.get('validate_assignment'):
            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']
        elif name not in cls.__pydantic_fields__:
            if cls.model_config.get('extra') != 'allow':
                # TODO - matching error
>               raise ValueError(f'"{cls.__name__}" object has no field "{name}"')
E               ValueError: "PydanticBenchmarkRun" object has no field "end_time"

.venv-new\Lib\site-packages\pydantic\main.py:1044: ValueError
_______________ TestBenchmarkEngine.test_cleanup_completed_runs _______________

self = <tests.unit.benchmarking.test_engine_new_api.TestBenchmarkEngine object at 0x0000016F8B283AD0>
benchmark_engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8E0B1A60>

    @pytest.mark.asyncio
    async def test_cleanup_completed_runs(self, benchmark_engine):
        """Test cleaning up old completed runs."""
        # Add old completed run
        old_run = PydanticBenchmarkRun(
            run_id="old_benchmark",
            status=BenchmarkStatus.completed,
            config={},
            created_at=datetime.utcnow() - timedelta(days=40),
            updated_at=datetime.utcnow() - timedelta(days=35),
        )
        benchmark_engine.completed_runs.append(old_run)
    
        # Add recent completed run
        recent_run = PydanticBenchmarkRun(
            run_id="recent_benchmark",
            status=BenchmarkStatus.completed,
            config={},
            created_at=datetime.utcnow() - timedelta(days=5),
            updated_at=datetime.utcnow() - timedelta(days=1),
        )
        benchmark_engine.completed_runs.append(recent_run)
    
>       benchmark_engine.cleanup_completed_runs(max_age_days=30)

tests\unit\benchmarking\test_engine_new_api.py:361: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\core\engine.py:1956: in cleanup_completed_runs
    st = r.start_time.timestamp() if r.start_time else 0
                                     ^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = PydanticBenchmarkRun(run_id='old_benchmark', status=<RunStatus.completed: 'completed'>, config={}, created_at=datetime.datetime(2025, 8, 7, 1, 27, 9, 551836), updated_at=datetime.datetime(2025, 8, 12, 1, 27, 9, 551836))
item = 'start_time'

    def __getattr__(self, item: str) -> Any:
        private_attributes = object.__getattribute__(self, '__private_attributes__')
        if item in private_attributes:
            attribute = private_attributes[item]
            if hasattr(attribute, '__get__'):
                return attribute.__get__(self, type(self))  # type: ignore
    
            try:
                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items
                return self.__pydantic_private__[item]  # type: ignore
            except KeyError as exc:
                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
        else:
            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.
            # See `BaseModel.__repr_args__` for more details
            try:
                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')
            except AttributeError:
                pydantic_extra = None
    
            if pydantic_extra:
                try:
                    return pydantic_extra[item]
                except KeyError as exc:
                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
            else:
                if hasattr(self.__class__, item):
                    return super().__getattribute__(item)  # Raises AttributeError if appropriate
                else:
                    # this is the current error
>                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E                   AttributeError: 'PydanticBenchmarkRun' object has no attribute 'start_time'

.venv-new\Lib\site-packages\pydantic\main.py:991: AttributeError
_________ TestBenchmarkEngine.test_calculate_scenario_kpis_no_results _________

self = <tests.unit.benchmarking.test_engine_new_api.TestBenchmarkEngine object at 0x0000016F8B2A05C0>
benchmark_engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8F661A00>

    def test_calculate_scenario_kpis_no_results(self, benchmark_engine):
        """Test calculating scenario KPIs with no results."""
        benchmark_engine.current_scenario_result = None
    
>       kpis = benchmark_engine._calculate_scenario_kpis()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'BenchmarkEngine' object has no attribute '_calculate_scenario_kpis'

tests\unit\benchmarking\test_engine_new_api.py:419: AttributeError
__________ TestBenchmarkEngine.test_save_results_with_completed_runs __________

self = <tests.unit.benchmarking.test_engine_new_api.TestBenchmarkEngine object at 0x0000016F8B2A0AD0>
benchmark_engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8E135EE0>

    @pytest.mark.asyncio
    async def test_save_results_with_completed_runs(self, benchmark_engine):
        """Test saving results with completed runs."""
        # Add completed run
>       completed_run = PydanticBenchmarkRun(
            run_id="completed_benchmark",
            status=BenchmarkStatus.completed,
            config={"benchmark_id": "test_benchmark"},
            created_at=datetime.utcnow() - timedelta(minutes=30),
            updated_at=datetime.utcnow() - timedelta(minutes=5),
            start_time=datetime.utcnow() - timedelta(minutes=30),
            end_time=datetime.utcnow() - timedelta(minutes=5),
        )
E       pydantic_core._pydantic_core.ValidationError: 2 validation errors for PydanticBenchmarkRun
E       start_time
E         Extra inputs are not permitted [type=extra_forbidden, input_value=datetime.datetime(2025, 9, 16, 0, 57, 9, 670059), input_type=datetime]
E           For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden
E       end_time
E         Extra inputs are not permitted [type=extra_forbidden, input_value=datetime.datetime(2025, 9, 16, 1, 22, 9, 670059), input_type=datetime]
E           For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden

tests\unit\benchmarking\test_engine_new_api.py:433: ValidationError
__________ TestBenchmarkEngine.test_get_summary_with_completed_runs ___________

self = <tests.unit.benchmarking.test_engine_new_api.TestBenchmarkEngine object at 0x0000016F8B2A0FB0>
benchmark_engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8F6635F0>

    def test_get_summary_with_completed_runs(self, benchmark_engine):
        """Test getting summary with completed runs."""
        # Add completed run
>       completed_run = PydanticBenchmarkRun(
            run_id="completed_benchmark",
            status=BenchmarkStatus.completed,
            config={"agents": [{"id": "test_agent"}]},
            created_at=datetime.utcnow() - timedelta(minutes=30),
            updated_at=datetime.utcnow() - timedelta(minutes=5),
            start_time=datetime.utcnow() - timedelta(minutes=30),
            end_time=datetime.utcnow() - timedelta(minutes=5),
        )
E       pydantic_core._pydantic_core.ValidationError: 2 validation errors for PydanticBenchmarkRun
E       start_time
E         Extra inputs are not permitted [type=extra_forbidden, input_value=datetime.datetime(2025, 9, 16, 0, 57, 9, 684793), input_type=datetime]
E           For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden
E       end_time
E         Extra inputs are not permitted [type=extra_forbidden, input_value=datetime.datetime(2025, 9, 16, 1, 22, 9, 684793), input_type=datetime]
E           For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden

tests\unit\benchmarking\test_engine_new_api.py:463: ValidationError
__________________ test_advanced_agent_forwarding_and_usage ___________________

    @pytest.mark.asyncio
    async def test_advanced_agent_forwarding_and_usage():
        bus = EventBus()
        await bus.start()
        bus.start_recording()
    
        enforcer = BudgetEnforcer(
            {"limits": {"total_tokens_per_tick": 1000}, "allow_soft_overage": False}
        )
        await enforcer.start(bus)
    
>       agent = AdvancedAgent(agent_id="agent-42", budget_enforcer=enforcer, event_bus=bus)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: AdvancedAgent.__init__() got an unexpected keyword argument 'agent_id'

tests\unit\test_advanced_agent_metering.py:21: TypeError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:10.099 | INFO | constraints.token_counter | [req=-] | Initialized tiktoken encoding for model: gpt-3.5-turbo
------------------------------ Captured log call ------------------------------
INFO     constraints.token_counter:token_counter.py:194 Initialized tiktoken encoding for model: gpt-3.5-turbo
_____________ test_advanced_agent_hard_exceed_raises_and_records ______________

    @pytest.mark.asyncio
    async def test_advanced_agent_hard_exceed_raises_and_records():
        bus = EventBus()
        await bus.start()
        bus.start_recording()
    
        enforcer = BudgetEnforcer(
            {"limits": {"total_tokens_per_tick": 20}, "allow_soft_overage": False}
        )
        await enforcer.start(bus)
    
>       agent = AdvancedAgent(agent_id="agent-99", budget_enforcer=enforcer, event_bus=bus)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: AdvancedAgent.__init__() got an unexpected keyword argument 'agent_id'

tests\unit\test_advanced_agent_metering.py:43: TypeError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:10.105 | INFO | constraints.token_counter | [req=-] | Initialized tiktoken encoding for model: gpt-3.5-turbo
------------------------------ Captured log call ------------------------------
INFO     constraints.token_counter:token_counter.py:194 Initialized tiktoken encoding for model: gpt-3.5-turbo
_ TestAdvancedMetricsExtended.test_cross_domain_metrics_calculate_multi_modal_integration_with_empty_data _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B3229F0>
cross_domain_metrics = <benchmarking.metrics.cross_domain.CrossDomainMetrics object at 0x0000016F8C1CFB60>

    def test_cross_domain_metrics_calculate_multi_modal_integration_with_empty_data(
        self, cross_domain_metrics
    ):
        """Test multi-modal integration calculation with empty data."""
        empty_data = {}
    
>       result = cross_domain_metrics.calculate_multi_modal_integration(empty_data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CrossDomainMetrics' object has no attribute 'calculate_multi_modal_integration'

tests\unit\test_advanced_metrics.py:1182: AttributeError
_ TestAdvancedMetricsExtended.test_statistical_analysis_framework_calculate_descriptive_statistics_with_empty_data _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B322C30>
statistical_analysis_framework = <benchmarking.metrics.statistical_analysis.StatisticalAnalysisFramework object at 0x0000016F8C1469C0>

    def test_statistical_analysis_framework_calculate_descriptive_statistics_with_empty_data(
        self, statistical_analysis_framework
    ):
        """Test descriptive statistics calculation with empty data."""
        empty_data = {}
    
>       result = statistical_analysis_framework.calculate_descriptive_statistics(empty_data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StatisticalAnalysisFramework' object has no attribute 'calculate_descriptive_statistics'

tests\unit\test_advanced_metrics.py:1194: AttributeError
_ TestAdvancedMetricsExtended.test_statistical_analysis_framework_calculate_inferential_statistics_with_empty_data _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B322E70>
statistical_analysis_framework = <benchmarking.metrics.statistical_analysis.StatisticalAnalysisFramework object at 0x0000016F8E0B2270>

    def test_statistical_analysis_framework_calculate_inferential_statistics_with_empty_data(
        self, statistical_analysis_framework
    ):
        """Test inferential statistics calculation with empty data."""
        empty_data = {}
    
>       result = statistical_analysis_framework.calculate_inferential_statistics(empty_data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StatisticalAnalysisFramework' object has no attribute 'calculate_inferential_statistics'

tests\unit\test_advanced_metrics.py:1206: AttributeError
_ TestAdvancedMetricsExtended.test_comparative_analysis_engine_calculate_performance_comparison_with_empty_data _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B323080>
comparative_analysis_engine = <benchmarking.metrics.comparative_analysis.ComparativeAnalysisEngine object at 0x0000016F8E0E1310>

    def test_comparative_analysis_engine_calculate_performance_comparison_with_empty_data(
        self, comparative_analysis_engine
    ):
        """Test performance comparison calculation with empty data."""
        empty_data = {}
    
>       result = comparative_analysis_engine.calculate_performance_comparison(empty_data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ComparativeAnalysisEngine' object has no attribute 'calculate_performance_comparison'. Did you mean: 'calculate_performance_gap_score'?

tests\unit\test_advanced_metrics.py:1218: AttributeError
_ TestAdvancedMetricsExtended.test_comparative_analysis_engine_calculate_efficiency_effectiveness_with_empty_data _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B323290>
comparative_analysis_engine = <benchmarking.metrics.comparative_analysis.ComparativeAnalysisEngine object at 0x0000016F8E0E2630>

    def test_comparative_analysis_engine_calculate_efficiency_effectiveness_with_empty_data(
        self, comparative_analysis_engine
    ):
        """Test efficiency-effectiveness calculation with empty data."""
        empty_data = {}
    
>       result = comparative_analysis_engine.calculate_efficiency_effectiveness(empty_data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ComparativeAnalysisEngine' object has no attribute 'calculate_efficiency_effectiveness'

tests\unit\test_advanced_metrics.py:1230: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_none_data[AdvancedCognitiveMetrics] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B3239B0>
metric_class = <class 'benchmarking.metrics.advanced_cognitive.AdvancedCognitiveMetrics'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_none_data(self, metric_class):
        """Test metrics calculation with None data."""
        metric = metric_class()
    
>       result = metric.calculate(None)
                 ^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1298: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\metrics\advanced_cognitive.py:133: in calculate
    logical_consistency = self.calculate_logical_consistency(data)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.advanced_cognitive.AdvancedCognitiveMetrics object at 0x0000016F8C793080>
data = None

    def calculate_logical_consistency(self, data: Dict[str, Any]) -> float:
        """
        Calculate logical consistency score.
    
        Args:
            data: Data containing logical consistency metrics
    
        Returns:
            Logical consistency score
        """
>       reasoning_traces = data.get("reasoning_traces", [])
                           ^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'get'

benchmarking\metrics\advanced_cognitive.py:174: AttributeError
-------------------------- Captured stdout teardown ---------------------------
2025-09-16 01:27:10.451 | ERROR | asyncio | [req=-] | Task was destroyed but it is pending!
task: <Task pending name='InMemoryEventBusRunner' coro=<InMemoryEventBus._runner() running at C:\Users\admin\Downloads\fba\fba_events\bus.py:309> wait_for=<Future pending cb=[Task.task_wakeup()]>>
2025-09-16 01:27:10.451 | ERROR | fba_events.bus | [req=-] | Unhandled exception in InMemoryEventBus runner: Event loop is closed
Traceback (most recent call last):
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 158, in get
    await getter
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 309, in _runner
    event, event_type, ts = await self._queue.get()
                            ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 160, in get
    getter.cancel()  # Just in case getter is not done yet.
    ^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 799, in call_soon
    self._check_closed()
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 545, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
2025-09-16 01:27:10.452 | ERROR | asyncio | [req=-] | Task was destroyed but it is pending!
task: <Task pending name='InMemoryEventBusRunner' coro=<InMemoryEventBus._runner() running at C:\Users\admin\Downloads\fba\fba_events\bus.py:309> wait_for=<Future pending cb=[Task.task_wakeup()]>>
2025-09-16 01:27:10.452 | ERROR | fba_events.bus | [req=-] | Unhandled exception in InMemoryEventBus runner: Event loop is closed
Traceback (most recent call last):
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 158, in get
    await getter
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 309, in _runner
    event, event_type, ts = await self._queue.get()
                            ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 160, in get
    getter.cancel()  # Just in case getter is not done yet.
    ^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 799, in call_soon
    self._check_closed()
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 545, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_none_data[BusinessIntelligenceMetrics] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B323BC0>
metric_class = <class 'benchmarking.metrics.business_intelligence.BusinessIntelligenceMetrics'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_none_data(self, metric_class):
        """Test metrics calculation with None data."""
        metric = metric_class()
    
>       result = metric.calculate(None)
                 ^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1298: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.business_intelligence.BusinessIntelligenceMetrics object at 0x0000016F8C12BA70>
data = None

    def calculate(self, data: Dict[str, Any]) -> float:
        """
        Calculate business intelligence performance score from accumulated event-driven data.
    
        Expected keys in data:
            - sales: List[Dict] items of SaleOccurred summaries with keys: total_revenue, total_profit, agent_id, tick
            - ad_spend: List[Dict] items of AdSpendEvent summaries with keys: spend, campaign_id, asin, agent_id, tick
            - inventory_updates: List[Dict] snapshots or events for inventory/resource utilization
            - decisions: List[Dict] agent decision/action approvals with controller priority mappings
        """
        # Aggregate ROI: profit / ad_spend (guard zero)
>       sales = data.get("sales", [])
                ^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'get'

benchmarking\metrics\business_intelligence.py:145: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_none_data[TechnicalPerformanceMetrics] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B323C80>
metric_class = <class 'benchmarking.metrics.technical_performance.TechnicalPerformanceMetrics'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_none_data(self, metric_class):
        """Test metrics calculation with None data."""
        metric = metric_class()
    
>       result = metric.calculate(None)
                 ^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1298: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\metrics\technical_performance.py:149: in calculate
    scalability = self.calculate_scalability(data)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.technical_performance.TechnicalPerformanceMetrics object at 0x0000016F8E0E2270>
data = None

    def calculate_scalability(self, data: Dict[str, Any]) -> float:
        """
        Calculate scalability under load score.
    
        Args:
            data: Data containing scalability metrics
    
        Returns:
            Scalability score
        """
>       load_test_results = data.get("load_test_results", [])
                            ^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'get'

benchmarking\metrics\technical_performance.py:190: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_none_data[EthicalSafetyMetrics] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B323D40>
metric_class = <class 'benchmarking.metrics.ethical_safety.EthicalSafetyMetrics'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_none_data(self, metric_class):
        """Test metrics calculation with None data."""
        metric = metric_class()
    
>       result = metric.calculate(None)
                 ^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1298: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.ethical_safety.EthicalSafetyMetrics object at 0x0000016F8E0B0B00>
data = None

    def calculate(self, data: Dict[str, Any]) -> float:
        """
        Calculate ethical and safety performance score using event-derived metrics.
    
        Expected keys in data:
            - compliance_violations: List[Dict] of ComplianceViolationEvent summaries
            - customer_reviews: List[Dict] of NegativeReviewEvent or similar with sentiment and resolution fields
            - responses: List[Dict] of agent responses containing compensation or tone fields for fairness analysis
        """
        # Safety protocol adherence: 1 - normalized violation rate
>       violations = data.get("compliance_violations", [])
                     ^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'get'

benchmarking\metrics\ethical_safety.py:155: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_none_data[CrossDomainMetrics] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B323E00>
metric_class = <class 'benchmarking.metrics.cross_domain.CrossDomainMetrics'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_none_data(self, metric_class):
        """Test metrics calculation with None data."""
        metric = metric_class()
    
>       result = metric.calculate(None)
                 ^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1298: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\metrics\cross_domain.py:136: in calculate
    domain_adaptation = self.calculate_domain_adaptation(data)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.cross_domain.CrossDomainMetrics object at 0x0000016F8C1CE180>
data = None

    def calculate_domain_adaptation(self, data: Dict[str, Any]) -> float:
        """
        Calculate domain adaptation capability score.
    
        Args:
            data: Data containing domain adaptation metrics
    
        Returns:
            Domain adaptation score
        """
>       domain_adaptations = data.get("domain_adaptations", [])
                             ^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'get'

benchmarking\metrics\cross_domain.py:181: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_none_data[StatisticalAnalysisFramework] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B323EC0>
metric_class = <class 'benchmarking.metrics.statistical_analysis.StatisticalAnalysisFramework'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_none_data(self, metric_class):
        """Test metrics calculation with None data."""
        metric = metric_class()
    
>       result = metric.calculate(None)
                 ^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1298: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\metrics\statistical_analysis.py:187: in calculate
    confidence_interval = self.calculate_confidence_interval_score(data)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.statistical_analysis.StatisticalAnalysisFramework object at 0x0000016F8E1A2990>
data = None

    def calculate_confidence_interval_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate confidence interval score.
    
        Args:
            data: Data containing confidence interval metrics
    
        Returns:
            Confidence interval score
        """
>       interval_data = data.get("confidence_interval_data", [])
                        ^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'get'

benchmarking\metrics\statistical_analysis.py:330: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_none_data[ComparativeAnalysisEngine] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B323F80>
metric_class = <class 'benchmarking.metrics.comparative_analysis.ComparativeAnalysisEngine'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_none_data(self, metric_class):
        """Test metrics calculation with None data."""
        metric = metric_class()
    
>       result = metric.calculate(None)
                 ^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1298: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\metrics\comparative_analysis.py:230: in calculate
    head_to_head = self.calculate_head_to_head_score(data)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.comparative_analysis.ComparativeAnalysisEngine object at 0x0000016F8C7BBAD0>
data = None

    def calculate_head_to_head_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate head-to-head comparison score.
    
        Args:
            data: Data containing head-to-head comparison metrics
    
        Returns:
            Head-to-head comparison score
        """
>       comparisons = data.get("head_to_head_comparisons", [])
                      ^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'get'

benchmarking\metrics\comparative_analysis.py:451: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_string_data[AdvancedCognitiveMetrics] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B33C200>
metric_class = <class 'benchmarking.metrics.advanced_cognitive.AdvancedCognitiveMetrics'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_string_data(self, metric_class):
        """Test metrics calculation with string data."""
        metric = metric_class()
    
>       result = metric.calculate("invalid")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1320: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\metrics\advanced_cognitive.py:133: in calculate
    logical_consistency = self.calculate_logical_consistency(data)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.advanced_cognitive.AdvancedCognitiveMetrics object at 0x0000016F8B283680>
data = 'invalid'

    def calculate_logical_consistency(self, data: Dict[str, Any]) -> float:
        """
        Calculate logical consistency score.
    
        Args:
            data: Data containing logical consistency metrics
    
        Returns:
            Logical consistency score
        """
>       reasoning_traces = data.get("reasoning_traces", [])
                           ^^^^^^^^
E       AttributeError: 'str' object has no attribute 'get'

benchmarking\metrics\advanced_cognitive.py:174: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_string_data[BusinessIntelligenceMetrics] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B33C410>
metric_class = <class 'benchmarking.metrics.business_intelligence.BusinessIntelligenceMetrics'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_string_data(self, metric_class):
        """Test metrics calculation with string data."""
        metric = metric_class()
    
>       result = metric.calculate("invalid")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1320: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.business_intelligence.BusinessIntelligenceMetrics object at 0x0000016F8C1CDDC0>
data = 'invalid'

    def calculate(self, data: Dict[str, Any]) -> float:
        """
        Calculate business intelligence performance score from accumulated event-driven data.
    
        Expected keys in data:
            - sales: List[Dict] items of SaleOccurred summaries with keys: total_revenue, total_profit, agent_id, tick
            - ad_spend: List[Dict] items of AdSpendEvent summaries with keys: spend, campaign_id, asin, agent_id, tick
            - inventory_updates: List[Dict] snapshots or events for inventory/resource utilization
            - decisions: List[Dict] agent decision/action approvals with controller priority mappings
        """
        # Aggregate ROI: profit / ad_spend (guard zero)
>       sales = data.get("sales", [])
                ^^^^^^^^
E       AttributeError: 'str' object has no attribute 'get'

benchmarking\metrics\business_intelligence.py:145: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_string_data[TechnicalPerformanceMetrics] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B33C4D0>
metric_class = <class 'benchmarking.metrics.technical_performance.TechnicalPerformanceMetrics'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_string_data(self, metric_class):
        """Test metrics calculation with string data."""
        metric = metric_class()
    
>       result = metric.calculate("invalid")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1320: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\metrics\technical_performance.py:149: in calculate
    scalability = self.calculate_scalability(data)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.technical_performance.TechnicalPerformanceMetrics object at 0x0000016F8E0B3770>
data = 'invalid'

    def calculate_scalability(self, data: Dict[str, Any]) -> float:
        """
        Calculate scalability under load score.
    
        Args:
            data: Data containing scalability metrics
    
        Returns:
            Scalability score
        """
>       load_test_results = data.get("load_test_results", [])
                            ^^^^^^^^
E       AttributeError: 'str' object has no attribute 'get'

benchmarking\metrics\technical_performance.py:190: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_string_data[EthicalSafetyMetrics] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B33C590>
metric_class = <class 'benchmarking.metrics.ethical_safety.EthicalSafetyMetrics'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_string_data(self, metric_class):
        """Test metrics calculation with string data."""
        metric = metric_class()
    
>       result = metric.calculate("invalid")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1320: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.ethical_safety.EthicalSafetyMetrics object at 0x0000016F8E0E26F0>
data = 'invalid'

    def calculate(self, data: Dict[str, Any]) -> float:
        """
        Calculate ethical and safety performance score using event-derived metrics.
    
        Expected keys in data:
            - compliance_violations: List[Dict] of ComplianceViolationEvent summaries
            - customer_reviews: List[Dict] of NegativeReviewEvent or similar with sentiment and resolution fields
            - responses: List[Dict] of agent responses containing compensation or tone fields for fairness analysis
        """
        # Safety protocol adherence: 1 - normalized violation rate
>       violations = data.get("compliance_violations", [])
                     ^^^^^^^^
E       AttributeError: 'str' object has no attribute 'get'

benchmarking\metrics\ethical_safety.py:155: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_string_data[CrossDomainMetrics] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B33C650>
metric_class = <class 'benchmarking.metrics.cross_domain.CrossDomainMetrics'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_string_data(self, metric_class):
        """Test metrics calculation with string data."""
        metric = metric_class()
    
>       result = metric.calculate("invalid")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1320: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\metrics\cross_domain.py:136: in calculate
    domain_adaptation = self.calculate_domain_adaptation(data)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.cross_domain.CrossDomainMetrics object at 0x0000016F8F6623F0>
data = 'invalid'

    def calculate_domain_adaptation(self, data: Dict[str, Any]) -> float:
        """
        Calculate domain adaptation capability score.
    
        Args:
            data: Data containing domain adaptation metrics
    
        Returns:
            Domain adaptation score
        """
>       domain_adaptations = data.get("domain_adaptations", [])
                             ^^^^^^^^
E       AttributeError: 'str' object has no attribute 'get'

benchmarking\metrics\cross_domain.py:181: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_string_data[StatisticalAnalysisFramework] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B33C710>
metric_class = <class 'benchmarking.metrics.statistical_analysis.StatisticalAnalysisFramework'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_string_data(self, metric_class):
        """Test metrics calculation with string data."""
        metric = metric_class()
    
>       result = metric.calculate("invalid")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1320: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\metrics\statistical_analysis.py:187: in calculate
    confidence_interval = self.calculate_confidence_interval_score(data)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.statistical_analysis.StatisticalAnalysisFramework object at 0x0000016F8F663500>
data = 'invalid'

    def calculate_confidence_interval_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate confidence interval score.
    
        Args:
            data: Data containing confidence interval metrics
    
        Returns:
            Confidence interval score
        """
>       interval_data = data.get("confidence_interval_data", [])
                        ^^^^^^^^
E       AttributeError: 'str' object has no attribute 'get'

benchmarking\metrics\statistical_analysis.py:330: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_string_data[ComparativeAnalysisEngine] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B33C7D0>
metric_class = <class 'benchmarking.metrics.comparative_analysis.ComparativeAnalysisEngine'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_string_data(self, metric_class):
        """Test metrics calculation with string data."""
        metric = metric_class()
    
>       result = metric.calculate("invalid")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1320: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\metrics\comparative_analysis.py:230: in calculate
    head_to_head = self.calculate_head_to_head_score(data)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.comparative_analysis.ComparativeAnalysisEngine object at 0x0000016F8F661880>
data = 'invalid'

    def calculate_head_to_head_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate head-to-head comparison score.
    
        Args:
            data: Data containing head-to-head comparison metrics
    
        Returns:
            Head-to-head comparison score
        """
>       comparisons = data.get("head_to_head_comparisons", [])
                      ^^^^^^^^
E       AttributeError: 'str' object has no attribute 'get'

benchmarking\metrics\comparative_analysis.py:451: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_list_data[AdvancedCognitiveMetrics] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B33CA10>
metric_class = <class 'benchmarking.metrics.advanced_cognitive.AdvancedCognitiveMetrics'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_list_data(self, metric_class):
        """Test metrics calculation with list data."""
        metric = metric_class()
    
>       result = metric.calculate([1, 2, 3])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1342: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\metrics\advanced_cognitive.py:133: in calculate
    logical_consistency = self.calculate_logical_consistency(data)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.advanced_cognitive.AdvancedCognitiveMetrics object at 0x0000016F8E0B1730>
data = [1, 2, 3]

    def calculate_logical_consistency(self, data: Dict[str, Any]) -> float:
        """
        Calculate logical consistency score.
    
        Args:
            data: Data containing logical consistency metrics
    
        Returns:
            Logical consistency score
        """
>       reasoning_traces = data.get("reasoning_traces", [])
                           ^^^^^^^^
E       AttributeError: 'list' object has no attribute 'get'

benchmarking\metrics\advanced_cognitive.py:174: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_list_data[BusinessIntelligenceMetrics] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B33CC20>
metric_class = <class 'benchmarking.metrics.business_intelligence.BusinessIntelligenceMetrics'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_list_data(self, metric_class):
        """Test metrics calculation with list data."""
        metric = metric_class()
    
>       result = metric.calculate([1, 2, 3])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1342: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.business_intelligence.BusinessIntelligenceMetrics object at 0x0000016F8E0E1F40>
data = [1, 2, 3]

    def calculate(self, data: Dict[str, Any]) -> float:
        """
        Calculate business intelligence performance score from accumulated event-driven data.
    
        Expected keys in data:
            - sales: List[Dict] items of SaleOccurred summaries with keys: total_revenue, total_profit, agent_id, tick
            - ad_spend: List[Dict] items of AdSpendEvent summaries with keys: spend, campaign_id, asin, agent_id, tick
            - inventory_updates: List[Dict] snapshots or events for inventory/resource utilization
            - decisions: List[Dict] agent decision/action approvals with controller priority mappings
        """
        # Aggregate ROI: profit / ad_spend (guard zero)
>       sales = data.get("sales", [])
                ^^^^^^^^
E       AttributeError: 'list' object has no attribute 'get'

benchmarking\metrics\business_intelligence.py:145: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_list_data[TechnicalPerformanceMetrics] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B33CCE0>
metric_class = <class 'benchmarking.metrics.technical_performance.TechnicalPerformanceMetrics'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_list_data(self, metric_class):
        """Test metrics calculation with list data."""
        metric = metric_class()
    
>       result = metric.calculate([1, 2, 3])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1342: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\metrics\technical_performance.py:149: in calculate
    scalability = self.calculate_scalability(data)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.technical_performance.TechnicalPerformanceMetrics object at 0x0000016F8B282660>
data = [1, 2, 3]

    def calculate_scalability(self, data: Dict[str, Any]) -> float:
        """
        Calculate scalability under load score.
    
        Args:
            data: Data containing scalability metrics
    
        Returns:
            Scalability score
        """
>       load_test_results = data.get("load_test_results", [])
                            ^^^^^^^^
E       AttributeError: 'list' object has no attribute 'get'

benchmarking\metrics\technical_performance.py:190: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_list_data[EthicalSafetyMetrics] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B33CDA0>
metric_class = <class 'benchmarking.metrics.ethical_safety.EthicalSafetyMetrics'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_list_data(self, metric_class):
        """Test metrics calculation with list data."""
        metric = metric_class()
    
>       result = metric.calculate([1, 2, 3])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1342: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.ethical_safety.EthicalSafetyMetrics object at 0x0000016F8E136960>
data = [1, 2, 3]

    def calculate(self, data: Dict[str, Any]) -> float:
        """
        Calculate ethical and safety performance score using event-derived metrics.
    
        Expected keys in data:
            - compliance_violations: List[Dict] of ComplianceViolationEvent summaries
            - customer_reviews: List[Dict] of NegativeReviewEvent or similar with sentiment and resolution fields
            - responses: List[Dict] of agent responses containing compensation or tone fields for fairness analysis
        """
        # Safety protocol adherence: 1 - normalized violation rate
>       violations = data.get("compliance_violations", [])
                     ^^^^^^^^
E       AttributeError: 'list' object has no attribute 'get'

benchmarking\metrics\ethical_safety.py:155: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_list_data[CrossDomainMetrics] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B33CE60>
metric_class = <class 'benchmarking.metrics.cross_domain.CrossDomainMetrics'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_list_data(self, metric_class):
        """Test metrics calculation with list data."""
        metric = metric_class()
    
>       result = metric.calculate([1, 2, 3])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1342: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\metrics\cross_domain.py:136: in calculate
    domain_adaptation = self.calculate_domain_adaptation(data)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.cross_domain.CrossDomainMetrics object at 0x0000016F8E137D70>
data = [1, 2, 3]

    def calculate_domain_adaptation(self, data: Dict[str, Any]) -> float:
        """
        Calculate domain adaptation capability score.
    
        Args:
            data: Data containing domain adaptation metrics
    
        Returns:
            Domain adaptation score
        """
>       domain_adaptations = data.get("domain_adaptations", [])
                             ^^^^^^^^
E       AttributeError: 'list' object has no attribute 'get'

benchmarking\metrics\cross_domain.py:181: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_list_data[StatisticalAnalysisFramework] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B2F5DF0>
metric_class = <class 'benchmarking.metrics.statistical_analysis.StatisticalAnalysisFramework'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_list_data(self, metric_class):
        """Test metrics calculation with list data."""
        metric = metric_class()
    
>       result = metric.calculate([1, 2, 3])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1342: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\metrics\statistical_analysis.py:187: in calculate
    confidence_interval = self.calculate_confidence_interval_score(data)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.statistical_analysis.StatisticalAnalysisFramework object at 0x0000016F8E135D60>
data = [1, 2, 3]

    def calculate_confidence_interval_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate confidence interval score.
    
        Args:
            data: Data containing confidence interval metrics
    
        Returns:
            Confidence interval score
        """
>       interval_data = data.get("confidence_interval_data", [])
                        ^^^^^^^^
E       AttributeError: 'list' object has no attribute 'get'

benchmarking\metrics\statistical_analysis.py:330: AttributeError
_ TestAdvancedMetricsExtended.test_metrics_calculate_with_list_data[ComparativeAnalysisEngine] _

self = <tests.unit.test_advanced_metrics.TestAdvancedMetricsExtended object at 0x0000016F8B323830>
metric_class = <class 'benchmarking.metrics.comparative_analysis.ComparativeAnalysisEngine'>

    @pytest.mark.parametrize(
        "metric_class",
        [
            AdvancedCognitiveMetrics,
            BusinessIntelligenceMetrics,
            TechnicalPerformanceMetrics,
            EthicalSafetyMetrics,
            CrossDomainMetrics,
            StatisticalAnalysisFramework,
            ComparativeAnalysisEngine,
        ],
    )
    def test_metrics_calculate_with_list_data(self, metric_class):
        """Test metrics calculation with list data."""
        metric = metric_class()
    
>       result = metric.calculate([1, 2, 3])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_advanced_metrics.py:1342: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\metrics\comparative_analysis.py:230: in calculate
    head_to_head = self.calculate_head_to_head_score(data)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.metrics.comparative_analysis.ComparativeAnalysisEngine object at 0x0000016F8C1CE4B0>
data = [1, 2, 3]

    def calculate_head_to_head_score(self, data: Dict[str, Any]) -> float:
        """
        Calculate head-to-head comparison score.
    
        Args:
            data: Data containing head-to-head comparison metrics
    
        Returns:
            Head-to-head comparison score
        """
>       comparisons = data.get("head_to_head_comparisons", [])
                      ^^^^^^^^
E       AttributeError: 'list' object has no attribute 'get'

benchmarking\metrics\comparative_analysis.py:451: AttributeError
______________________ TestCrossDomainMetrics.test_init _______________________

self = <tests.unit.test_advanced_metrics.TestCrossDomainMetrics object at 0x0000016F8B33F800>
cross_domain_metrics = <benchmarking.metrics.cross_domain.CrossDomainMetrics object at 0x0000016F8F662E40>

    def test_init(self, cross_domain_metrics):
        """Test CrossDomainMetrics initialization."""
        assert cross_domain_metrics.name == "cross_domain_performance"
        assert cross_domain_metrics.description is not None
        assert cross_domain_metrics.unit == "score"
        assert cross_domain_metrics.config.min_value == 0.0
        assert cross_domain_metrics.config.max_value == 100.0
>       assert cross_domain_metrics.config.target_value == 80.0
E       AssertionError: assert 75.0 == 80.0
E        +  where 75.0 = MetricConfig(name='cross_domain_performance', description='Cross-domain evaluation score', unit='score', min_value=0.0, max_value=100.0, target_value=75.0, weight=1.0, enabled=True).target_value
E        +    where MetricConfig(name='cross_domain_performance', description='Cross-domain evaluation score', unit='score', min_value=0.0, max_value=100.0, target_value=75.0, weight=1.0, enabled=True) = <benchmarking.metrics.cross_domain.CrossDomainMetrics object at 0x0000016F8F662E40>.config

tests\unit\test_advanced_metrics.py:1688: AssertionError
_ TestCrossDomainMetrics.test_calculate_multi_modal_integration[data0-expected_range0] _

self = <tests.unit.test_advanced_metrics.TestCrossDomainMetrics object at 0x0000016F8B33FEF0>
cross_domain_metrics = <benchmarking.metrics.cross_domain.CrossDomainMetrics object at 0x0000016F8F6609E0>
data = {'cross_modal_reasoning': 0.75, 'modality_balancing': 0.8, 'modality_fusion': 0.9, 'multi_modal_integration': 0.85}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "multi_modal_integration": 0.85,
                    "cross_modal_reasoning": 0.75,
                    "modality_fusion": 0.90,
                    "modality_balancing": 0.80,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "multi_modal_integration": 0.6,
                    "cross_modal_reasoning": 0.6,
                    "modality_fusion": 0.6,
                    "modality_balancing": 0.6,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_multi_modal_integration(self, cross_domain_metrics, data, expected_range):
        """Test multi-modal integration calculation with different data."""
>       result = cross_domain_metrics.calculate_multi_modal_integration(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CrossDomainMetrics' object has no attribute 'calculate_multi_modal_integration'

tests\unit\test_advanced_metrics.py:1745: AttributeError
_ TestCrossDomainMetrics.test_calculate_multi_modal_integration[data1-expected_range1] _

self = <tests.unit.test_advanced_metrics.TestCrossDomainMetrics object at 0x0000016F8B3600B0>
cross_domain_metrics = <benchmarking.metrics.cross_domain.CrossDomainMetrics object at 0x0000016F8E18DB50>
data = {'cross_modal_reasoning': 0.6, 'modality_balancing': 0.6, 'modality_fusion': 0.6, 'multi_modal_integration': 0.6}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "multi_modal_integration": 0.85,
                    "cross_modal_reasoning": 0.75,
                    "modality_fusion": 0.90,
                    "modality_balancing": 0.80,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "multi_modal_integration": 0.6,
                    "cross_modal_reasoning": 0.6,
                    "modality_fusion": 0.6,
                    "modality_balancing": 0.6,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_multi_modal_integration(self, cross_domain_metrics, data, expected_range):
        """Test multi-modal integration calculation with different data."""
>       result = cross_domain_metrics.calculate_multi_modal_integration(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CrossDomainMetrics' object has no attribute 'calculate_multi_modal_integration'

tests\unit\test_advanced_metrics.py:1745: AttributeError
_ TestCrossDomainMetrics.test_calculate_context_awareness[data0-expected_range0] _

self = <tests.unit.test_advanced_metrics.TestCrossDomainMetrics object at 0x0000016F8B360350>
cross_domain_metrics = <benchmarking.metrics.cross_domain.CrossDomainMetrics object at 0x0000016F8E18DC40>
data = {'context_switching': 0.85, 'multi_tasking': 0.8, 'resource_allocation': 0.9, 'task_prioritization': 0.75}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "context_switching": 0.85,
                    "task_prioritization": 0.75,
                    "resource_allocation": 0.90,
                    "multi_tasking": 0.80,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "context_switching": 0.4,
                    "task_prioritization": 0.4,
                    "resource_allocation": 0.4,
                    "multi_tasking": 0.4,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_context_awareness(self, cross_domain_metrics, data, expected_range):
        """Test context awareness calculation with different data."""
>       result = cross_domain_metrics.calculate_context_awareness(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CrossDomainMetrics' object has no attribute 'calculate_context_awareness'

tests\unit\test_advanced_metrics.py:1775: AttributeError
_ TestCrossDomainMetrics.test_calculate_context_awareness[data1-expected_range1] _

self = <tests.unit.test_advanced_metrics.TestCrossDomainMetrics object at 0x0000016F8B3604D0>
cross_domain_metrics = <benchmarking.metrics.cross_domain.CrossDomainMetrics object at 0x0000016F8F698EF0>
data = {'context_switching': 0.4, 'multi_tasking': 0.4, 'resource_allocation': 0.4, 'task_prioritization': 0.4}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "context_switching": 0.85,
                    "task_prioritization": 0.75,
                    "resource_allocation": 0.90,
                    "multi_tasking": 0.80,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "context_switching": 0.4,
                    "task_prioritization": 0.4,
                    "resource_allocation": 0.4,
                    "multi_tasking": 0.4,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_context_awareness(self, cross_domain_metrics, data, expected_range):
        """Test context awareness calculation with different data."""
>       result = cross_domain_metrics.calculate_context_awareness(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CrossDomainMetrics' object has no attribute 'calculate_context_awareness'

tests\unit\test_advanced_metrics.py:1775: AttributeError
_ TestCrossDomainMetrics.test_calculate_collaboration[data0-expected_range0] __

self = <tests.unit.test_advanced_metrics.TestCrossDomainMetrics object at 0x0000016F8B360770>
cross_domain_metrics = <benchmarking.metrics.cross_domain.CrossDomainMetrics object at 0x0000016F8F698CE0>
data = {'collaborative_effectiveness': 0.85, 'communication_clarity': 0.75, 'conflict_resolution': 0.8, 'team_coordination': 0.9}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "collaborative_effectiveness": 0.85,
                    "communication_clarity": 0.75,
                    "team_coordination": 0.90,
                    "conflict_resolution": 0.80,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "collaborative_effectiveness": 0.9,
                    "communication_clarity": 0.9,
                    "team_coordination": 0.9,
                    "conflict_resolution": 0.9,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_collaboration(self, cross_domain_metrics, data, expected_range):
        """Test collaboration calculation with different data."""
>       result = cross_domain_metrics.calculate_collaboration(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CrossDomainMetrics' object has no attribute 'calculate_collaboration'. Did you mean: 'calculate_generalization'?

tests\unit\test_advanced_metrics.py:1805: AttributeError
_ TestCrossDomainMetrics.test_calculate_collaboration[data1-expected_range1] __

self = <tests.unit.test_advanced_metrics.TestCrossDomainMetrics object at 0x0000016F8B3608F0>
cross_domain_metrics = <benchmarking.metrics.cross_domain.CrossDomainMetrics object at 0x0000016F8F698D70>
data = {'collaborative_effectiveness': 0.9, 'communication_clarity': 0.9, 'conflict_resolution': 0.9, 'team_coordination': 0.9}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "collaborative_effectiveness": 0.85,
                    "communication_clarity": 0.75,
                    "team_coordination": 0.90,
                    "conflict_resolution": 0.80,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "collaborative_effectiveness": 0.9,
                    "communication_clarity": 0.9,
                    "team_coordination": 0.9,
                    "conflict_resolution": 0.9,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_collaboration(self, cross_domain_metrics, data, expected_range):
        """Test collaboration calculation with different data."""
>       result = cross_domain_metrics.calculate_collaboration(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CrossDomainMetrics' object has no attribute 'calculate_collaboration'. Did you mean: 'calculate_generalization'?

tests\unit\test_advanced_metrics.py:1805: AttributeError
_________________ TestStatisticalAnalysisFramework.test_init __________________

self = <tests.unit.test_advanced_metrics.TestStatisticalAnalysisFramework object at 0x0000016F8B360FE0>
statistical_analysis_framework = <benchmarking.metrics.statistical_analysis.StatisticalAnalysisFramework object at 0x0000016F8F69A150>

    def test_init(self, statistical_analysis_framework):
        """Test StatisticalAnalysisFramework initialization."""
>       assert statistical_analysis_framework.name == "statistical_analysis"
E       AssertionError: assert 'statistical_...s_performance' == 'statistical_analysis'
E         
E         - statistical_analysis
E         + statistical_analysis_performance
E         ?                     ++++++++++++

tests\unit\test_advanced_metrics.py:1846: AssertionError
_ TestStatisticalAnalysisFramework.test_calculate_descriptive_statistics[data0-expected_range0] _

self = <tests.unit.test_advanced_metrics.TestStatisticalAnalysisFramework object at 0x0000016F8B3612B0>
statistical_analysis_framework = <benchmarking.metrics.statistical_analysis.StatisticalAnalysisFramework object at 0x0000016F8F69BF80>
data = {'mean': 0.85, 'median': 0.83, 'mode': 0.86, 'std_dev': 0.1, ...}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {"mean": 0.85, "median": 0.83, "mode": 0.86, "variance": 0.01, "std_dev": 0.1},
                (0.0, 100.0),
            ),
            (
                {"mean": 0.5, "median": 0.5, "mode": 0.5, "variance": 0.25, "std_dev": 0.5},
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_descriptive_statistics(
        self, statistical_analysis_framework, data, expected_range
    ):
        """Test descriptive statistics calculation with different data."""
>       result = statistical_analysis_framework.calculate_descriptive_statistics(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StatisticalAnalysisFramework' object has no attribute 'calculate_descriptive_statistics'

tests\unit\test_advanced_metrics.py:1870: AttributeError
_ TestStatisticalAnalysisFramework.test_calculate_descriptive_statistics[data1-expected_range1] _

self = <tests.unit.test_advanced_metrics.TestStatisticalAnalysisFramework object at 0x0000016F8B361430>
statistical_analysis_framework = <benchmarking.metrics.statistical_analysis.StatisticalAnalysisFramework object at 0x0000016F8F69B2C0>
data = {'mean': 0.5, 'median': 0.5, 'mode': 0.5, 'std_dev': 0.5, ...}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {"mean": 0.85, "median": 0.83, "mode": 0.86, "variance": 0.01, "std_dev": 0.1},
                (0.0, 100.0),
            ),
            (
                {"mean": 0.5, "median": 0.5, "mode": 0.5, "variance": 0.25, "std_dev": 0.5},
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_descriptive_statistics(
        self, statistical_analysis_framework, data, expected_range
    ):
        """Test descriptive statistics calculation with different data."""
>       result = statistical_analysis_framework.calculate_descriptive_statistics(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StatisticalAnalysisFramework' object has no attribute 'calculate_descriptive_statistics'

tests\unit\test_advanced_metrics.py:1870: AttributeError
_ TestStatisticalAnalysisFramework.test_calculate_inferential_statistics[data0-expected_range0] _

self = <tests.unit.test_advanced_metrics.TestStatisticalAnalysisFramework object at 0x0000016F8B3616D0>
statistical_analysis_framework = <benchmarking.metrics.statistical_analysis.StatisticalAnalysisFramework object at 0x0000016F8E18FB60>
data = {'confidence_level': 0.95, 'effect_size': 0.8, 'p_value': 0.01, 'statistical_power': 0.9}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "p_value": 0.01,
                    "confidence_level": 0.95,
                    "effect_size": 0.8,
                    "statistical_power": 0.9,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "p_value": 0.05,
                    "confidence_level": 0.99,
                    "effect_size": 0.5,
                    "statistical_power": 0.8,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_inferential_statistics(
        self, statistical_analysis_framework, data, expected_range
    ):
        """Test inferential statistics calculation with different data."""
>       result = statistical_analysis_framework.calculate_inferential_statistics(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StatisticalAnalysisFramework' object has no attribute 'calculate_inferential_statistics'

tests\unit\test_advanced_metrics.py:1902: AttributeError
_ TestStatisticalAnalysisFramework.test_calculate_inferential_statistics[data1-expected_range1] _

self = <tests.unit.test_advanced_metrics.TestStatisticalAnalysisFramework object at 0x0000016F8B361850>
statistical_analysis_framework = <benchmarking.metrics.statistical_analysis.StatisticalAnalysisFramework object at 0x0000016F8E18CFE0>
data = {'confidence_level': 0.99, 'effect_size': 0.5, 'p_value': 0.05, 'statistical_power': 0.8}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "p_value": 0.01,
                    "confidence_level": 0.95,
                    "effect_size": 0.8,
                    "statistical_power": 0.9,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "p_value": 0.05,
                    "confidence_level": 0.99,
                    "effect_size": 0.5,
                    "statistical_power": 0.8,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_inferential_statistics(
        self, statistical_analysis_framework, data, expected_range
    ):
        """Test inferential statistics calculation with different data."""
>       result = statistical_analysis_framework.calculate_inferential_statistics(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StatisticalAnalysisFramework' object has no attribute 'calculate_inferential_statistics'

tests\unit\test_advanced_metrics.py:1902: AttributeError
_ TestStatisticalAnalysisFramework.test_calculate_time_series_analysis[data0-expected_range0] _

self = <tests.unit.test_advanced_metrics.TestStatisticalAnalysisFramework object at 0x0000016F8B33DEB0>
statistical_analysis_framework = <benchmarking.metrics.statistical_analysis.StatisticalAnalysisFramework object at 0x0000016F8F660AD0>
data = {'autocorrelation': 0.3, 'forecast_accuracy': 0.9, 'seasonality': 0.2, 'trend_strength': 0.85}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "trend_strength": 0.85,
                    "seasonality": 0.2,
                    "autocorrelation": 0.3,
                    "forecast_accuracy": 0.9,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "trend_strength": 0.4,
                    "seasonality": 0.1,
                    "autocorrelation": 0.2,
                    "forecast_accuracy": 0.6,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_time_series_analysis(
        self, statistical_analysis_framework, data, expected_range
    ):
        """Test time series analysis calculation with different data."""
>       result = statistical_analysis_framework.calculate_time_series_analysis(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StatisticalAnalysisFramework' object has no attribute 'calculate_time_series_analysis'. Did you mean: 'calculate_power_analysis'?

tests\unit\test_advanced_metrics.py:1934: AttributeError
_ TestStatisticalAnalysisFramework.test_calculate_time_series_analysis[data1-expected_range1] _

self = <tests.unit.test_advanced_metrics.TestStatisticalAnalysisFramework object at 0x0000016F8B3614F0>
statistical_analysis_framework = <benchmarking.metrics.statistical_analysis.StatisticalAnalysisFramework object at 0x0000016F8E0B0B90>
data = {'autocorrelation': 0.2, 'forecast_accuracy': 0.6, 'seasonality': 0.1, 'trend_strength': 0.4}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "trend_strength": 0.85,
                    "seasonality": 0.2,
                    "autocorrelation": 0.3,
                    "forecast_accuracy": 0.9,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "trend_strength": 0.4,
                    "seasonality": 0.1,
                    "autocorrelation": 0.2,
                    "forecast_accuracy": 0.6,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_time_series_analysis(
        self, statistical_analysis_framework, data, expected_range
    ):
        """Test time series analysis calculation with different data."""
>       result = statistical_analysis_framework.calculate_time_series_analysis(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StatisticalAnalysisFramework' object has no attribute 'calculate_time_series_analysis'. Did you mean: 'calculate_power_analysis'?

tests\unit\test_advanced_metrics.py:1934: AttributeError
_ TestStatisticalAnalysisFramework.test_calculate_multivariate_analysis[data0-expected_range0] _

self = <tests.unit.test_advanced_metrics.TestStatisticalAnalysisFramework object at 0x0000016F8B360170>
statistical_analysis_framework = <benchmarking.metrics.statistical_analysis.StatisticalAnalysisFramework object at 0x0000016F8E1A3A40>
data = {'correlation_matrix': [[1.0, 0.8], [0.8, 1.0]], 'covariance_matrix': [[0.1, 0.08], [0.08, 0.1]], 'multivariate_normality': 0.9, 'outlier_detection': 0.95}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "correlation_matrix": [[1.0, 0.8], [0.8, 1.0]],
                    "covariance_matrix": [[0.1, 0.08], [0.08, 0.1]],
                    "multivariate_normality": 0.9,
                    "outlier_detection": 0.95,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "correlation_matrix": [[1.0, 0.2], [0.2, 1.0]],
                    "covariance_matrix": [[0.1, 0.02], [0.02, 0.1]],
                    "multivariate_normality": 0.7,
                    "outlier_detection": 0.8,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_multivariate_analysis(
        self, statistical_analysis_framework, data, expected_range
    ):
        """Test multivariate analysis calculation with different data."""
>       result = statistical_analysis_framework.calculate_multivariate_analysis(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StatisticalAnalysisFramework' object has no attribute 'calculate_multivariate_analysis'. Did you mean: 'calculate_correlation_analysis'?

tests\unit\test_advanced_metrics.py:1966: AttributeError
_ TestStatisticalAnalysisFramework.test_calculate_multivariate_analysis[data1-expected_range1] _

self = <tests.unit.test_advanced_metrics.TestStatisticalAnalysisFramework object at 0x0000016F8B361AF0>
statistical_analysis_framework = <benchmarking.metrics.statistical_analysis.StatisticalAnalysisFramework object at 0x0000016F90EBB8F0>
data = {'correlation_matrix': [[1.0, 0.2], [0.2, 1.0]], 'covariance_matrix': [[0.1, 0.02], [0.02, 0.1]], 'multivariate_normality': 0.7, 'outlier_detection': 0.8}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "correlation_matrix": [[1.0, 0.8], [0.8, 1.0]],
                    "covariance_matrix": [[0.1, 0.08], [0.08, 0.1]],
                    "multivariate_normality": 0.9,
                    "outlier_detection": 0.95,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "correlation_matrix": [[1.0, 0.2], [0.2, 1.0]],
                    "covariance_matrix": [[0.1, 0.02], [0.02, 0.1]],
                    "multivariate_normality": 0.7,
                    "outlier_detection": 0.8,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_multivariate_analysis(
        self, statistical_analysis_framework, data, expected_range
    ):
        """Test multivariate analysis calculation with different data."""
>       result = statistical_analysis_framework.calculate_multivariate_analysis(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StatisticalAnalysisFramework' object has no attribute 'calculate_multivariate_analysis'. Did you mean: 'calculate_correlation_analysis'?

tests\unit\test_advanced_metrics.py:1966: AttributeError
___________________ TestComparativeAnalysisEngine.test_init ___________________

self = <tests.unit.test_advanced_metrics.TestComparativeAnalysisEngine object at 0x0000016F8B3621E0>
comparative_analysis_engine = <benchmarking.metrics.comparative_analysis.ComparativeAnalysisEngine object at 0x0000016F90EB9CD0>

    def test_init(self, comparative_analysis_engine):
        """Test ComparativeAnalysisEngine initialization."""
>       assert comparative_analysis_engine.name == "comparative_analysis"
E       AssertionError: assert 'comparative_...s_performance' == 'comparative_analysis'
E         
E         - comparative_analysis
E         + comparative_analysis_performance
E         ?                     ++++++++++++

tests\unit\test_advanced_metrics.py:2007: AssertionError
_ TestComparativeAnalysisEngine.test_calculate_performance_comparison[data0-expected_range0] _

self = <tests.unit.test_advanced_metrics.TestComparativeAnalysisEngine object at 0x0000016F8B3624B0>
comparative_analysis_engine = <benchmarking.metrics.comparative_analysis.ComparativeAnalysisEngine object at 0x0000016F90EBB110>
data = {'baseline_performance': 0.7, 'current_performance': 0.85, 'improvement_rate': 0.214, 'regression_detection': False}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "baseline_performance": 0.7,
                    "current_performance": 0.85,
                    "improvement_rate": 0.214,
                    "regression_detection": False,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "baseline_performance": 0.9,
                    "current_performance": 0.7,
                    "improvement_rate": -0.222,
                    "regression_detection": True,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_performance_comparison(
        self, comparative_analysis_engine, data, expected_range
    ):
        """Test performance comparison calculation with different data."""
>       result = comparative_analysis_engine.calculate_performance_comparison(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ComparativeAnalysisEngine' object has no attribute 'calculate_performance_comparison'. Did you mean: 'calculate_performance_gap_score'?

tests\unit\test_advanced_metrics.py:2041: AttributeError
_ TestComparativeAnalysisEngine.test_calculate_performance_comparison[data1-expected_range1] _

self = <tests.unit.test_advanced_metrics.TestComparativeAnalysisEngine object at 0x0000016F8B362630>
comparative_analysis_engine = <benchmarking.metrics.comparative_analysis.ComparativeAnalysisEngine object at 0x0000016F8FD10050>
data = {'baseline_performance': 0.9, 'current_performance': 0.7, 'improvement_rate': -0.222, 'regression_detection': True}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "baseline_performance": 0.7,
                    "current_performance": 0.85,
                    "improvement_rate": 0.214,
                    "regression_detection": False,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "baseline_performance": 0.9,
                    "current_performance": 0.7,
                    "improvement_rate": -0.222,
                    "regression_detection": True,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_performance_comparison(
        self, comparative_analysis_engine, data, expected_range
    ):
        """Test performance comparison calculation with different data."""
>       result = comparative_analysis_engine.calculate_performance_comparison(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ComparativeAnalysisEngine' object has no attribute 'calculate_performance_comparison'. Did you mean: 'calculate_performance_gap_score'?

tests\unit\test_advanced_metrics.py:2041: AttributeError
_ TestComparativeAnalysisEngine.test_calculate_efficiency_effectiveness[data0-expected_range0] _

self = <tests.unit.test_advanced_metrics.TestComparativeAnalysisEngine object at 0x0000016F8B3628D0>
comparative_analysis_engine = <benchmarking.metrics.comparative_analysis.ComparativeAnalysisEngine object at 0x0000016F90EBBCE0>
data = {'effectiveness_metrics': {'accuracy': 0.9, 'quality': 0.85, 'reliability': 0.95}, 'efficiency_metrics': {'cpu': 0.9, 'memory': 0.85, 'time': 0.8}, 'tradeoff_analysis': {'memory_vs_quality': 0.85, 'time_vs_accuracy': 0.8}}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "efficiency_metrics": {"time": 0.8, "memory": 0.85, "cpu": 0.9},
                    "effectiveness_metrics": {
                        "accuracy": 0.9,
                        "quality": 0.85,
                        "reliability": 0.95,
                    },
                    "tradeoff_analysis": {"time_vs_accuracy": 0.8, "memory_vs_quality": 0.85},
                },
                (0.0, 100.0),
            ),
            (
                {
                    "efficiency_metrics": {"time": 0.5, "memory": 0.5, "cpu": 0.5},
                    "effectiveness_metrics": {"accuracy": 0.5, "quality": 0.5, "reliability": 0.5},
                    "tradeoff_analysis": {"time_vs_accuracy": 0.5, "memory_vs_quality": 0.5},
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_efficiency_effectiveness(
        self, comparative_analysis_engine, data, expected_range
    ):
        """Test efficiency-effectiveness calculation with different data."""
>       result = comparative_analysis_engine.calculate_efficiency_effectiveness(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ComparativeAnalysisEngine' object has no attribute 'calculate_efficiency_effectiveness'

tests\unit\test_advanced_metrics.py:2075: AttributeError
_ TestComparativeAnalysisEngine.test_calculate_efficiency_effectiveness[data1-expected_range1] _

self = <tests.unit.test_advanced_metrics.TestComparativeAnalysisEngine object at 0x0000016F8B362A50>
comparative_analysis_engine = <benchmarking.metrics.comparative_analysis.ComparativeAnalysisEngine object at 0x0000016F90EBA7B0>
data = {'effectiveness_metrics': {'accuracy': 0.5, 'quality': 0.5, 'reliability': 0.5}, 'efficiency_metrics': {'cpu': 0.5, 'memory': 0.5, 'time': 0.5}, 'tradeoff_analysis': {'memory_vs_quality': 0.5, 'time_vs_accuracy': 0.5}}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "efficiency_metrics": {"time": 0.8, "memory": 0.85, "cpu": 0.9},
                    "effectiveness_metrics": {
                        "accuracy": 0.9,
                        "quality": 0.85,
                        "reliability": 0.95,
                    },
                    "tradeoff_analysis": {"time_vs_accuracy": 0.8, "memory_vs_quality": 0.85},
                },
                (0.0, 100.0),
            ),
            (
                {
                    "efficiency_metrics": {"time": 0.5, "memory": 0.5, "cpu": 0.5},
                    "effectiveness_metrics": {"accuracy": 0.5, "quality": 0.5, "reliability": 0.5},
                    "tradeoff_analysis": {"time_vs_accuracy": 0.5, "memory_vs_quality": 0.5},
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_efficiency_effectiveness(
        self, comparative_analysis_engine, data, expected_range
    ):
        """Test efficiency-effectiveness calculation with different data."""
>       result = comparative_analysis_engine.calculate_efficiency_effectiveness(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ComparativeAnalysisEngine' object has no attribute 'calculate_efficiency_effectiveness'

tests\unit\test_advanced_metrics.py:2075: AttributeError
_ TestComparativeAnalysisEngine.test_calculate_cost_benefit_analysis[data0-expected_range0] _

self = <tests.unit.test_advanced_metrics.TestComparativeAnalysisEngine object at 0x0000016F8B362CF0>
comparative_analysis_engine = <benchmarking.metrics.comparative_analysis.ComparativeAnalysisEngine object at 0x0000016F8FD845C0>
data = {'benefit_analysis': {'performance': 0.9, 'reliability': 0.95, 'scalability': 0.85}, 'cost_analysis': {'deployment': 0.85, 'development': 0.8, 'maintenance': 0.9}, 'roi_calculation': 0.87}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "cost_analysis": {"development": 0.8, "deployment": 0.85, "maintenance": 0.9},
                    "benefit_analysis": {
                        "performance": 0.9,
                        "scalability": 0.85,
                        "reliability": 0.95,
                    },
                    "roi_calculation": 0.87,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "cost_analysis": {"development": 0.3, "deployment": 0.3, "maintenance": 0.3},
                    "benefit_analysis": {
                        "performance": 0.3,
                        "scalability": 0.3,
                        "reliability": 0.3,
                    },
                    "roi_calculation": 0.3,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_cost_benefit_analysis(
        self, comparative_analysis_engine, data, expected_range
    ):
        """Test cost-benefit analysis calculation with different data."""
>       result = comparative_analysis_engine.calculate_cost_benefit_analysis(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ComparativeAnalysisEngine' object has no attribute 'calculate_cost_benefit_analysis'

tests\unit\test_advanced_metrics.py:2113: AttributeError
_ TestComparativeAnalysisEngine.test_calculate_cost_benefit_analysis[data1-expected_range1] _

self = <tests.unit.test_advanced_metrics.TestComparativeAnalysisEngine object at 0x0000016F8B362E70>
comparative_analysis_engine = <benchmarking.metrics.comparative_analysis.ComparativeAnalysisEngine object at 0x0000016F8E0B3770>
data = {'benefit_analysis': {'performance': 0.3, 'reliability': 0.3, 'scalability': 0.3}, 'cost_analysis': {'deployment': 0.3, 'development': 0.3, 'maintenance': 0.3}, 'roi_calculation': 0.3}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "cost_analysis": {"development": 0.8, "deployment": 0.85, "maintenance": 0.9},
                    "benefit_analysis": {
                        "performance": 0.9,
                        "scalability": 0.85,
                        "reliability": 0.95,
                    },
                    "roi_calculation": 0.87,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "cost_analysis": {"development": 0.3, "deployment": 0.3, "maintenance": 0.3},
                    "benefit_analysis": {
                        "performance": 0.3,
                        "scalability": 0.3,
                        "reliability": 0.3,
                    },
                    "roi_calculation": 0.3,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_cost_benefit_analysis(
        self, comparative_analysis_engine, data, expected_range
    ):
        """Test cost-benefit analysis calculation with different data."""
>       result = comparative_analysis_engine.calculate_cost_benefit_analysis(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ComparativeAnalysisEngine' object has no attribute 'calculate_cost_benefit_analysis'

tests\unit\test_advanced_metrics.py:2113: AttributeError
_ TestComparativeAnalysisEngine.test_calculate_scalability_adaptability[data0-expected_range0] _

self = <tests.unit.test_advanced_metrics.TestComparativeAnalysisEngine object at 0x0000016F8B363110>
comparative_analysis_engine = <benchmarking.metrics.comparative_analysis.ComparativeAnalysisEngine object at 0x0000016F8F660950>
data = {'adaptability_metrics': {'customization': 0.9, 'flexibility': 0.85, 'integration': 0.8}, 'future_proofing': 0.87, 'scalability_metrics': {'elasticity': 0.95, 'horizontal': 0.9, 'vertical': 0.85}}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "scalability_metrics": {
                        "horizontal": 0.9,
                        "vertical": 0.85,
                        "elasticity": 0.95,
                    },
                    "adaptability_metrics": {
                        "flexibility": 0.85,
                        "customization": 0.9,
                        "integration": 0.8,
                    },
                    "future_proofing": 0.87,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "scalability_metrics": {"horizontal": 0.4, "vertical": 0.4, "elasticity": 0.4},
                    "adaptability_metrics": {
                        "flexibility": 0.4,
                        "customization": 0.4,
                        "integration": 0.4,
                    },
                    "future_proofing": 0.4,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_scalability_adaptability(
        self, comparative_analysis_engine, data, expected_range
    ):
        """Test scalability-adaptability calculation with different data."""
>       result = comparative_analysis_engine.calculate_scalability_adaptability(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ComparativeAnalysisEngine' object has no attribute 'calculate_scalability_adaptability'

tests\unit\test_advanced_metrics.py:2155: AttributeError
_ TestComparativeAnalysisEngine.test_calculate_scalability_adaptability[data1-expected_range1] _

self = <tests.unit.test_advanced_metrics.TestComparativeAnalysisEngine object at 0x0000016F8B363290>
comparative_analysis_engine = <benchmarking.metrics.comparative_analysis.ComparativeAnalysisEngine object at 0x0000016F8E18F020>
data = {'adaptability_metrics': {'customization': 0.4, 'flexibility': 0.4, 'integration': 0.4}, 'future_proofing': 0.4, 'scalability_metrics': {'elasticity': 0.4, 'horizontal': 0.4, 'vertical': 0.4}}
expected_range = (0.0, 100.0)

    @pytest.mark.parametrize(
        "data,expected_range",
        [
            (
                {
                    "scalability_metrics": {
                        "horizontal": 0.9,
                        "vertical": 0.85,
                        "elasticity": 0.95,
                    },
                    "adaptability_metrics": {
                        "flexibility": 0.85,
                        "customization": 0.9,
                        "integration": 0.8,
                    },
                    "future_proofing": 0.87,
                },
                (0.0, 100.0),
            ),
            (
                {
                    "scalability_metrics": {"horizontal": 0.4, "vertical": 0.4, "elasticity": 0.4},
                    "adaptability_metrics": {
                        "flexibility": 0.4,
                        "customization": 0.4,
                        "integration": 0.4,
                    },
                    "future_proofing": 0.4,
                },
                (0.0, 100.0),
            ),
        ],
    )
    def test_calculate_scalability_adaptability(
        self, comparative_analysis_engine, data, expected_range
    ):
        """Test scalability-adaptability calculation with different data."""
>       result = comparative_analysis_engine.calculate_scalability_adaptability(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ComparativeAnalysisEngine' object has no attribute 'calculate_scalability_adaptability'

tests\unit\test_advanced_metrics.py:2155: AttributeError
_____________________ test_low_conversion_decreases_price _____________________

event_bus = <fba_bench_core.event_bus.EventBus object at 0x0000016F8F69BF20>
toolbox = <fba_bench_core.services.toolbox_api_service.ToolboxAPIService object at 0x0000016F8F69A3C0>

    @pytest.mark.asyncio
    async def test_low_conversion_decreases_price(event_bus: EventBus, toolbox: ToolboxAPIService):
        asin = "B00BASL01"
        await seed_snapshot(event_bus, asin, 2000, 0.03)  # $20.00, low CR
    
        agent = BaselineAgentV1(agent_id="agent-1", toolbox=toolbox)
        resp = agent.decide(asin)
>       assert resp is not None
E       assert None is not None

tests\unit\test_baseline_agent_v1.py:61: AssertionError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:12.434 | INFO | fba_events.bus | [req=-] | Event published
------------------------------ Captured log call ------------------------------
INFO     fba_events.bus:bus.py:290 Event published
____________________ test_high_conversion_increases_price _____________________

event_bus = <fba_bench_core.event_bus.EventBus object at 0x0000016F8F698560>
toolbox = <fba_bench_core.services.toolbox_api_service.ToolboxAPIService object at 0x0000016F8F69B590>

    @pytest.mark.asyncio
    async def test_high_conversion_increases_price(event_bus: EventBus, toolbox: ToolboxAPIService):
        asin = "B00BASL02"
        await seed_snapshot(event_bus, asin, 2000, 0.25)  # $20.00, high CR
    
        agent = BaselineAgentV1(agent_id="agent-1", toolbox=toolbox)
        resp = agent.decide(asin)
>       assert resp is not None
E       assert None is not None

tests\unit\test_baseline_agent_v1.py:86: AssertionError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:12.505 | INFO | fba_events.bus | [req=-] | Event published
------------------------------ Captured log call ------------------------------
INFO     fba_events.bus:bus.py:290 Event published
__________________ test_ema_updates_single_asin_over_3_sales __________________

    @pytest.mark.asyncio
    async def test_ema_updates_single_asin_over_3_sales():
        # Config: alpha 0.5 for both to simplify expected math
        svc = BsrEngineV3Service(
            config={"ema_alpha_velocity": Decimal("0.5"), "ema_alpha_conversion": Decimal("0.5")}
        )
        bus = MockEventBus()
        await bus.start()
        try:
            await svc.start(bus)
    
            asin = "ASIN-TEST-1"
            # Three sales with constant conversion 0.5 (sold/demanded = 1/2)
            sales = [
                make_sale_event(asin, 2, 4),
                make_sale_event(asin, 4, 8),
                make_sale_event(asin, 6, 12),
            ]
            for ev in sales:
                bus.event_processed.clear()
>               await bus2.publish(ev)
                      ^^^^
E               NameError: name 'bus2' is not defined

tests\unit\test_bsr_engine_v3.py:109: NameError
___________________ test_relative_indices_after_min_samples ___________________

    @pytest.mark.asyncio
    async def test_relative_indices_after_min_samples():
        cfg = {
            "ema_alpha_velocity": Decimal("0.5"),
            "ema_alpha_conversion": Decimal("0.5"),
            "min_samples_to_index": 3,
            "index_floor": Decimal("0.01"),
            "index_ceiling": Decimal("100.0"),
            "smoothing_eps": Decimal("1e-9"),
        }
        svc = BsrEngineV3Service(config=cfg)
        bus = MockEventBus()
        await bus.start()
        try:
            await svc.start(bus)
            bus.event_processed.clear()
            # One competitor snapshot to initialize market EMAs
            comps = [
                comp_state("C1", Money.from_dollars("20.00"), 1000, 10.0),
                comp_state("C2", Money.from_dollars("25.00"), 2000, 20.0),
            ]
            await bus.publish(make_competitor_update(1, comps))
            await asyncio.wait_for(bus.event_processed.wait(), timeout=1)
    
            asin = "ASIN-IX"
            # Three sales to reach min_samples
            for ev in [
                make_sale_event(asin, 2, 4),
                make_sale_event(asin, 4, 8),
                make_sale_event(asin, 6, 12),
            ]:
                bus.event_processed.clear()
>               await bus2.publish(ev)
                      ^^^^
E               NameError: name 'bus2' is not defined

tests\unit\test_bsr_engine_v3.py:211: NameError
___________________ test_warning_threshold_tokens_per_tick ____________________

    @pytest.mark.asyncio
    async def test_warning_threshold_tokens_per_tick():
        # Setup EventBus with in-memory backend
        bus = EventBus()
        await bus.start()
        bus.start_recording()
    
        # Configure small per-tick token limit and 50% warning threshold
        enforcer = BudgetEnforcer(
            {
                "limits": {"total_tokens_per_tick": 100},
                "warning_threshold_pct": 0.5,
                "allow_soft_overage": False,
            }
        )
        await enforcer.start(bus)
    
        agent = "agent-a"
        # 30 + 25 = 55 tokens, crosses 50% threshold
        res = await enforcer.meter_api_call(agent, "research", tokens_prompt=30, tokens_completion=25)
        assert res["exceeded"] is False
    
        # Allow event loop to process published events
        await asyncio.sleep(0.05)
        rec = bus.get_recorded_events()
>       warnings = [e for e in rec if e.get("event_type") == "BudgetWarning"]
                               ^^^
E       TypeError: 'coroutine' object is not iterable

tests\unit\test_budget_metering.py:36: TypeError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:12.606 | INFO | constraints.token_counter | [req=-] | Initialized tiktoken encoding for model: gpt-3.5-turbo
2025-09-16 01:27:12.607 | INFO | fba_events.bus | [req=-] | Event published
------------------------------ Captured log call ------------------------------
INFO     constraints.token_counter:token_counter.py:194 Initialized tiktoken encoding for model: gpt-3.5-turbo
INFO     fba_events.bus:bus.py:290 Event published
___________________ test_hard_exceed_total_tokens_per_tick ____________________

    @pytest.mark.asyncio
    async def test_hard_exceed_total_tokens_per_tick():
        bus = EventBus()
        await bus.start()
        bus.start_recording()
    
        enforcer = BudgetEnforcer(
            {"limits": {"total_tokens_per_tick": 60}, "allow_soft_overage": False}
        )
        await enforcer.start(bus)
    
        agent = "agent-b"
        res = await enforcer.meter_api_call(agent, "planner", tokens_prompt=61, tokens_completion=0)
        assert res["exceeded"] is True
        assert res["severity"] == "hard_fail"
        assert res["limit"] == 60
        assert res["usage"] == 61
    
        await asyncio.sleep(0.05)
        rec = bus.get_recorded_events()
>       exceeded = [e for e in rec if e.get("event_type") == "BudgetExceeded"]
                               ^^^
E       TypeError: 'coroutine' object is not iterable

tests\unit\test_budget_metering.py:67: TypeError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:12.674 | INFO | constraints.token_counter | [req=-] | Initialized tiktoken encoding for model: gpt-3.5-turbo
2025-09-16 01:27:12.674 | INFO | fba_events.bus | [req=-] | Event published
------------------------------ Captured log call ------------------------------
INFO     constraints.token_counter:token_counter.py:194 Initialized tiktoken encoding for model: gpt-3.5-turbo
INFO     fba_events.bus:bus.py:290 Event published
-------------------------- Captured stdout teardown ---------------------------
2025-09-16 01:27:12.732 | ERROR | asyncio | [req=-] | Task was destroyed but it is pending!
task: <Task pending name='InMemoryEventBusRunner' coro=<InMemoryEventBus._runner() running at C:\Users\admin\Downloads\fba\fba_events\bus.py:309> wait_for=<Future pending cb=[Task.task_wakeup()]>>
2025-09-16 01:27:12.733 | ERROR | fba_events.bus | [req=-] | Unhandled exception in InMemoryEventBus runner: Event loop is closed
Traceback (most recent call last):
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 158, in get
    await getter
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 309, in _runner
    event, event_type, ts = await self._queue.get()
                            ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 160, in get
    getter.cancel()  # Just in case getter is not done yet.
    ^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 799, in call_soon
    self._check_closed()
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 545, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
_______________________ test_per_tool_calls_limit_tick ________________________

    @pytest.mark.asyncio
    async def test_per_tool_calls_limit_tick():
        bus = EventBus()
        await bus.start()
        bus.start_recording()
    
        enforcer = BudgetEnforcer(
            {"tool_limits": {"search": {"calls_per_tick": 2}}, "allow_soft_overage": False}
        )
        await enforcer.start(bus)
    
        agent = "agent-c"
        r1 = await enforcer.meter_api_call(agent, "search")
        assert not r1["exceeded"]
        r2 = await enforcer.meter_api_call(agent, "search")
        assert not r2["exceeded"]
        r3 = await enforcer.meter_api_call(agent, "search")
        assert r3["exceeded"] is True
    
        await asyncio.sleep(0.05)
        rec = bus.get_recorded_events()
>       exceeded = [e for e in rec if e.get("event_type") == "BudgetExceeded"]
                               ^^^
E       TypeError: 'coroutine' object is not iterable

tests\unit\test_budget_metering.py:97: TypeError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:12.739 | INFO | constraints.token_counter | [req=-] | Initialized tiktoken encoding for model: gpt-3.5-turbo
2025-09-16 01:27:12.739 | INFO | fba_events.bus | [req=-] | Event published
2025-09-16 01:27:12.739 | INFO | fba_events.bus | [req=-] | Event published
------------------------------ Captured log call ------------------------------
INFO     constraints.token_counter:token_counter.py:194 Initialized tiktoken encoding for model: gpt-3.5-turbo
INFO     fba_events.bus:bus.py:290 Event published
INFO     fba_events.bus:bus.py:290 Event published
_______________ test_update_reputation_score[event_input1-86.0] _______________

event_input = ReputationEvent(customer_id='cust2', type='policy_violation', severity='medium', weight=1.0, current_score=90.0, metadata=None)
expected_score = 86.0

    @pytest.mark.parametrize(
        "event_input, expected_score",
        [
            # Example 1: positive_review, high severity
            (
                ReputationEvent(
                    customer_id="cust1",
                    type="positive_review",
                    severity="high",
                    current_score=50.0,
                ),
                52.0,
            ),
            # Example 2: policy_violation
            (
                ReputationEvent(
                    customer_id="cust2",
                    type="policy_violation",
                    current_score=90.0,
                ),
                86.0,
            ),
            # Example 3: negative_review, high severity, weight=2
            (
                ReputationEvent(
                    customer_id="cust3",
                    type="negative_review",
                    severity="high",
                    weight=2.0,
                    current_score=10.0,
                ),
                4.0,
            ),
            # Additional test cases
            # sale_success
            (
                ReputationEvent(
                    customer_id="cust4",
                    type="sale_success",
                    current_score=75.0,
                ),
                75.3,
            ),
            # positive_review, medium severity (default)
            (
                ReputationEvent(
                    customer_id="cust5",
                    type="positive_review",
                    severity="medium",
                    current_score=60.0,
                ),
                61.0,
            ),
            # negative_review, medium severity
            (
                ReputationEvent(
                    customer_id="cust6",
                    type="negative_review",
                    severity="medium",
                    current_score=60.0,
                ),
                58.5,
            ),
            # dispute_approved
            (
                ReputationEvent(
                    customer_id="cust7",
                    type="dispute_approved",
                    current_score=80.0,
                ),
                80.5,
            ),
            # dispute_denied, medium severity
            (
                ReputationEvent(
                    customer_id="cust8",
                    type="dispute_denied",
                    severity="medium",
                    current_score=80.0,
                ),
                78.0,
            ),
            # dispute_denied, low severity
            (
                ReputationEvent(
                    customer_id="cust9",
                    type="dispute_denied",
                    severity="low",
                    current_score=80.0,
                ),
                79.0,
            ),
            # dispute_escalated
            (
                ReputationEvent(
                    customer_id="cust10",
                    type="dispute_escalated",
                    current_score=70.0,
                ),
                69.5,
            ),
            # late_shipping, high severity
            (
                ReputationEvent(
                    customer_id="cust11",
                    type="late_shipping",
                    severity="high",
                    current_score=70.0,
                ),
                69.0,
            ),
            # late_shipping, low severity
            (
                ReputationEvent(
                    customer_id="cust12",
                    type="late_shipping",
                    severity="low",
                    current_score=70.0,
                ),
                69.5,
            ),
            # return
            (
                ReputationEvent(
                    customer_id="cust13",
                    type="return",
                    current_score=70.0,
                ),
                69.3,
            ),
            # unknown type
            (
                ReputationEvent(
                    customer_id="cust14",
                    type="unknown_event_type",
                    current_score=70.0,
                ),
                70.0,
            ),
            # Test clipping at 0
            (
                ReputationEvent(
                    customer_id="cust15",
                    type="policy_violation",
                    weight=10.0,  # Large weight to force below 0
                    current_score=5.0,
                ),
                0.0,
            ),
            # Test clipping at 100
            (
                ReputationEvent(
                    customer_id="cust16",
                    type="positive_review",
                    severity="high",
                    weight=10.0,  # Large weight to force above 100
                    current_score=95.0,
                ),
                100.0,
            ),
            # Test with metadata (should be ignored)
            (
                ReputationEvent(
                    customer_id="cust17",
                    type="sale_success",
                    current_score=50.0,
                    metadata={"some_key": "some_value"},
                ),
                50.3,
            ),
        ],
    )
    def test_update_reputation_score(event_input: ReputationEvent, expected_score: float):
        """Tests the update_reputation_score method with various inputs."""
        # Create mock dependencies
        mock_event_bus = MagicMock()
        mock_world_store = MagicMock()
    
        # Create the service with mocked dependencies
        svc = CustomerReputationService(mock_event_bus, mock_world_store)
    
        result = svc.update_reputation_score(event_input)
>       assert result == expected_score
E       assert 88.0 == 86.0

tests\unit\test_customer_reputation_service.py:190: AssertionError
_______________ test_update_reputation_score[event_input6-80.5] _______________

event_input = ReputationEvent(customer_id='cust7', type='dispute_approved', severity='medium', weight=1.0, current_score=80.0, metadata=None)
expected_score = 80.5

    @pytest.mark.parametrize(
        "event_input, expected_score",
        [
            # Example 1: positive_review, high severity
            (
                ReputationEvent(
                    customer_id="cust1",
                    type="positive_review",
                    severity="high",
                    current_score=50.0,
                ),
                52.0,
            ),
            # Example 2: policy_violation
            (
                ReputationEvent(
                    customer_id="cust2",
                    type="policy_violation",
                    current_score=90.0,
                ),
                86.0,
            ),
            # Example 3: negative_review, high severity, weight=2
            (
                ReputationEvent(
                    customer_id="cust3",
                    type="negative_review",
                    severity="high",
                    weight=2.0,
                    current_score=10.0,
                ),
                4.0,
            ),
            # Additional test cases
            # sale_success
            (
                ReputationEvent(
                    customer_id="cust4",
                    type="sale_success",
                    current_score=75.0,
                ),
                75.3,
            ),
            # positive_review, medium severity (default)
            (
                ReputationEvent(
                    customer_id="cust5",
                    type="positive_review",
                    severity="medium",
                    current_score=60.0,
                ),
                61.0,
            ),
            # negative_review, medium severity
            (
                ReputationEvent(
                    customer_id="cust6",
                    type="negative_review",
                    severity="medium",
                    current_score=60.0,
                ),
                58.5,
            ),
            # dispute_approved
            (
                ReputationEvent(
                    customer_id="cust7",
                    type="dispute_approved",
                    current_score=80.0,
                ),
                80.5,
            ),
            # dispute_denied, medium severity
            (
                ReputationEvent(
                    customer_id="cust8",
                    type="dispute_denied",
                    severity="medium",
                    current_score=80.0,
                ),
                78.0,
            ),
            # dispute_denied, low severity
            (
                ReputationEvent(
                    customer_id="cust9",
                    type="dispute_denied",
                    severity="low",
                    current_score=80.0,
                ),
                79.0,
            ),
            # dispute_escalated
            (
                ReputationEvent(
                    customer_id="cust10",
                    type="dispute_escalated",
                    current_score=70.0,
                ),
                69.5,
            ),
            # late_shipping, high severity
            (
                ReputationEvent(
                    customer_id="cust11",
                    type="late_shipping",
                    severity="high",
                    current_score=70.0,
                ),
                69.0,
            ),
            # late_shipping, low severity
            (
                ReputationEvent(
                    customer_id="cust12",
                    type="late_shipping",
                    severity="low",
                    current_score=70.0,
                ),
                69.5,
            ),
            # return
            (
                ReputationEvent(
                    customer_id="cust13",
                    type="return",
                    current_score=70.0,
                ),
                69.3,
            ),
            # unknown type
            (
                ReputationEvent(
                    customer_id="cust14",
                    type="unknown_event_type",
                    current_score=70.0,
                ),
                70.0,
            ),
            # Test clipping at 0
            (
                ReputationEvent(
                    customer_id="cust15",
                    type="policy_violation",
                    weight=10.0,  # Large weight to force below 0
                    current_score=5.0,
                ),
                0.0,
            ),
            # Test clipping at 100
            (
                ReputationEvent(
                    customer_id="cust16",
                    type="positive_review",
                    severity="high",
                    weight=10.0,  # Large weight to force above 100
                    current_score=95.0,
                ),
                100.0,
            ),
            # Test with metadata (should be ignored)
            (
                ReputationEvent(
                    customer_id="cust17",
                    type="sale_success",
                    current_score=50.0,
                    metadata={"some_key": "some_value"},
                ),
                50.3,
            ),
        ],
    )
    def test_update_reputation_score(event_input: ReputationEvent, expected_score: float):
        """Tests the update_reputation_score method with various inputs."""
        # Create mock dependencies
        mock_event_bus = MagicMock()
        mock_world_store = MagicMock()
    
        # Create the service with mocked dependencies
        svc = CustomerReputationService(mock_event_bus, mock_world_store)
    
        result = svc.update_reputation_score(event_input)
>       assert result == expected_score
E       assert 79.0 == 80.5

tests\unit\test_customer_reputation_service.py:190: AssertionError
_______________ test_update_reputation_score[event_input7-78.0] _______________

event_input = ReputationEvent(customer_id='cust8', type='dispute_denied', severity='medium', weight=1.0, current_score=80.0, metadata=None)
expected_score = 78.0

    @pytest.mark.parametrize(
        "event_input, expected_score",
        [
            # Example 1: positive_review, high severity
            (
                ReputationEvent(
                    customer_id="cust1",
                    type="positive_review",
                    severity="high",
                    current_score=50.0,
                ),
                52.0,
            ),
            # Example 2: policy_violation
            (
                ReputationEvent(
                    customer_id="cust2",
                    type="policy_violation",
                    current_score=90.0,
                ),
                86.0,
            ),
            # Example 3: negative_review, high severity, weight=2
            (
                ReputationEvent(
                    customer_id="cust3",
                    type="negative_review",
                    severity="high",
                    weight=2.0,
                    current_score=10.0,
                ),
                4.0,
            ),
            # Additional test cases
            # sale_success
            (
                ReputationEvent(
                    customer_id="cust4",
                    type="sale_success",
                    current_score=75.0,
                ),
                75.3,
            ),
            # positive_review, medium severity (default)
            (
                ReputationEvent(
                    customer_id="cust5",
                    type="positive_review",
                    severity="medium",
                    current_score=60.0,
                ),
                61.0,
            ),
            # negative_review, medium severity
            (
                ReputationEvent(
                    customer_id="cust6",
                    type="negative_review",
                    severity="medium",
                    current_score=60.0,
                ),
                58.5,
            ),
            # dispute_approved
            (
                ReputationEvent(
                    customer_id="cust7",
                    type="dispute_approved",
                    current_score=80.0,
                ),
                80.5,
            ),
            # dispute_denied, medium severity
            (
                ReputationEvent(
                    customer_id="cust8",
                    type="dispute_denied",
                    severity="medium",
                    current_score=80.0,
                ),
                78.0,
            ),
            # dispute_denied, low severity
            (
                ReputationEvent(
                    customer_id="cust9",
                    type="dispute_denied",
                    severity="low",
                    current_score=80.0,
                ),
                79.0,
            ),
            # dispute_escalated
            (
                ReputationEvent(
                    customer_id="cust10",
                    type="dispute_escalated",
                    current_score=70.0,
                ),
                69.5,
            ),
            # late_shipping, high severity
            (
                ReputationEvent(
                    customer_id="cust11",
                    type="late_shipping",
                    severity="high",
                    current_score=70.0,
                ),
                69.0,
            ),
            # late_shipping, low severity
            (
                ReputationEvent(
                    customer_id="cust12",
                    type="late_shipping",
                    severity="low",
                    current_score=70.0,
                ),
                69.5,
            ),
            # return
            (
                ReputationEvent(
                    customer_id="cust13",
                    type="return",
                    current_score=70.0,
                ),
                69.3,
            ),
            # unknown type
            (
                ReputationEvent(
                    customer_id="cust14",
                    type="unknown_event_type",
                    current_score=70.0,
                ),
                70.0,
            ),
            # Test clipping at 0
            (
                ReputationEvent(
                    customer_id="cust15",
                    type="policy_violation",
                    weight=10.0,  # Large weight to force below 0
                    current_score=5.0,
                ),
                0.0,
            ),
            # Test clipping at 100
            (
                ReputationEvent(
                    customer_id="cust16",
                    type="positive_review",
                    severity="high",
                    weight=10.0,  # Large weight to force above 100
                    current_score=95.0,
                ),
                100.0,
            ),
            # Test with metadata (should be ignored)
            (
                ReputationEvent(
                    customer_id="cust17",
                    type="sale_success",
                    current_score=50.0,
                    metadata={"some_key": "some_value"},
                ),
                50.3,
            ),
        ],
    )
    def test_update_reputation_score(event_input: ReputationEvent, expected_score: float):
        """Tests the update_reputation_score method with various inputs."""
        # Create mock dependencies
        mock_event_bus = MagicMock()
        mock_world_store = MagicMock()
    
        # Create the service with mocked dependencies
        svc = CustomerReputationService(mock_event_bus, mock_world_store)
    
        result = svc.update_reputation_score(event_input)
>       assert result == expected_score
E       assert 81.0 == 78.0

tests\unit\test_customer_reputation_service.py:190: AssertionError
_______________ test_update_reputation_score[event_input8-79.0] _______________

event_input = ReputationEvent(customer_id='cust9', type='dispute_denied', severity='low', weight=1.0, current_score=80.0, metadata=None)
expected_score = 79.0

    @pytest.mark.parametrize(
        "event_input, expected_score",
        [
            # Example 1: positive_review, high severity
            (
                ReputationEvent(
                    customer_id="cust1",
                    type="positive_review",
                    severity="high",
                    current_score=50.0,
                ),
                52.0,
            ),
            # Example 2: policy_violation
            (
                ReputationEvent(
                    customer_id="cust2",
                    type="policy_violation",
                    current_score=90.0,
                ),
                86.0,
            ),
            # Example 3: negative_review, high severity, weight=2
            (
                ReputationEvent(
                    customer_id="cust3",
                    type="negative_review",
                    severity="high",
                    weight=2.0,
                    current_score=10.0,
                ),
                4.0,
            ),
            # Additional test cases
            # sale_success
            (
                ReputationEvent(
                    customer_id="cust4",
                    type="sale_success",
                    current_score=75.0,
                ),
                75.3,
            ),
            # positive_review, medium severity (default)
            (
                ReputationEvent(
                    customer_id="cust5",
                    type="positive_review",
                    severity="medium",
                    current_score=60.0,
                ),
                61.0,
            ),
            # negative_review, medium severity
            (
                ReputationEvent(
                    customer_id="cust6",
                    type="negative_review",
                    severity="medium",
                    current_score=60.0,
                ),
                58.5,
            ),
            # dispute_approved
            (
                ReputationEvent(
                    customer_id="cust7",
                    type="dispute_approved",
                    current_score=80.0,
                ),
                80.5,
            ),
            # dispute_denied, medium severity
            (
                ReputationEvent(
                    customer_id="cust8",
                    type="dispute_denied",
                    severity="medium",
                    current_score=80.0,
                ),
                78.0,
            ),
            # dispute_denied, low severity
            (
                ReputationEvent(
                    customer_id="cust9",
                    type="dispute_denied",
                    severity="low",
                    current_score=80.0,
                ),
                79.0,
            ),
            # dispute_escalated
            (
                ReputationEvent(
                    customer_id="cust10",
                    type="dispute_escalated",
                    current_score=70.0,
                ),
                69.5,
            ),
            # late_shipping, high severity
            (
                ReputationEvent(
                    customer_id="cust11",
                    type="late_shipping",
                    severity="high",
                    current_score=70.0,
                ),
                69.0,
            ),
            # late_shipping, low severity
            (
                ReputationEvent(
                    customer_id="cust12",
                    type="late_shipping",
                    severity="low",
                    current_score=70.0,
                ),
                69.5,
            ),
            # return
            (
                ReputationEvent(
                    customer_id="cust13",
                    type="return",
                    current_score=70.0,
                ),
                69.3,
            ),
            # unknown type
            (
                ReputationEvent(
                    customer_id="cust14",
                    type="unknown_event_type",
                    current_score=70.0,
                ),
                70.0,
            ),
            # Test clipping at 0
            (
                ReputationEvent(
                    customer_id="cust15",
                    type="policy_violation",
                    weight=10.0,  # Large weight to force below 0
                    current_score=5.0,
                ),
                0.0,
            ),
            # Test clipping at 100
            (
                ReputationEvent(
                    customer_id="cust16",
                    type="positive_review",
                    severity="high",
                    weight=10.0,  # Large weight to force above 100
                    current_score=95.0,
                ),
                100.0,
            ),
            # Test with metadata (should be ignored)
            (
                ReputationEvent(
                    customer_id="cust17",
                    type="sale_success",
                    current_score=50.0,
                    metadata={"some_key": "some_value"},
                ),
                50.3,
            ),
        ],
    )
    def test_update_reputation_score(event_input: ReputationEvent, expected_score: float):
        """Tests the update_reputation_score method with various inputs."""
        # Create mock dependencies
        mock_event_bus = MagicMock()
        mock_world_store = MagicMock()
    
        # Create the service with mocked dependencies
        svc = CustomerReputationService(mock_event_bus, mock_world_store)
    
        result = svc.update_reputation_score(event_input)
>       assert result == expected_score
E       assert 80.5 == 79.0

tests\unit\test_customer_reputation_service.py:190: AssertionError
-------------------------- Captured stdout teardown ---------------------------
2025-09-16 01:27:12.888 | ERROR | asyncio | [req=-] | Task was destroyed but it is pending!
task: <Task pending name='InMemoryEventBusRunner' coro=<InMemoryEventBus._runner() running at C:\Users\admin\Downloads\fba\fba_events\bus.py:309> wait_for=<Future pending cb=[Task.task_wakeup()]>>
2025-09-16 01:27:12.888 | ERROR | fba_events.bus | [req=-] | Unhandled exception in InMemoryEventBus runner: Event loop is closed
Traceback (most recent call last):
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\pathlib.py", line 441, in __str__
    return self._str
           ^^^^^^^^^
AttributeError: 'WindowsPath' object has no attribute '_str'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 158, in get
    await getter
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 309, in _runner
    event, event_type, ts = await self._queue.get()
                            ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 160, in get
    getter.cancel()  # Just in case getter is not done yet.
    ^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 799, in call_soon
    self._check_closed()
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 545, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
_______________ test_update_reputation_score[event_input9-69.5] _______________

event_input = ReputationEvent(customer_id='cust10', type='dispute_escalated', severity='medium', weight=1.0, current_score=70.0, metadata=None)
expected_score = 69.5

    @pytest.mark.parametrize(
        "event_input, expected_score",
        [
            # Example 1: positive_review, high severity
            (
                ReputationEvent(
                    customer_id="cust1",
                    type="positive_review",
                    severity="high",
                    current_score=50.0,
                ),
                52.0,
            ),
            # Example 2: policy_violation
            (
                ReputationEvent(
                    customer_id="cust2",
                    type="policy_violation",
                    current_score=90.0,
                ),
                86.0,
            ),
            # Example 3: negative_review, high severity, weight=2
            (
                ReputationEvent(
                    customer_id="cust3",
                    type="negative_review",
                    severity="high",
                    weight=2.0,
                    current_score=10.0,
                ),
                4.0,
            ),
            # Additional test cases
            # sale_success
            (
                ReputationEvent(
                    customer_id="cust4",
                    type="sale_success",
                    current_score=75.0,
                ),
                75.3,
            ),
            # positive_review, medium severity (default)
            (
                ReputationEvent(
                    customer_id="cust5",
                    type="positive_review",
                    severity="medium",
                    current_score=60.0,
                ),
                61.0,
            ),
            # negative_review, medium severity
            (
                ReputationEvent(
                    customer_id="cust6",
                    type="negative_review",
                    severity="medium",
                    current_score=60.0,
                ),
                58.5,
            ),
            # dispute_approved
            (
                ReputationEvent(
                    customer_id="cust7",
                    type="dispute_approved",
                    current_score=80.0,
                ),
                80.5,
            ),
            # dispute_denied, medium severity
            (
                ReputationEvent(
                    customer_id="cust8",
                    type="dispute_denied",
                    severity="medium",
                    current_score=80.0,
                ),
                78.0,
            ),
            # dispute_denied, low severity
            (
                ReputationEvent(
                    customer_id="cust9",
                    type="dispute_denied",
                    severity="low",
                    current_score=80.0,
                ),
                79.0,
            ),
            # dispute_escalated
            (
                ReputationEvent(
                    customer_id="cust10",
                    type="dispute_escalated",
                    current_score=70.0,
                ),
                69.5,
            ),
            # late_shipping, high severity
            (
                ReputationEvent(
                    customer_id="cust11",
                    type="late_shipping",
                    severity="high",
                    current_score=70.0,
                ),
                69.0,
            ),
            # late_shipping, low severity
            (
                ReputationEvent(
                    customer_id="cust12",
                    type="late_shipping",
                    severity="low",
                    current_score=70.0,
                ),
                69.5,
            ),
            # return
            (
                ReputationEvent(
                    customer_id="cust13",
                    type="return",
                    current_score=70.0,
                ),
                69.3,
            ),
            # unknown type
            (
                ReputationEvent(
                    customer_id="cust14",
                    type="unknown_event_type",
                    current_score=70.0,
                ),
                70.0,
            ),
            # Test clipping at 0
            (
                ReputationEvent(
                    customer_id="cust15",
                    type="policy_violation",
                    weight=10.0,  # Large weight to force below 0
                    current_score=5.0,
                ),
                0.0,
            ),
            # Test clipping at 100
            (
                ReputationEvent(
                    customer_id="cust16",
                    type="positive_review",
                    severity="high",
                    weight=10.0,  # Large weight to force above 100
                    current_score=95.0,
                ),
                100.0,
            ),
            # Test with metadata (should be ignored)
            (
                ReputationEvent(
                    customer_id="cust17",
                    type="sale_success",
                    current_score=50.0,
                    metadata={"some_key": "some_value"},
                ),
                50.3,
            ),
        ],
    )
    def test_update_reputation_score(event_input: ReputationEvent, expected_score: float):
        """Tests the update_reputation_score method with various inputs."""
        # Create mock dependencies
        mock_event_bus = MagicMock()
        mock_world_store = MagicMock()
    
        # Create the service with mocked dependencies
        svc = CustomerReputationService(mock_event_bus, mock_world_store)
    
        result = svc.update_reputation_score(event_input)
>       assert result == expected_score
E       assert 69.0 == 69.5

tests\unit\test_customer_reputation_service.py:190: AssertionError
______________ test_update_reputation_score[event_input10-69.0] _______________

event_input = ReputationEvent(customer_id='cust11', type='late_shipping', severity='high', weight=1.0, current_score=70.0, metadata=None)
expected_score = 69.0

    @pytest.mark.parametrize(
        "event_input, expected_score",
        [
            # Example 1: positive_review, high severity
            (
                ReputationEvent(
                    customer_id="cust1",
                    type="positive_review",
                    severity="high",
                    current_score=50.0,
                ),
                52.0,
            ),
            # Example 2: policy_violation
            (
                ReputationEvent(
                    customer_id="cust2",
                    type="policy_violation",
                    current_score=90.0,
                ),
                86.0,
            ),
            # Example 3: negative_review, high severity, weight=2
            (
                ReputationEvent(
                    customer_id="cust3",
                    type="negative_review",
                    severity="high",
                    weight=2.0,
                    current_score=10.0,
                ),
                4.0,
            ),
            # Additional test cases
            # sale_success
            (
                ReputationEvent(
                    customer_id="cust4",
                    type="sale_success",
                    current_score=75.0,
                ),
                75.3,
            ),
            # positive_review, medium severity (default)
            (
                ReputationEvent(
                    customer_id="cust5",
                    type="positive_review",
                    severity="medium",
                    current_score=60.0,
                ),
                61.0,
            ),
            # negative_review, medium severity
            (
                ReputationEvent(
                    customer_id="cust6",
                    type="negative_review",
                    severity="medium",
                    current_score=60.0,
                ),
                58.5,
            ),
            # dispute_approved
            (
                ReputationEvent(
                    customer_id="cust7",
                    type="dispute_approved",
                    current_score=80.0,
                ),
                80.5,
            ),
            # dispute_denied, medium severity
            (
                ReputationEvent(
                    customer_id="cust8",
                    type="dispute_denied",
                    severity="medium",
                    current_score=80.0,
                ),
                78.0,
            ),
            # dispute_denied, low severity
            (
                ReputationEvent(
                    customer_id="cust9",
                    type="dispute_denied",
                    severity="low",
                    current_score=80.0,
                ),
                79.0,
            ),
            # dispute_escalated
            (
                ReputationEvent(
                    customer_id="cust10",
                    type="dispute_escalated",
                    current_score=70.0,
                ),
                69.5,
            ),
            # late_shipping, high severity
            (
                ReputationEvent(
                    customer_id="cust11",
                    type="late_shipping",
                    severity="high",
                    current_score=70.0,
                ),
                69.0,
            ),
            # late_shipping, low severity
            (
                ReputationEvent(
                    customer_id="cust12",
                    type="late_shipping",
                    severity="low",
                    current_score=70.0,
                ),
                69.5,
            ),
            # return
            (
                ReputationEvent(
                    customer_id="cust13",
                    type="return",
                    current_score=70.0,
                ),
                69.3,
            ),
            # unknown type
            (
                ReputationEvent(
                    customer_id="cust14",
                    type="unknown_event_type",
                    current_score=70.0,
                ),
                70.0,
            ),
            # Test clipping at 0
            (
                ReputationEvent(
                    customer_id="cust15",
                    type="policy_violation",
                    weight=10.0,  # Large weight to force below 0
                    current_score=5.0,
                ),
                0.0,
            ),
            # Test clipping at 100
            (
                ReputationEvent(
                    customer_id="cust16",
                    type="positive_review",
                    severity="high",
                    weight=10.0,  # Large weight to force above 100
                    current_score=95.0,
                ),
                100.0,
            ),
            # Test with metadata (should be ignored)
            (
                ReputationEvent(
                    customer_id="cust17",
                    type="sale_success",
                    current_score=50.0,
                    metadata={"some_key": "some_value"},
                ),
                50.3,
            ),
        ],
    )
    def test_update_reputation_score(event_input: ReputationEvent, expected_score: float):
        """Tests the update_reputation_score method with various inputs."""
        # Create mock dependencies
        mock_event_bus = MagicMock()
        mock_world_store = MagicMock()
    
        # Create the service with mocked dependencies
        svc = CustomerReputationService(mock_event_bus, mock_world_store)
    
        result = svc.update_reputation_score(event_input)
>       assert result == expected_score
E       assert 68.0 == 69.0

tests\unit\test_customer_reputation_service.py:190: AssertionError
__________________________ test_deployment_lifecycle __________________________

tmp_path = WindowsPath('C:/Users/admin/AppData/Local/Temp/pytest-of-admin/pytest-49/test_deployment_lifecycle0')

    @pytest.mark.asyncio
    async def test_deployment_lifecycle(tmp_path):
        dm = DeploymentManager()  # base_dir defaults to repo root (pytest cwd)
    
        config = {
            "name": "unit-test-benchmark",
            "type": "benchmark",
            "resources": {"cpu": 1, "memory": "256Mi", "storage": "1Gi"},
            "scaling": {"enabled": True, "min_instances": 1, "max_instances": 1},
        }
    
        deployment_id = await dm.deploy(config)
        assert isinstance(deployment_id, str) and deployment_id
    
        status = await dm.get_status(deployment_id)
        assert status["status"] == "running"
        assert status["name"] == "unit-test-benchmark"
        assert status["resources"]["cpu"] == 1
    
        ok = await dm.scale(deployment_id, 3)
>       assert ok is True
E       assert False is True

tests\unit\test_deployment_manager.py:26: AssertionError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:27:12.929 | INFO | infrastructure.deployment | [req=-] | DeploymentManager initialized (env=kubernetes, base_dir=C:\Users\admin\Downloads\fba)
2025-09-16 01:27:12.930 | INFO | infrastructure.deployment | [req=-] | Creating deployment unit-test-benchm-4d16c4c2 (unit-test-benchmark)
2025-09-16 01:27:12.935 | INFO | infrastructure.deployment | [req=-] | [unit-test-benchm-4d16c4c2] kubectl apply: kubectl apply -f C:\Users\admin\Downloads\fba\infrastructure\deployment\kubernetes.yaml
2025-09-16 01:27:33.095 | WARNING | infrastructure.deployment | [req=-] | [unit-test-benchm-4d16c4c2] kubectl apply failed (code=1)
2025-09-16 01:27:33.095 | WARNING | infrastructure.deployment | [req=-] | error: error validating "C:\\Users\\admin\\Downloads\\fba\\infrastructure\\deployment\\kubernetes.yaml": error validating data: failed to download openapi: Get "https://kubernetes.docker.internal:6443/openapi/v2?timeout=32s": EOF; if you choose to ignore these errors, turn validation off with --validate=false
2025-09-16 01:27:33.095 | INFO | infrastructure.deployment | [req=-] | Deployment unit-test-benchm-4d16c4c2 is running
2025-09-16 01:27:33.100 | INFO | infrastructure.deployment | [req=-] | [unit-test-benchm-4d16c4c2] docker compose scale: docker compose -f C:\Users\admin\Downloads\fba\infrastructure\deployment\docker-compose.yml up -d --scale benchmark=3
2025-09-16 01:27:33.223 | WARNING | infrastructure.deployment | [req=-] | [unit-test-benchm-4d16c4c2] docker compose scale failed (code=1)
2025-09-16 01:27:33.223 | WARNING | infrastructure.deployment | [req=-] | time="2025-09-15T18:27:33-07:00" level=warning msg="C:\\Users\\admin\\Downloads\\fba\\infrastructure\\deployment\\docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
no such service: benchmark: not found
------------------------------ Captured log call ------------------------------
INFO     infrastructure.deployment:__init__.py:118 DeploymentManager initialized (env=kubernetes, base_dir=C:\Users\admin\Downloads\fba)
INFO     infrastructure.deployment:__init__.py:148 Creating deployment unit-test-benchm-4d16c4c2 (unit-test-benchmark)
INFO     infrastructure.deployment:__init__.py:361 [unit-test-benchm-4d16c4c2] kubectl apply: kubectl apply -f C:\Users\admin\Downloads\fba\infrastructure\deployment\kubernetes.yaml
WARNING  infrastructure.deployment:__init__.py:389 [unit-test-benchm-4d16c4c2] kubectl apply failed (code=1)
WARNING  infrastructure.deployment:__init__.py:391 error: error validating "C:\\Users\\admin\\Downloads\\fba\\infrastructure\\deployment\\kubernetes.yaml": error validating data: failed to download openapi: Get "https://kubernetes.docker.internal:6443/openapi/v2?timeout=32s": EOF; if you choose to ignore these errors, turn validation off with --validate=false
INFO     infrastructure.deployment:__init__.py:161 Deployment unit-test-benchm-4d16c4c2 is running
INFO     infrastructure.deployment:__init__.py:361 [unit-test-benchm-4d16c4c2] docker compose scale: docker compose -f C:\Users\admin\Downloads\fba\infrastructure\deployment\docker-compose.yml up -d --scale benchmark=3
WARNING  infrastructure.deployment:__init__.py:389 [unit-test-benchm-4d16c4c2] docker compose scale failed (code=1)
WARNING  infrastructure.deployment:__init__.py:391 time="2025-09-15T18:27:33-07:00" level=warning msg="C:\\Users\\admin\\Downloads\\fba\\infrastructure\\deployment\\docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
no such service: benchmark: not found
____ TestDualMemoryManager.test_create_memory_store_with_unsupported_type _____

self = <tests.unit.test_dual_memory_manager.TestDualMemoryManager object at 0x0000016F8B3D05C0>
memory_manager = <memory_experiments.dual_memory_manager.DualMemoryManager object at 0x0000016F8E1A1970>

    @pytest.mark.asyncio
    async def test_create_memory_store_with_unsupported_type(self, memory_manager):
        """Test that _create_memory_store falls back to InMemoryStore for unsupported types."""
        # Create a mock unsupported store type
        unsupported_type = Mock()
        unsupported_type.value = "unsupported_store"
    
        with patch("memory_experiments.dual_memory_manager.logger") as mock_logger:
>           store = memory_manager._create_memory_store(unsupported_type)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_dual_memory_manager.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <memory_experiments.dual_memory_manager.DualMemoryManager object at 0x0000016F8E1A1970>
store_type = <Mock id='1578637074720'>

    def _create_memory_store(self, store_type: MemoryStoreType) -> MemoryStore:
        """
        Factory method to create memory store instances based on configuration.
        Raises ValueError for unsupported types to prevent silent configuration errors.
    
        Args:
            store_type: Type of memory store to create
    
        Returns:
            Instance of the requested memory store
        """
        if store_type == MemoryStoreType.IN_MEMORY:
            return InMemoryStore()
        # Add other store types here when implemented (e.g., if store_type == MemoryStoreType.VECTOR_DB:)
        else:
            # Raise an error instead of falling back to prevent silent misconfiguration
>           raise ValueError(
                f"Unsupported memory store type: {store_type}. Please configure a valid store."
            )
E           ValueError: Unsupported memory store type: <Mock id='1578637074720'>. Please configure a valid store.

memory_experiments\dual_memory_manager.py:388: ValueError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:15.099 | INFO | memory_experiments.dual_memory_manager | [req=-] | DualMemoryManager initialized for agent test_agent with mode MemoryMode.REFLECTION_ENABLED
----------------------------- Captured log setup ------------------------------
INFO     memory_experiments.dual_memory_manager:dual_memory_manager.py:229 DualMemoryManager initialized for agent test_agent with mode MemoryMode.REFLECTION_ENABLED
______ TestDualMemoryManager.test_retrieve_memories_with_memory_enabled _______

self = <tests.unit.test_dual_memory_manager.TestDualMemoryManager object at 0x0000016F8B3D0EC0>
memory_manager = <memory_experiments.dual_memory_manager.DualMemoryManager object at 0x0000016F8F6609E0>

    @pytest.mark.asyncio
    async def test_retrieve_memories_with_memory_enabled(self, memory_manager):
        """Test retrieving memories when memory is enabled."""
        # Create a mock event
        mock_event = Mock()
        mock_event.event_type = "TestEvent"
        mock_event.timestamp = datetime.now()
    
        # Store the event
>       await memory_manager.store_event(mock_event)

tests\unit\test_dual_memory_manager.py:174: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
memory_experiments\dual_memory_manager.py:251: in store_event
    memory_event.importance_score = await self._calculate_importance_score(memory_event)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <memory_experiments.dual_memory_manager.DualMemoryManager object at 0x0000016F8F6609E0>
memory = MemoryEvent(event_id='test_agent-0-TestEvent-2025-09-15T18:31:15.125727', event_type='TestEvent', content="<Mock id='1...None, domain='general', embedding=None, consolidation_score=0.0, promoted_to_long_term=False, promotion_timestamp=None)

    async def _calculate_importance_score(self, memory: MemoryEvent) -> float:
        """
        Calculate importance score for a memory event using configurable weights.
        """
>       base_score = self.config.base_importance_score
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'MemoryConfig' object has no attribute 'base_importance_score'

memory_experiments\dual_memory_manager.py:396: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:15.123 | INFO | memory_experiments.dual_memory_manager | [req=-] | DualMemoryManager initialized for agent test_agent with mode MemoryMode.REFLECTION_ENABLED
----------------------------- Captured log setup ------------------------------
INFO     memory_experiments.dual_memory_manager:dual_memory_manager.py:229 DualMemoryManager initialized for agent test_agent with mode MemoryMode.REFLECTION_ENABLED
________________ TestDualMemoryManager.test_get_memory_summary ________________

self = <tests.unit.test_dual_memory_manager.TestDualMemoryManager object at 0x0000016F8B3D1490>
memory_manager = <memory_experiments.dual_memory_manager.DualMemoryManager object at 0x0000016F8E18E9C0>

    @pytest.mark.asyncio
    async def test_get_memory_summary(self, memory_manager):
        """Test getting memory summary."""
        # Create a mock event
        mock_event = Mock()
        mock_event.event_type = "TestEvent"
        mock_event.timestamp = datetime.now()
    
        # Store the event
>       await memory_manager.store_event(mock_event)

tests\unit\test_dual_memory_manager.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
memory_experiments\dual_memory_manager.py:251: in store_event
    memory_event.importance_score = await self._calculate_importance_score(memory_event)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <memory_experiments.dual_memory_manager.DualMemoryManager object at 0x0000016F8E18E9C0>
memory = MemoryEvent(event_id='test_agent-0-TestEvent-2025-09-15T18:31:15.159796', event_type='TestEvent', content="<Mock id='1...None, domain='general', embedding=None, consolidation_score=0.0, promoted_to_long_term=False, promotion_timestamp=None)

    async def _calculate_importance_score(self, memory: MemoryEvent) -> float:
        """
        Calculate importance score for a memory event using configurable weights.
        """
>       base_score = self.config.base_importance_score
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'MemoryConfig' object has no attribute 'base_importance_score'

memory_experiments\dual_memory_manager.py:396: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:15.158 | INFO | memory_experiments.dual_memory_manager | [req=-] | DualMemoryManager initialized for agent test_agent with mode MemoryMode.REFLECTION_ENABLED
----------------------------- Captured log setup ------------------------------
INFO     memory_experiments.dual_memory_manager:dual_memory_manager.py:229 DualMemoryManager initialized for agent test_agent with mode MemoryMode.REFLECTION_ENABLED
__________________ TestDualMemoryManager.test_clear_memories __________________

self = <tests.unit.test_dual_memory_manager.TestDualMemoryManager object at 0x0000016F8B3D17C0>
memory_manager = <memory_experiments.dual_memory_manager.DualMemoryManager object at 0x0000016F8F15C830>

    @pytest.mark.asyncio
    async def test_clear_memories(self, memory_manager):
        """Test clearing memories."""
        # Create a mock event
        mock_event = Mock()
        mock_event.event_type = "TestEvent"
        mock_event.timestamp = datetime.now()
    
        # Store the event
>       await memory_manager.store_event(mock_event)

tests\unit\test_dual_memory_manager.py:237: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
memory_experiments\dual_memory_manager.py:251: in store_event
    memory_event.importance_score = await self._calculate_importance_score(memory_event)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <memory_experiments.dual_memory_manager.DualMemoryManager object at 0x0000016F8F15C830>
memory = MemoryEvent(event_id='test_agent-0-TestEvent-2025-09-15T18:31:15.190880', event_type='TestEvent', content="<Mock id='1...None, domain='general', embedding=None, consolidation_score=0.0, promoted_to_long_term=False, promotion_timestamp=None)

    async def _calculate_importance_score(self, memory: MemoryEvent) -> float:
        """
        Calculate importance score for a memory event using configurable weights.
        """
>       base_score = self.config.base_importance_score
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'MemoryConfig' object has no attribute 'base_importance_score'

memory_experiments\dual_memory_manager.py:396: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:15.189 | INFO | memory_experiments.dual_memory_manager | [req=-] | DualMemoryManager initialized for agent test_agent with mode MemoryMode.REFLECTION_ENABLED
----------------------------- Captured log setup ------------------------------
INFO     memory_experiments.dual_memory_manager:dual_memory_manager.py:229 DualMemoryManager initialized for agent test_agent with mode MemoryMode.REFLECTION_ENABLED
____________ TestDualMemoryManager.test_get_memories_for_promotion ____________

self = <tests.unit.test_dual_memory_manager.TestDualMemoryManager object at 0x0000016F8B3D2150>
memory_manager = <memory_experiments.dual_memory_manager.DualMemoryManager object at 0x0000016F8F15D9A0>

    @pytest.mark.asyncio
    async def test_get_memories_for_promotion(self, memory_manager):
        """Test getting memories for promotion."""
        # Create a mock event
        mock_event = Mock()
        mock_event.event_type = "TestEvent"
        mock_event.timestamp = datetime.now()
    
        # Store the event
>       await memory_manager.store_event(mock_event)

tests\unit\test_dual_memory_manager.py:299: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
memory_experiments\dual_memory_manager.py:251: in store_event
    memory_event.importance_score = await self._calculate_importance_score(memory_event)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <memory_experiments.dual_memory_manager.DualMemoryManager object at 0x0000016F8F15D9A0>
memory = MemoryEvent(event_id='test_agent-0-TestEvent-2025-09-15T18:31:15.227193', event_type='TestEvent', content="<Mock id='1...None, domain='general', embedding=None, consolidation_score=0.0, promoted_to_long_term=False, promotion_timestamp=None)

    async def _calculate_importance_score(self, memory: MemoryEvent) -> float:
        """
        Calculate importance score for a memory event using configurable weights.
        """
>       base_score = self.config.base_importance_score
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'MemoryConfig' object has no attribute 'base_importance_score'

memory_experiments\dual_memory_manager.py:396: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:15.226 | INFO | memory_experiments.dual_memory_manager | [req=-] | DualMemoryManager initialized for agent test_agent with mode MemoryMode.REFLECTION_ENABLED
----------------------------- Captured log setup ------------------------------
INFO     memory_experiments.dual_memory_manager:dual_memory_manager.py:229 DualMemoryManager initialized for agent test_agent with mode MemoryMode.REFLECTION_ENABLED
_________________ TestDualMemoryManager.test_promote_memories _________________

self = <tests.unit.test_dual_memory_manager.TestDualMemoryManager object at 0x0000016F8B3A5A00>
memory_manager = <memory_experiments.dual_memory_manager.DualMemoryManager object at 0x0000016F8F15FE30>

    @pytest.mark.asyncio
    async def test_promote_memories(self, memory_manager):
        """Test promoting memories from short-term to long-term storage."""
        # Create a mock event
        mock_event = Mock()
        mock_event.event_type = "TestEvent"
        mock_event.timestamp = datetime.now()
    
        # Store the event
>       await memory_manager.store_event(mock_event)

tests\unit\test_dual_memory_manager.py:319: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
memory_experiments\dual_memory_manager.py:251: in store_event
    memory_event.importance_score = await self._calculate_importance_score(memory_event)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <memory_experiments.dual_memory_manager.DualMemoryManager object at 0x0000016F8F15FE30>
memory = MemoryEvent(event_id='test_agent-0-TestEvent-2025-09-15T18:31:15.259502', event_type='TestEvent', content="<Mock id='1...None, domain='general', embedding=None, consolidation_score=0.0, promoted_to_long_term=False, promotion_timestamp=None)

    async def _calculate_importance_score(self, memory: MemoryEvent) -> float:
        """
        Calculate importance score for a memory event using configurable weights.
        """
>       base_score = self.config.base_importance_score
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'MemoryConfig' object has no attribute 'base_importance_score'

memory_experiments\dual_memory_manager.py:396: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:15.257 | INFO | memory_experiments.dual_memory_manager | [req=-] | DualMemoryManager initialized for agent test_agent with mode MemoryMode.REFLECTION_ENABLED
----------------------------- Captured log setup ------------------------------
INFO     memory_experiments.dual_memory_manager:dual_memory_manager.py:229 DualMemoryManager initialized for agent test_agent with mode MemoryMode.REFLECTION_ENABLED
_ TestBenchmarkEngine.test_execute_scenario_with_different_statuses[completed-completed] _

self = <tests.unit.test_engine.TestBenchmarkEngine object at 0x0000016F8B3FD160>
engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8F15C650>
test_config = {'agents': [{'config': {'model': 'test_model', 'temperature': 0.5}, 'enabled': True, 'framework': 'test', 'id': 'test_...ration', 'environment': {'deterministic': True, 'max_workers': 1, 'parallel_execution': False, 'random_seed': 42}, ...}
scenario_status = 'completed', expected_status = 'completed'

    @pytest.mark.asyncio
    @pytest.mark.parametrize(
        "scenario_status,expected_status",
        [("completed", "completed"), ("failed", "failed"), ("timeout", "timeout")],
    )
    async def test_execute_scenario_with_different_statuses(
        self, engine, test_config, scenario_status, expected_status
    ):
        """Test scenario execution with different status outcomes."""
        scenario_config = test_config["scenarios"][0]
        agent_config = test_config["agents"][0]
    
        # Mock scenario execution
        with patch.object(engine, "_load_scenario") as mock_load_scenario:
            mock_scenario = Mock()
    
            if scenario_status == "completed":
                mock_scenario.run.return_value = {
                    "status": "completed",
                    "metrics": {"score": 0.9},
                    "events": [],
                    "execution_time": 5.0,
                }
            elif scenario_status == "failed":
                mock_scenario.run.side_effect = Exception("Scenario failed")
            elif scenario_status == "timeout":
                mock_scenario.run.side_effect = asyncio.TimeoutError("Scenario timeout")
    
            mock_load_scenario.return_value = mock_scenario
    
            # Execute scenario
            result = await engine._execute_scenario(scenario_config, agent_config, test_config, 1)
    
            # Verify result
            assert result["scenario_id"] == "test_scenario"
>           assert result["status"] == expected_status
E           AssertionError: assert 'failed' == 'completed'
E             
E             - completed
E             + failed

tests\unit\test_engine.py:243: AssertionError
_ TestBenchmarkEngine.test_execute_scenario_with_different_statuses[timeout-timeout] _

self = <tests.unit.test_engine.TestBenchmarkEngine object at 0x0000016F8B3FD3D0>
engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8F15CE30>
test_config = {'agents': [{'config': {'model': 'test_model', 'temperature': 0.5}, 'enabled': True, 'framework': 'test', 'id': 'test_...ration', 'environment': {'deterministic': True, 'max_workers': 1, 'parallel_execution': False, 'random_seed': 42}, ...}
scenario_status = 'timeout', expected_status = 'timeout'

    @pytest.mark.asyncio
    @pytest.mark.parametrize(
        "scenario_status,expected_status",
        [("completed", "completed"), ("failed", "failed"), ("timeout", "timeout")],
    )
    async def test_execute_scenario_with_different_statuses(
        self, engine, test_config, scenario_status, expected_status
    ):
        """Test scenario execution with different status outcomes."""
        scenario_config = test_config["scenarios"][0]
        agent_config = test_config["agents"][0]
    
        # Mock scenario execution
        with patch.object(engine, "_load_scenario") as mock_load_scenario:
            mock_scenario = Mock()
    
            if scenario_status == "completed":
                mock_scenario.run.return_value = {
                    "status": "completed",
                    "metrics": {"score": 0.9},
                    "events": [],
                    "execution_time": 5.0,
                }
            elif scenario_status == "failed":
                mock_scenario.run.side_effect = Exception("Scenario failed")
            elif scenario_status == "timeout":
                mock_scenario.run.side_effect = asyncio.TimeoutError("Scenario timeout")
    
            mock_load_scenario.return_value = mock_scenario
    
            # Execute scenario
            result = await engine._execute_scenario(scenario_config, agent_config, test_config, 1)
    
            # Verify result
            assert result["scenario_id"] == "test_scenario"
>           assert result["status"] == expected_status
E           AssertionError: assert 'failed' == 'timeout'
E             
E             - timeout
E             + failed

tests\unit\test_engine.py:243: AssertionError
_________________ TestBenchmarkEngineExtended.test_initialize _________________

self = <tests.unit.test_engine.TestBenchmarkEngineExtended object at 0x0000016F8B40C920>
engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8F1235F0>
config_manager = <Mock spec='ConfigurationManager' id='1578653332832'>
integration_manager = <Mock spec='IntegrationManager' id='1578653332784'>

    @pytest.mark.asyncio
    async def test_initialize(self, engine, config_manager, integration_manager):
        """Test BenchmarkEngine initialization."""
        # Mock dependencies
>       config_manager.initialize.return_value = None
        ^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_engine.py:592: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <Mock spec='ConfigurationManager' id='1578653332832'>
name = 'initialize'

    def __getattr__(self, name):
        if name in {'_mock_methods', '_mock_unsafe'}:
            raise AttributeError(name)
        elif self._mock_methods is not None:
            if name not in self._mock_methods or name in _all_magics:
>               raise AttributeError("Mock object has no attribute %r" % name)
E               AttributeError: Mock object has no attribute 'initialize'

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:660: AttributeError
_______ TestBenchmarkEngineExtended.test_initialize_already_initialized _______

self = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8F1220F0>

    async def initialize(self) -> None:
        """
        Initialize configuration and integrations.
        Idempotent: safe to call multiple times.
        """
        try:
            # Allow sync or async implementations
>           await _maybe_await(self.config_manager.initialize())
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

benchmarking\core\engine.py:1375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <Mock spec='ConfigurationManager' id='1578653328224'>
name = 'initialize'

    def __getattr__(self, name):
        if name in {'_mock_methods', '_mock_unsafe'}:
            raise AttributeError(name)
        elif self._mock_methods is not None:
            if name not in self._mock_methods or name in _all_magics:
>               raise AttributeError("Mock object has no attribute %r" % name)
E               AttributeError: Mock object has no attribute 'initialize'

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:660: AttributeError

The above exception was the direct cause of the following exception:

self = <tests.unit.test_engine.TestBenchmarkEngineExtended object at 0x0000016F8B3FEE40>
engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8F1220F0>

    @pytest.mark.asyncio
    async def test_initialize_already_initialized(self, engine):
        """Test BenchmarkEngine initialization when already initialized."""
        engine._initialized = True
    
        # Initialize engine
>       await engine.initialize()

tests\unit\test_engine.py:609: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8F1220F0>

    async def initialize(self) -> None:
        """
        Initialize configuration and integrations.
        Idempotent: safe to call multiple times.
        """
        try:
            # Allow sync or async implementations
            await _maybe_await(self.config_manager.initialize())
            await _maybe_await(self.integration_manager.initialize())
            self._initialized = True
        except Exception as e:
            self._initialized = False
>           raise BenchmarkError(f"Failed to initialize benchmark engine: {e}") from e
E           benchmarking.core.engine.BenchmarkError: Failed to initialize benchmark engine: Mock object has no attribute 'initialize'

benchmarking\core\engine.py:1380: BenchmarkError
_________ TestBenchmarkEngineExtended.test_initialize_with_exception __________

self = <tests.unit.test_engine.TestBenchmarkEngineExtended object at 0x0000016F8B3FD340>
engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8F121610>
config_manager = <Mock spec='ConfigurationManager' id='1578653322272'>

    @pytest.mark.asyncio
    async def test_initialize_with_exception(self, engine, config_manager):
        """Test BenchmarkEngine initialization with exception."""
>       config_manager.initialize.side_effect = Exception("Config initialization failed")
        ^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_engine.py:617: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <Mock spec='ConfigurationManager' id='1578653322272'>
name = 'initialize'

    def __getattr__(self, name):
        if name in {'_mock_methods', '_mock_unsafe'}:
            raise AttributeError(name)
        elif self._mock_methods is not None:
            if name not in self._mock_methods or name in _all_magics:
>               raise AttributeError("Mock object has no attribute %r" % name)
E               AttributeError: Mock object has no attribute 'initialize'

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:660: AttributeError
___________ TestBenchmarkEngineExtended.test_save_benchmark_results ___________

self = <tests.unit.test_engine.TestBenchmarkEngineExtended object at 0x0000016F8B40DDF0>
engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8F123470>
test_config = {'agents': [{'config': {'model': 'test_model', 'temperature': 0.5}, 'enabled': True, 'framework': 'test', 'id': 'test_...ration', 'environment': {'deterministic': True, 'max_workers': 1, 'parallel_execution': False, 'random_seed': 42}, ...}

    @pytest.mark.asyncio
    async def test_save_benchmark_results(self, engine, test_config):
        """Test saving benchmark results."""
        # Create a test result
        result = BenchmarkResult(
            benchmark_id="test_benchmark",
            status=BenchmarkStatus.COMPLETED,
            overall_score=0.85,
            start_time=datetime.now(),
            end_time=datetime.now(),
            config=test_config,
            results={"scenario_results": []},
            metadata={"test": "metadata"},
        )
    
        # Create a temporary directory
        with tempfile.TemporaryDirectory() as temp_dir:
            # Mock config manager to return temp directory
>           engine.config_manager.get_output_path.return_value = temp_dir
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_engine.py:808: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <Mock spec='ConfigurationManager' id='1578653332640'>
name = 'get_output_path'

    def __getattr__(self, name):
        if name in {'_mock_methods', '_mock_unsafe'}:
            raise AttributeError(name)
        elif self._mock_methods is not None:
            if name not in self._mock_methods or name in _all_magics:
>               raise AttributeError("Mock object has no attribute %r" % name)
E               AttributeError: Mock object has no attribute 'get_output_path'

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:660: AttributeError
_____ TestBenchmarkEngineExtended.test_save_benchmark_results_with_error ______

self = <tests.unit.test_engine.TestBenchmarkEngineExtended object at 0x0000016F8B40E0F0>
engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8F1212B0>
test_config = {'agents': [{'config': {'model': 'test_model', 'temperature': 0.5}, 'enabled': True, 'framework': 'test', 'id': 'test_...ration', 'environment': {'deterministic': True, 'max_workers': 1, 'parallel_execution': False, 'random_seed': 42}, ...}

    @pytest.mark.asyncio
    async def test_save_benchmark_results_with_error(self, engine, test_config):
        """Test saving benchmark results with error."""
        # Create a test result
        result = BenchmarkResult(
            benchmark_id="test_benchmark",
            status=BenchmarkStatus.COMPLETED,
            overall_score=0.85,
            start_time=datetime.now(),
            end_time=datetime.now(),
            config=test_config,
            results={"scenario_results": []},
            metadata={"test": "metadata"},
        )
    
        # Mock config manager to raise exception
>       engine.config_manager.get_output_path.side_effect = Exception("Failed to get output path")
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_engine.py:841: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <Mock spec='ConfigurationManager' id='1578653334320'>
name = 'get_output_path'

    def __getattr__(self, name):
        if name in {'_mock_methods', '_mock_unsafe'}:
            raise AttributeError(name)
        elif self._mock_methods is not None:
            if name not in self._mock_methods or name in _all_magics:
>               raise AttributeError("Mock object has no attribute %r" % name)
E               AttributeError: Mock object has no attribute 'get_output_path'

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:660: AttributeError
_______________ TestBenchmarkEngineExtended.test_load_scenario ________________

self = <benchmarking.scenarios.registry.ScenarioRegistry object at 0x0000016F87731BE0>
scenario_id = 'test'

    def get(self, scenario_id: str) -> Type[Any]:
        """
        Retrieve a scenario class by its ID.
    
        Args:
            scenario_id: The unique string identifier for the scenario.
    
        Returns:
            The registered scenario class.
    
        Raises:
            KeyError: If no scenario with the given ID is registered.
        """
        try:
>           return self._scenarios[scenario_id]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           KeyError: 'test'

benchmarking\scenarios\registry.py:56: KeyError

During handling of the above exception, another exception occurred:

self = <tests.unit.test_engine.TestBenchmarkEngineExtended object at 0x0000016F8B40E3F0>
engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8FD0AD80>
test_config = {'agents': [{'config': {'model': 'test_model', 'temperature': 0.5}, 'enabled': True, 'framework': 'test', 'id': 'test_...ration', 'environment': {'deterministic': True, 'max_workers': 1, 'parallel_execution': False, 'random_seed': 42}, ...}

    @pytest.mark.asyncio
    async def test_load_scenario(self, engine, test_config):
        """Test loading a scenario."""
        scenario_config = test_config["scenarios"][0]
    
        # Mock scenario registry
        with patch("benchmarking.core.engine.scenario_registry") as mock_registry:
            mock_scenario = Mock()
            mock_registry.get_scenario.return_value = mock_scenario
    
            # Load scenario
>           scenario = await engine._load_scenario(scenario_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_engine.py:858: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\core\engine.py:1716: in _load_scenario
    obj = _scenario_registry.get(key)  # type: ignore[attr-defined]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.scenarios.registry.ScenarioRegistry object at 0x0000016F87731BE0>
scenario_id = 'test'

    def get(self, scenario_id: str) -> Type[Any]:
        """
        Retrieve a scenario class by its ID.
    
        Args:
            scenario_id: The unique string identifier for the scenario.
    
        Returns:
            The registered scenario class.
    
        Raises:
            KeyError: If no scenario with the given ID is registered.
        """
        try:
            return self._scenarios[scenario_id]
        except KeyError:
>           raise KeyError(f"No scenario registered with ID '{scenario_id}'.")
E           KeyError: "No scenario registered with ID 'test'."

benchmarking\scenarios\registry.py:58: KeyError
__________ TestBenchmarkEngineExtended.test_load_scenario_not_found ___________

self = <benchmarking.scenarios.registry.ScenarioRegistry object at 0x0000016F87731BE0>
scenario_id = 'test'

    def get(self, scenario_id: str) -> Type[Any]:
        """
        Retrieve a scenario class by its ID.
    
        Args:
            scenario_id: The unique string identifier for the scenario.
    
        Returns:
            The registered scenario class.
    
        Raises:
            KeyError: If no scenario with the given ID is registered.
        """
        try:
>           return self._scenarios[scenario_id]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           KeyError: 'test'

benchmarking\scenarios\registry.py:56: KeyError

During handling of the above exception, another exception occurred:

self = <tests.unit.test_engine.TestBenchmarkEngineExtended object at 0x0000016F8B40E6F0>
engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8E18D670>
test_config = {'agents': [{'config': {'model': 'test_model', 'temperature': 0.5}, 'enabled': True, 'framework': 'test', 'id': 'test_...ration', 'environment': {'deterministic': True, 'max_workers': 1, 'parallel_execution': False, 'random_seed': 42}, ...}

    @pytest.mark.asyncio
    async def test_load_scenario_not_found(self, engine, test_config):
        """Test loading a scenario that doesn't exist."""
        scenario_config = test_config["scenarios"][0]
    
        # Mock scenario registry
        with patch("benchmarking.core.engine.scenario_registry") as mock_registry:
            mock_registry.get_scenario.return_value = None
    
            # Load scenario and expect exception
            with pytest.raises(BenchmarkError, match="Scenario test not found"):
>               await engine._load_scenario(scenario_config)

tests\unit\test_engine.py:875: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\core\engine.py:1716: in _load_scenario
    obj = _scenario_registry.get(key)  # type: ignore[attr-defined]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.scenarios.registry.ScenarioRegistry object at 0x0000016F87731BE0>
scenario_id = 'test'

    def get(self, scenario_id: str) -> Type[Any]:
        """
        Retrieve a scenario class by its ID.
    
        Args:
            scenario_id: The unique string identifier for the scenario.
    
        Returns:
            The registered scenario class.
    
        Raises:
            KeyError: If no scenario with the given ID is registered.
        """
        try:
            return self._scenarios[scenario_id]
        except KeyError:
>           raise KeyError(f"No scenario registered with ID '{scenario_id}'.")
E           KeyError: "No scenario registered with ID 'test'."

benchmarking\scenarios\registry.py:58: KeyError
_________________ TestBenchmarkEngineExtended.test_load_agent _________________

self = <tests.unit.test_engine.TestBenchmarkEngineExtended object at 0x0000016F8B40E9F0>
engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8F19BAD0>
test_config = {'agents': [{'config': {'model': 'test_model', 'temperature': 0.5}, 'enabled': True, 'framework': 'test', 'id': 'test_...ration', 'environment': {'deterministic': True, 'max_workers': 1, 'parallel_execution': False, 'random_seed': 42}, ...}

    @pytest.mark.asyncio
    async def test_load_agent(self, engine, test_config):
        """Test loading an agent."""
        agent_config = test_config["agents"][0]
    
        # Mock agent registry
>       with patch("benchmarking.core.engine.agent_registry") as mock_registry:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_engine.py:883: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x0000016F8FD12930>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'benchmarking.core.engine' from 'C:\\Users\\admin\\Downloads\\fba\\benchmarking\\core\\engine.py'> does not have the attribute 'agent_registry'

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1437: AttributeError
____________ TestBenchmarkEngineExtended.test_load_agent_not_found ____________

self = <tests.unit.test_engine.TestBenchmarkEngineExtended object at 0x0000016F8B40ECF0>
engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8F1987A0>
test_config = {'agents': [{'config': {'model': 'test_model', 'temperature': 0.5}, 'enabled': True, 'framework': 'test', 'id': 'test_...ration', 'environment': {'deterministic': True, 'max_workers': 1, 'parallel_execution': False, 'random_seed': 42}, ...}

    @pytest.mark.asyncio
    async def test_load_agent_not_found(self, engine, test_config):
        """Test loading an agent that doesn't exist."""
        agent_config = test_config["agents"][0]
    
        # Mock agent registry
>       with patch("benchmarking.core.engine.agent_registry") as mock_registry:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_engine.py:900: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x0000016F8F1986E0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'benchmarking.core.engine' from 'C:\\Users\\admin\\Downloads\\fba\\benchmarking\\core\\engine.py'> does not have the attribute 'agent_registry'

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1437: AttributeError
-------------------------- Captured stdout teardown ---------------------------
2025-09-16 01:31:16.481 | ERROR | asyncio | [req=-] | Task was destroyed but it is pending!
task: <Task pending name='InMemoryEventBusRunner' coro=<InMemoryEventBus._runner() running at C:\Users\admin\Downloads\fba\fba_events\bus.py:309> wait_for=<Future pending cb=[Task.task_wakeup()]>>
2025-09-16 01:31:16.481 | ERROR | fba_events.bus | [req=-] | Unhandled exception in InMemoryEventBus runner: Event loop is closed
Traceback (most recent call last):
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 158, in get
    await getter
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 309, in _runner
    event, event_type, ts = await self._queue.get()
                            ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 160, in get
    getter.cancel()  # Just in case getter is not done yet.
    ^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 799, in call_soon
    self._check_closed()
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 545, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
________ TestBenchmarkEngineExtended.test_calculate_cognitive_metrics _________

self = <tests.unit.test_engine.TestBenchmarkEngineExtended object at 0x0000016F8B3FD970>
engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8E0E2570>

    @pytest.mark.asyncio
    async def test_calculate_cognitive_metrics(self, engine):
        """Test calculating cognitive metrics."""
        # Mock events and data
        events = [{"type": "DecisionMade", "data": {"quality": 0.9}}]
        agent_data = {"reasoning": {"accuracy": 0.8}}
        scenario_data = {"complexity": "medium"}
    
        # Mock metrics registry
>       with patch("benchmarking.core.engine.metrics_registry") as mock_registry:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_engine.py:916: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x0000016F8E0E1100>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'benchmarking.core.engine' from 'C:\\Users\\admin\\Downloads\\fba\\benchmarking\\core\\engine.py'> does not have the attribute 'metrics_registry'

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1437: AttributeError
_________ TestBenchmarkEngineExtended.test_calculate_business_metrics _________

self = <tests.unit.test_engine.TestBenchmarkEngineExtended object at 0x0000016F8B40F050>
engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8E1A1B80>

    @pytest.mark.asyncio
    async def test_calculate_business_metrics(self, engine):
        """Test calculating business metrics."""
        # Mock events and data
        events = [{"type": "TransactionCompleted", "data": {"value": 100}}]
        agent_data = {"efficiency": {"time_saved": 10}}
        scenario_data = {"domain": "finance"}
    
        # Mock metrics registry
>       with patch("benchmarking.core.engine.metrics_registry") as mock_registry:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_engine.py:938: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x0000016F8C1FCC80>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'benchmarking.core.engine' from 'C:\\Users\\admin\\Downloads\\fba\\benchmarking\\core\\engine.py'> does not have the attribute 'metrics_registry'

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1437: AttributeError
________ TestBenchmarkEngineExtended.test_calculate_technical_metrics _________

self = <tests.unit.test_engine.TestBenchmarkEngineExtended object at 0x0000016F8B40E540>
engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8E1A0770>

    @pytest.mark.asyncio
    async def test_calculate_technical_metrics(self, engine):
        """Test calculating technical metrics."""
        # Mock events and data
        events = [{"type": "ApiCall", "data": {"response_time": 100}}]
        agent_data = {"performance": {"memory_usage": 50}}
        scenario_data = {"environment": "test"}
    
        # Mock metrics registry
>       with patch("benchmarking.core.engine.metrics_registry") as mock_registry:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_engine.py:960: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x0000016F8E1A3290>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'benchmarking.core.engine' from 'C:\\Users\\admin\\Downloads\\fba\\benchmarking\\core\\engine.py'> does not have the attribute 'metrics_registry'

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1437: AttributeError
______ TestBenchmarkEngineExtended.test_calculate_metrics_with_exception ______

self = <tests.unit.test_engine.TestBenchmarkEngineExtended object at 0x0000016F8B40DEB0>
engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8E1A1430>

    @pytest.mark.asyncio
    async def test_calculate_metrics_with_exception(self, engine):
        """Test calculating metrics with exception."""
        # Mock metrics registry to raise exception
>       with patch("benchmarking.core.engine.metrics_registry") as mock_registry:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_engine.py:977: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x0000016F8E1A3FE0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'benchmarking.core.engine' from 'C:\\Users\\admin\\Downloads\\fba\\benchmarking\\core\\engine.py'> does not have the attribute 'metrics_registry'

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1437: AttributeError
_____ TestBenchmarkEngineExtended.test_aggregate_results_with_none_scores _____

self = <tests.unit.test_engine.TestBenchmarkEngineExtended object at 0x0000016F8B40CFE0>
engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8E136B10>

    def test_aggregate_results_with_none_scores(self, engine):
        """Test aggregating results with None scores."""
        run_results = [
            {
                "scenario_id": "test_scenario",
                "agent_id": "test_agent",
                "run_number": 1,
                "metrics": {"score": 0.85},
                "execution_time": 10.0,
                "status": "completed",
            },
            {
                "scenario_id": "test_scenario",
                "agent_id": "test_agent",
                "run_number": 2,
                "metrics": {"score": None},
                "execution_time": 12.0,
                "status": "completed",
            },
        ]
    
        aggregated = engine._aggregate_results(run_results)
    
        # Verify aggregation
>       assert aggregated["overall_score"] == 0.425  # Only first score counted
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       assert 0.85 == 0.425

tests\unit\test_engine.py:1022: AssertionError
___ TestBenchmarkEngineExtended.test_aggregate_results_with_missing_metrics ___

self = <tests.unit.test_engine.TestBenchmarkEngineExtended object at 0x0000016F8B40C7D0>
engine = <benchmarking.core.engine.BenchmarkEngine object at 0x0000016F8E135A60>

    def test_aggregate_results_with_missing_metrics(self, engine):
        """Test aggregating results with missing metrics."""
        run_results = [
            {
                "scenario_id": "test_scenario",
                "agent_id": "test_agent",
                "run_number": 1,
                "metrics": {"score": 0.85},
                "execution_time": 10.0,
                "status": "completed",
            },
            {
                "scenario_id": "test_scenario",
                "agent_id": "test_agent",
                "run_number": 2,
                "execution_time": 12.0,
                "status": "completed",
                # Missing metrics
            },
        ]
    
        aggregated = engine._aggregate_results(run_results)
    
        # Verify aggregation
>       assert aggregated["overall_score"] == 0.425  # Only first score counted
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       assert 0.85 == 0.425

tests\unit\test_engine.py:1050: AssertionError
________________ TestResourceManager.test_enforce_cost_limits _________________

self = <tests.unit.test_infrastructure_complete.TestResourceManager testMethod=test_enforce_cost_limits>

    def test_enforce_cost_limits(self):
        """Test enforcing cost limits."""
        self.resource_manager.set_global_token_cap(1000)
        self.resource_manager.record_llm_cost("gpt-4", 100.0, 1000)
    
        # This should raise an exception due to exceeding cost limits
>       with self.assertRaises(Exception):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: Exception not raised

tests\unit\test_infrastructure_complete.py:430: AssertionError
______________ TestDeploymentManager.test_deploy_docker_compose _______________

self = <tests.unit.test_infrastructure_complete.TestDeploymentManager testMethod=test_deploy_docker_compose>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.config = DeploymentConfig(
            environment=DeploymentEnvironment.LOCAL,
            deployment_type=DeploymentType.DOCKER_COMPOSE,
            resource_limits={"cpu": "2", "memory": "4GB"},
            scaling_config={"min_instances": 1, "max_instances": 3},
        )
E       TypeError: DeploymentConfig.__init__() got an unexpected keyword argument 'resource_limits'

tests\unit\test_infrastructure_complete.py:439: TypeError
________________ TestDeploymentManager.test_deploy_kubernetes _________________

self = <tests.unit.test_infrastructure_complete.TestDeploymentManager testMethod=test_deploy_kubernetes>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.config = DeploymentConfig(
            environment=DeploymentEnvironment.LOCAL,
            deployment_type=DeploymentType.DOCKER_COMPOSE,
            resource_limits={"cpu": "2", "memory": "4GB"},
            scaling_config={"min_instances": 1, "max_instances": 3},
        )
E       TypeError: DeploymentConfig.__init__() got an unexpected keyword argument 'resource_limits'

tests\unit\test_infrastructure_complete.py:439: TypeError
___________________ TestDeploymentManager.test_deploy_local ___________________

self = <tests.unit.test_infrastructure_complete.TestDeploymentManager testMethod=test_deploy_local>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.config = DeploymentConfig(
            environment=DeploymentEnvironment.LOCAL,
            deployment_type=DeploymentType.DOCKER_COMPOSE,
            resource_limits={"cpu": "2", "memory": "4GB"},
            scaling_config={"min_instances": 1, "max_instances": 3},
        )
E       TypeError: DeploymentConfig.__init__() got an unexpected keyword argument 'resource_limits'

tests\unit\test_infrastructure_complete.py:439: TypeError
________ TestDeploymentManager.test_deployment_manager_initialization _________

self = <tests.unit.test_infrastructure_complete.TestDeploymentManager testMethod=test_deployment_manager_initialization>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.config = DeploymentConfig(
            environment=DeploymentEnvironment.LOCAL,
            deployment_type=DeploymentType.DOCKER_COMPOSE,
            resource_limits={"cpu": "2", "memory": "4GB"},
            scaling_config={"min_instances": 1, "max_instances": 3},
        )
E       TypeError: DeploymentConfig.__init__() got an unexpected keyword argument 'resource_limits'

tests\unit\test_infrastructure_complete.py:439: TypeError
______________ TestDeploymentManager.test_set_default_resources _______________

self = <tests.unit.test_infrastructure_complete.TestDeploymentManager testMethod=test_set_default_resources>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.config = DeploymentConfig(
            environment=DeploymentEnvironment.LOCAL,
            deployment_type=DeploymentType.DOCKER_COMPOSE,
            resource_limits={"cpu": "2", "memory": "4GB"},
            scaling_config={"min_instances": 1, "max_instances": 3},
        )
E       TypeError: DeploymentConfig.__init__() got an unexpected keyword argument 'resource_limits'

tests\unit\test_infrastructure_complete.py:439: TypeError
______________ TestDeploymentManager.test_status_docker_compose _______________

self = <tests.unit.test_infrastructure_complete.TestDeploymentManager testMethod=test_status_docker_compose>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.config = DeploymentConfig(
            environment=DeploymentEnvironment.LOCAL,
            deployment_type=DeploymentType.DOCKER_COMPOSE,
            resource_limits={"cpu": "2", "memory": "4GB"},
            scaling_config={"min_instances": 1, "max_instances": 3},
        )
E       TypeError: DeploymentConfig.__init__() got an unexpected keyword argument 'resource_limits'

tests\unit\test_infrastructure_complete.py:439: TypeError
_ TestIntegrationManager.test_integration_manager_initialize_agent_runners_exception _

self = <tests.unit.test_integration.TestIntegrationManager object at 0x0000016F8B517A10>
integration_manager = <benchmarking.integration.manager.IntegrationManager object at 0x0000016F8F17B260>

    @pytest.mark.asyncio
    async def test_integration_manager_initialize_agent_runners_exception(
        self, integration_manager
    ):
        """Test agent runners integration initialization with exception."""
        with patch("benchmarking.integration.manager.AGENT_RUNNERS_AVAILABLE", True):
            with patch("benchmarking.integration.manager.RunnerFactory") as mock_factory:
                mock_factory.list_runners.side_effect = Exception("Test error")
    
                await integration_manager._initialize_agent_runners_integration()
    
                status = integration_manager.status["agent_runners"]
                assert status.available is True  # Module is available, but initialization failed
>               assert "Failed to initialize agent runners: Test error" in status.issues
E               AssertionError: assert 'Failed to initialize agent runners: Test error' in []
E                +  where [] = IntegrationStatus(component='agent_runners', available=True, version='integrated', capabilities=['runner_crewai', 'runner_diy', 'runner_langchain', 'runner_mock'], issues=[]).issues

tests\unit\test_integration.py:343: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:17.393 | INFO | benchmarking.integration.manager | [req=-] | Initialized IntegrationManager
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.manager:manager.py:184 Initialized IntegrationManager
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:17.395 | INFO | benchmarking.integration.manager | [req=-] | Agent runners integration successful. Available runners: ['crewai', 'diy', 'langchain', 'mock']
------------------------------ Captured log call ------------------------------
INFO     benchmarking.integration.manager:manager.py:230 Agent runners integration successful. Available runners: ['crewai', 'diy', 'langchain', 'mock']
_ TestIntegrationManager.test_integration_manager_initialize_memory_systems_success _

self = <tests.unit.test_integration.TestIntegrationManager object at 0x0000016F8B516D20>
integration_manager = <benchmarking.integration.manager.IntegrationManager object at 0x0000016F8F109DF0>

    @pytest.mark.asyncio
    async def test_integration_manager_initialize_memory_systems_success(self, integration_manager):
        """Test successful memory systems integration initialization."""
>       with patch("benchmarking.integration.manager.DualMemoryManager") as mock_manager:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_integration.py:407: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x0000016F8F109CD0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'benchmarking.integration.manager' from 'C:\\Users\\admin\\Downloads\\fba\\benchmarking\\integration\\manager.py'> does not have the attribute 'DualMemoryManager'

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1437: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:17.447 | INFO | benchmarking.integration.manager | [req=-] | Initialized IntegrationManager
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.manager:manager.py:184 Initialized IntegrationManager
_ TestIntegrationManager.test_integration_manager_initialize_memory_systems_unavailable _

self = <tests.unit.test_integration.TestIntegrationManager object at 0x0000016F8B5163F0>
integration_manager = <benchmarking.integration.manager.IntegrationManager object at 0x0000016F8F1789E0>

    @pytest.mark.asyncio
    async def test_integration_manager_initialize_memory_systems_unavailable(
        self, integration_manager
    ):
        """Test memory systems integration initialization when unavailable."""
>       with patch("benchmarking.integration.manager.DualMemoryManager", side_effect=ImportError):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_integration.py:423: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x0000016F8F178DD0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'benchmarking.integration.manager' from 'C:\\Users\\admin\\Downloads\\fba\\benchmarking\\integration\\manager.py'> does not have the attribute 'DualMemoryManager'

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1437: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:17.556 | INFO | benchmarking.integration.manager | [req=-] | Initialized IntegrationManager
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.manager:manager.py:184 Initialized IntegrationManager
_ TestIntegrationManager.test_integration_manager_initialize_custom_integrations_missing_function _

self = <tests.unit.test_integration.TestIntegrationManager object at 0x0000016F8B5383E0>
integration_manager = <benchmarking.integration.manager.IntegrationManager object at 0x0000016F8F10AF60>

    @pytest.mark.asyncio
    async def test_integration_manager_initialize_custom_integrations_missing_function(
        self, integration_manager
    ):
        """Test custom integrations initialization with missing function."""
        integration_manager.config.custom_integrations = {
            "custom1": {"type": "test", "module": "test_module"}
        }
    
        with patch("importlib.import_module") as mock_import:
            mock_module = Mock()
            del mock_module.initialize_integration  # Remove the function
            mock_import.return_value = mock_module
    
            await integration_manager._initialize_custom_integrations()
    
            status = integration_manager.status["custom_custom1"]
            assert status.available is False
>           assert "missing initialize_integration function" in status.issues
E           AssertionError: assert 'missing initialize_integration function' in ['Module test_module missing initialize_integration function']
E            +  where ['Module test_module missing initialize_integration function'] = IntegrationStatus(component='custom_custom1', available=False, version=None, capabilities=[], issues=['Module test_module missing initialize_integration function']).issues

tests\unit\test_integration.py:475: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:17.665 | INFO | benchmarking.integration.manager | [req=-] | Initialized IntegrationManager
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.manager:manager.py:184 Initialized IntegrationManager
_ TestIntegrationManager.test_integration_manager_create_agent_runner_success _

self = <tests.unit.test_integration.TestIntegrationManager object at 0x0000016F8B538CE0>
integration_manager = <benchmarking.integration.manager.IntegrationManager object at 0x0000016F8F10A9F0>

    @pytest.mark.asyncio
    async def test_integration_manager_create_agent_runner_success(self, integration_manager):
        """Test successful agent runner creation."""
        with patch("benchmarking.integration.manager.AGENT_RUNNERS_AVAILABLE", True):
            with patch("benchmarking.integration.manager.RunnerFactory") as mock_factory:
                mock_runner = Mock()
                mock_factory.create_and_initialize_runner.return_value = mock_runner
    
                runner = await integration_manager.create_agent_runner(
                    "diy", "test_agent", {"param": "value"}
                )
    
>               assert runner == mock_runner
E               AssertionError: assert None == <Mock name='RunnerFactory.create_and_initialize_runner()' id='1578653236688'>

tests\unit\test_integration.py:532: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:17.705 | INFO | benchmarking.integration.manager | [req=-] | Initialized IntegrationManager
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.manager:manager.py:184 Initialized IntegrationManager
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:17.707 | ERROR | benchmarking.integration.manager | [req=-] | Agent runners integration not available
------------------------------ Captured log call ------------------------------
ERROR    benchmarking.integration.manager:manager.py:445 Agent runners integration not available
_ TestIntegrationManager.test_integration_manager_run_legacy_metrics_success __

self = <tests.unit.test_integration.TestIntegrationManager object at 0x0000016F8B5395E0>
integration_manager = <benchmarking.integration.manager.IntegrationManager object at 0x0000016F8F10A210>

    @pytest.mark.asyncio
    async def test_integration_manager_run_legacy_metrics_success(self, integration_manager):
        """Test successful legacy metrics run."""
        integration_manager.legacy_metric_suite = Mock()
        integration_manager.legacy_metric_suite.calculate_kpis.return_value = {
            "overall_score": 0.8,
            "breakdown": {"finance": 0.9},
            "timestamp": "2023-01-01T00:00:00",
            "tick_number": 1,
        }
    
        events = [
            {"type": "SaleOccurred", "data": {"amount": 100}},
            {"type": "SetPriceCommand", "data": {"price": 50}},
        ]
    
        result = await integration_manager.run_legacy_metrics(1, events)
    
>       assert result is not None
E       assert None is not None

tests\unit\test_integration.py:578: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:17.747 | INFO | benchmarking.integration.manager | [req=-] | Initialized IntegrationManager
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.manager:manager.py:184 Initialized IntegrationManager
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:17.748 | ERROR | benchmarking.integration.manager | [req=-] | Legacy metrics integration not available
------------------------------ Captured log call ------------------------------
ERROR    benchmarking.integration.manager:manager.py:474 Legacy metrics integration not available
__ TestIntegrationManager.test_integration_manager_deploy_benchmark_success ___

self = <tests.unit.test_integration.TestIntegrationManager object at 0x0000016F8B539E50>
integration_manager = <benchmarking.integration.manager.IntegrationManager object at 0x0000016F8F108710>

    @pytest.mark.asyncio
    async def test_integration_manager_deploy_benchmark_success(self, integration_manager):
        """Test successful benchmark deployment."""
        integration_manager.deployment_manager = Mock()
        integration_manager.deployment_manager.deploy.return_value = "deployment_123"
    
        config = {
            "benchmark_id": "test_benchmark",
            "environment": {"max_workers": 4, "parallel_execution": True},
        }
    
        deployment_id = await integration_manager.deploy_benchmark(config)
    
>       assert deployment_id == "deployment_123"
E       AssertionError: assert None == 'deployment_123'

tests\unit\test_integration.py:618: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:17.787 | INFO | benchmarking.integration.manager | [req=-] | Initialized IntegrationManager
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.manager:manager.py:184 Initialized IntegrationManager
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:17.788 | ERROR | benchmarking.integration.manager | [req=-] | Infrastructure integration not available
------------------------------ Captured log call ------------------------------
ERROR    benchmarking.integration.manager:manager.py:506 Infrastructure integration not available
____ TestIntegrationManager.test_integration_manager_publish_event_success ____

self = <tests.unit.test_integration.TestIntegrationManager object at 0x0000016F8B539BE0>
integration_manager = <benchmarking.integration.manager.IntegrationManager object at 0x0000016F8F17B380>

    @pytest.mark.asyncio
    async def test_integration_manager_publish_event_success(self, integration_manager):
        """Test successful event publishing."""
        integration_manager._event_bus = Mock()
        integration_manager._event_bus.publish = AsyncMock()
    
        await integration_manager.publish_event("test_event", {"key": "value"})
    
>       integration_manager._event_bus.publish.assert_called_once_with(
            "test_event", {"key": "value"}
        )

tests\unit\test_integration.py:654: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <AsyncMock name='mock.publish' id='1578653694768'>
args = ('test_event', {'key': 'value'}), kwargs = {}
msg = "Expected 'publish' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'publish' to be called once. Called 0 times.

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:960: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:17.827 | INFO | benchmarking.integration.manager | [req=-] | Initialized IntegrationManager
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.manager:manager.py:184 Initialized IntegrationManager
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:17.829 | ERROR | benchmarking.integration.manager | [req=-] | Event bus integration not available
------------------------------ Captured log call ------------------------------
ERROR    benchmarking.integration.manager:manager.py:544 Event bus integration not available
_ TestIntegrationManager.test_integration_manager_subscribe_to_event_success __

self = <tests.unit.test_integration.TestIntegrationManager object at 0x0000016F8B538D40>
integration_manager = <benchmarking.integration.manager.IntegrationManager object at 0x0000016F8F1788C0>

    @pytest.mark.asyncio
    async def test_integration_manager_subscribe_to_event_success(self, integration_manager):
        """Test successful event subscription."""
        integration_manager._event_bus = Mock()
        integration_manager._event_bus.subscribe = AsyncMock()
    
        async def handler(event):
            pass
    
        await integration_manager.subscribe_to_event("test_event", handler)
    
>       integration_manager._event_bus.subscribe.assert_called_once_with("test_event", handler)

tests\unit\test_integration.py:677: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <AsyncMock name='mock.subscribe' id='1578653688096'>
args = ('test_event', <function TestIntegrationManager.test_integration_manager_subscribe_to_event_success.<locals>.handler at 0x0000016F8F8F6DE0>)
kwargs = {}, msg = "Expected 'subscribe' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'subscribe' to be called once. Called 0 times.

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:960: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:17.910 | INFO | benchmarking.integration.manager | [req=-] | Initialized IntegrationManager
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.manager:manager.py:184 Initialized IntegrationManager
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:17.911 | ERROR | benchmarking.integration.manager | [req=-] | Event bus integration not available
------------------------------ Captured log call ------------------------------
ERROR    benchmarking.integration.manager:manager.py:563 Event bus integration not available
________ TestAgentAdapter.test_agent_adapter_execute_decision_success _________

self = <tests.unit.test_integration.TestAgentAdapter object at 0x0000016F8B55C1D0>
agent_adapter = <benchmarking.integration.agent_adapter.AgentAdapter object at 0x0000016F8C129490>

    @pytest.mark.asyncio
    async def test_agent_adapter_execute_decision_success(self, agent_adapter):
        """Test successful decision execution."""
        # Setup
        mock_runner = MockAgentRunner()
        agent_adapter.agent_runner = mock_runner
        agent_adapter._initialized = True
    
        simulation_state = {
            "tick": 1,
            "simulation_time": "2023-01-01T00:00:00",
            "products": [],
            "recent_events": [],
            "financial_position": {},
            "market_conditions": {},
            "agent_state": {},
        }
    
        # Execute
        result = await agent_adapter.execute_decision(simulation_state)
    
        # Verify
        assert result.success is True
        assert result.agent_id == "test_agent"
        assert result.framework == "diy"
        assert len(result.tool_calls) == 1
        assert result.tool_calls[0].tool_name == "test_tool"
>       assert result.execution_time > 0
E       AssertionError: assert 0.0 > 0
E        +  where 0.0 = AgentExecutionResult(agent_id='test_agent', framework='diy', success=True, tool_calls=[<Mock id='1578603035488'>], exe...ent_id': 'test_agent', 'framework': 'diy', 'steps': [], 'end_time': '2025-09-15T18:31:18.042157', 'status': 'success'}).execution_time

tests\unit\test_integration.py:1056: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:18.040 | INFO | benchmarking.integration.agent_adapter | [req=-] | Initialized AgentAdapter for diy agent test_agent
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.agent_adapter:agent_adapter.py:125 Initialized AgentAdapter for diy agent test_agent
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:18.042 | INFO | benchmarking.integration.agent_adapter | [req=-] | Agent test_agent executed decision successfully
------------------------------ Captured log call ------------------------------
INFO     benchmarking.integration.agent_adapter:agent_adapter.py:207 Agent test_agent executed decision successfully
____ TestAgentAdapter.test_agent_adapter_execute_decision_not_initialized _____

self = <tests.unit.test_integration.TestAgentAdapter object at 0x0000016F8B55C4D0>
agent_adapter = <benchmarking.integration.agent_adapter.AgentAdapter object at 0x0000016F8C129EB0>

    @pytest.mark.asyncio
    async def test_agent_adapter_execute_decision_not_initialized(self, agent_adapter):
        """Test decision execution when not initialized."""
        agent_adapter._initialized = False
        agent_adapter.initialize = AsyncMock(return_value=False)
    
        simulation_state = {}
    
        result = await agent_adapter.execute_decision(simulation_state)
    
        assert result.success is False
        assert result.error_message == "Agent adapter not initialized"
>       assert len(agent_adapter._execution_history) == 1
E       assert 0 == 1
E        +  where 0 = len([])
E        +    where [] = <benchmarking.integration.agent_adapter.AgentAdapter object at 0x0000016F8C129EB0>._execution_history

tests\unit\test_integration.py:1078: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:18.078 | INFO | benchmarking.integration.agent_adapter | [req=-] | Initialized AgentAdapter for diy agent test_agent
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.agent_adapter:agent_adapter.py:125 Initialized AgentAdapter for diy agent test_agent
_______ TestAgentAdapter.test_agent_adapter_execute_decision_exception ________

self = <tests.unit.test_integration.TestAgentAdapter object at 0x0000016F8B55C7D0>
agent_adapter = <benchmarking.integration.agent_adapter.AgentAdapter object at 0x0000016F8FD09790>

    @pytest.mark.asyncio
    async def test_agent_adapter_execute_decision_exception(self, agent_adapter):
        """Test decision execution with exception."""
        # Setup
        mock_runner = MockAgentRunner()
        mock_runner.decide = AsyncMock(side_effect=Exception("Test error"))
        agent_adapter.agent_runner = mock_runner
        agent_adapter._initialized = True
    
        simulation_state = {}
    
        # Execute
        result = await agent_adapter.execute_decision(simulation_state)
    
        # Verify
        assert result.success is False
>       assert result.error_message == "Test error"
E       AssertionError: assert 'Agent decisi...s: Test error' == 'Test error'
E         
E         - Test error
E         + Agent decision failed after 2 attempts: Test error

tests\unit\test_integration.py:1096: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:18.114 | INFO | benchmarking.integration.agent_adapter | [req=-] | Initialized AgentAdapter for diy agent test_agent
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.agent_adapter:agent_adapter.py:125 Initialized AgentAdapter for diy agent test_agent
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:18.115 | WARNING | benchmarking.integration.agent_adapter | [req=-] | Agent test_agent decision failed (attempt 1): Test error
2025-09-16 01:31:19.123 | WARNING | benchmarking.integration.agent_adapter | [req=-] | Agent test_agent decision failed (attempt 2): Test error
2025-09-16 01:31:19.123 | ERROR | benchmarking.integration.agent_adapter | [req=-] | Agent test_agent execution failed: Agent decision failed after 2 attempts: Test error
------------------------------ Captured log call ------------------------------
WARNING  benchmarking.integration.agent_adapter:agent_adapter.py:265 Agent test_agent decision failed (attempt 1): Test error
WARNING  benchmarking.integration.agent_adapter:agent_adapter.py:265 Agent test_agent decision failed (attempt 2): Test error
ERROR    benchmarking.integration.agent_adapter:agent_adapter.py:224 Agent test_agent execution failed: Agent decision failed after 2 attempts: Test error
_____ TestAgentAdapter.test_agent_adapter_execute_decision_retry_success ______

self = <tests.unit.test_integration.TestAgentAdapter object at 0x0000016F8B55CDD0>
agent_adapter = <benchmarking.integration.agent_adapter.AgentAdapter object at 0x0000016F8F10B740>

    @pytest.mark.asyncio
    async def test_agent_adapter_execute_decision_retry_success(self, agent_adapter):
        """Test decision execution with retry success."""
        # Setup
        mock_runner = MockAgentRunner()
>       mock_runner.decide = AsyncMock(side_effect=[Exception("First error"), mock_runner.decide()])
                                                                              ^^^^^^^^^^^^^^^^^^^^
E       TypeError: MockAgentRunner.decide() missing 1 required positional argument: 'simulation_state'

tests\unit\test_integration.py:1125: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:20.171 | INFO | benchmarking.integration.agent_adapter | [req=-] | Initialized AgentAdapter for diy agent test_agent
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.agent_adapter:agent_adapter.py:125 Initialized AgentAdapter for diy agent test_agent
____________ TestAgentAdapter.test_agent_adapter_end_trace_enabled ____________

self = <tests.unit.test_integration.TestAgentAdapter object at 0x0000016F8B55CCE0>
agent_adapter = <benchmarking.integration.agent_adapter.AgentAdapter object at 0x0000016F8F14BF80>

    def test_agent_adapter_end_trace_enabled(self, agent_adapter):
        """Test ending trace when enabled."""
        agent_adapter.config.enable_tracing = True
        agent_adapter._current_trace = {"operation": "test"}
    
>       agent_adapter._end_trace("success")

tests\unit\test_integration.py:1263: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\integration\agent_adapter.py:396: in _end_trace
    asyncio.create_task(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

coro = <coroutine object AsyncMockMixin._execute_mock_call at 0x0000016F8C862840>

    def create_task(coro, *, name=None, context=None):
        """Schedule the execution of a coroutine object in a spawn task.
    
        Return a Task object.
        """
>       loop = events.get_running_loop()
               ^^^^^^^^^^^^^^^^^^^^^^^^^
E       RuntimeError: no running event loop

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\tasks.py:417: RuntimeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:20.226 | INFO | benchmarking.integration.agent_adapter | [req=-] | Initialized AgentAdapter for diy agent test_agent
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.agent_adapter:agent_adapter.py:125 Initialized AgentAdapter for diy agent test_agent
__________ TestAgentAdapter.test_agent_adapter_end_trace_with_error ___________

self = <tests.unit.test_integration.TestAgentAdapter object at 0x0000016F8B55C6B0>
agent_adapter = <benchmarking.integration.agent_adapter.AgentAdapter object at 0x0000016F8F19B020>

    def test_agent_adapter_end_trace_with_error(self, agent_adapter):
        """Test ending trace with error."""
        agent_adapter.config.enable_tracing = True
        agent_adapter._current_trace = {"operation": "test"}
    
>       agent_adapter._end_trace("error", "Test error")

tests\unit\test_integration.py:1278: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\integration\agent_adapter.py:396: in _end_trace
    asyncio.create_task(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

coro = <coroutine object AsyncMockMixin._execute_mock_call at 0x0000016F8C860540>

    def create_task(coro, *, name=None, context=None):
        """Schedule the execution of a coroutine object in a spawn task.
    
        Return a Task object.
        """
>       loop = events.get_running_loop()
               ^^^^^^^^^^^^^^^^^^^^^^^^^
E       RuntimeError: no running event loop

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\tasks.py:417: RuntimeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:20.279 | INFO | benchmarking.integration.agent_adapter | [req=-] | Initialized AgentAdapter for diy agent test_agent
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.agent_adapter:agent_adapter.py:125 Initialized AgentAdapter for diy agent test_agent
_____________ TestAgentAdapter.test_agent_adapter_cleanup_success _____________

self = <tests.unit.test_integration.TestAgentAdapter object at 0x0000016F8B55EF60>
agent_adapter = <benchmarking.integration.agent_adapter.AgentAdapter object at 0x0000016F8F17AF60>

    @pytest.mark.asyncio
    async def test_agent_adapter_cleanup_success(self, agent_adapter):
        """Test successful cleanup."""
        mock_runner = MockAgentRunner()
        agent_adapter.agent_runner = mock_runner
        agent_adapter._initialized = True
    
        await agent_adapter.cleanup()
    
        assert agent_adapter._initialized is False
>       mock_runner.cleanup.assert_called_once()
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'function' object has no attribute 'assert_called_once'

tests\unit\test_integration.py:1394: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:20.357 | INFO | benchmarking.integration.agent_adapter | [req=-] | Initialized AgentAdapter for diy agent test_agent
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.agent_adapter:agent_adapter.py:125 Initialized AgentAdapter for diy agent test_agent
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:20.358 | INFO | benchmarking.integration.agent_adapter | [req=-] | Cleaned up AgentAdapter for test_agent
------------------------------ Captured log call ------------------------------
INFO     benchmarking.integration.agent_adapter:agent_adapter.py:473 Cleaned up AgentAdapter for test_agent
____________ TestAgentAdapter.test_agent_adapter_cleanup_exception ____________

self = <tests.unit.test_integration.TestAgentAdapter object at 0x0000016F8B55F260>
agent_adapter = <benchmarking.integration.agent_adapter.AgentAdapter object at 0x0000016F8F1784A0>

    @pytest.mark.asyncio
    async def test_agent_adapter_cleanup_exception(self, agent_adapter):
        """Test cleanup with exception."""
        mock_runner = MockAgentRunner()
        mock_runner.cleanup = AsyncMock(side_effect=Exception("Test error"))
        agent_adapter.agent_runner = mock_runner
        agent_adapter._initialized = True
    
        # Should not raise exception
        await agent_adapter.cleanup()
    
>       assert agent_adapter._initialized is False
E       assert True is False
E        +  where True = <benchmarking.integration.agent_adapter.AgentAdapter object at 0x0000016F8F1784A0>._initialized

tests\unit\test_integration.py:1407: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:20.394 | INFO | benchmarking.integration.agent_adapter | [req=-] | Initialized AgentAdapter for diy agent test_agent
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.agent_adapter:agent_adapter.py:125 Initialized AgentAdapter for diy agent test_agent
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:20.396 | ERROR | benchmarking.integration.agent_adapter | [req=-] | Failed to cleanup AgentAdapter for test_agent: Test error
------------------------------ Captured log call ------------------------------
ERROR    benchmarking.integration.agent_adapter:agent_adapter.py:476 Failed to cleanup AgentAdapter for test_agent: Test error
___________ TestMetricsAdapter.test_metrics_adapter_initialization ____________

self = <tests.unit.test_integration.TestMetricsAdapter object at 0x0000016F8B55FF80>
metrics_adapter_config = MetricsAdapterConfig(enable_legacy_metrics=True, enable_new_metrics=True, merge_results=True, legacy_weights={'finance': 0.5}, custom_transformers={'metric1': 'normalize'})
integration_manager = <Mock spec='IntegrationManager' id='1578590652416'>

    def test_metrics_adapter_initialization(self, metrics_adapter_config, integration_manager):
        """Test metrics adapter initialization."""
        adapter = MetricsAdapter(metrics_adapter_config, integration_manager)
    
        assert adapter.config == metrics_adapter_config
        assert adapter.integration_manager == integration_manager
        assert adapter.legacy_metric_suite is None
        assert adapter._initialized is False
    
        # Check default legacy weights
>       assert adapter.legacy_weights["finance"] == 0.20
E       assert 0.5 == 0.2

tests\unit\test_integration.py:1637: AssertionError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:20.444 | INFO | benchmarking.integration.metrics_adapter | [req=-] | Initialized MetricsAdapter
------------------------------ Captured log call ------------------------------
INFO     benchmarking.integration.metrics_adapter:metrics_adapter.py:119 Initialized MetricsAdapter
______ TestMetricsAdapter.test_metrics_adapter_calculate_metrics_success ______

self = <tests.unit.test_integration.TestMetricsAdapter object at 0x0000016F8B57CD40>
metrics_adapter = <benchmarking.integration.metrics_adapter.MetricsAdapter object at 0x0000016F8F1A8770>

    @pytest.mark.asyncio
    async def test_metrics_adapter_calculate_metrics_success(self, metrics_adapter):
        """Test successful metrics calculation."""
        # Setup
        metrics_adapter._initialized = True
        metrics_adapter.legacy_metric_suite = MockMetricSuite()
    
        with patch.object(metrics_adapter, "_calculate_legacy_metrics") as mock_legacy:
            with patch.object(metrics_adapter, "_calculate_new_metrics") as mock_new:
                mock_legacy.return_value = {"score": 0.8, "warnings": []}
                mock_new.return_value = {"accuracy": 0.9, "warnings": []}
    
                with patch.object(metrics_adapter, "_merge_metrics") as mock_merge:
                    mock_merge.return_value = {"combined": 0.85}
    
                    result = await metrics_adapter.calculate_metrics(
                        tick_number=1,
                        events=[{"type": "test", "data": {}}],
                        context={"additional": "info"},
                    )
    
                    # Verify
                    assert result.success is True
                    assert result.legacy_metrics == {"score": 0.8, "warnings": []}
                    assert result.new_metrics == {"accuracy": 0.9, "warnings": []}
                    assert result.merged_metrics == {"combined": 0.85}
>                   assert result.execution_time > 0
E                   AssertionError: assert 0.0 > 0
E                    +  where 0.0 = MetricsAdapterResult(success=True, legacy_metrics={'score': 0.8, 'warnings': []}, new_metrics={'accuracy': 0.9, 'warnings': []}, merged_metrics={'combined': 0.85}, execution_time=0.0, error_message=None, warnings=[]).execution_time

tests\unit\test_integration.py:1716: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:20.492 | INFO | benchmarking.integration.metrics_adapter | [req=-] | Initialized MetricsAdapter
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.metrics_adapter:metrics_adapter.py:119 Initialized MetricsAdapter
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:20.493 | INFO | benchmarking.integration.metrics_adapter | [req=-] | Successfully calculated metrics for tick 1
------------------------------ Captured log call ------------------------------
INFO     benchmarking.integration.metrics_adapter:metrics_adapter.py:205 Successfully calculated metrics for tick 1
__ TestMetricsAdapter.test_metrics_adapter_calculate_metrics_legacy_disabled __

self = <tests.unit.test_integration.TestMetricsAdapter object at 0x0000016F8B57D340>
metrics_adapter = <benchmarking.integration.metrics_adapter.MetricsAdapter object at 0x0000016F8F123470>

    @pytest.mark.asyncio
    async def test_metrics_adapter_calculate_metrics_legacy_disabled(self, metrics_adapter):
        """Test metrics calculation with legacy disabled."""
        metrics_adapter._initialized = True
        metrics_adapter.config.enable_legacy_metrics = False
    
        with patch.object(metrics_adapter, "_calculate_new_metrics") as mock_new:
            mock_new.return_value = {"accuracy": 0.9, "warnings": []}
    
            result = await metrics_adapter.calculate_metrics(1, [])
    
            assert result.success is True
            assert result.legacy_metrics == {}
            assert result.new_metrics == {"accuracy": 0.9, "warnings": []}
>           assert result.merged_metrics == {}  # No legacy metrics to merge
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           AssertionError: assert {'legacy_metr...arnings': []}} == {}
E             
E             Left contains 3 more items:
E             {'legacy_metrics': {},
E              'merged_at': '2025-09-15T18:31:20.532701',
E              'new_metrics': {'accuracy': 0.9, 'warnings': []}}
E             Use -v to get more diff

tests\unit\test_integration.py:1755: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:20.530 | INFO | benchmarking.integration.metrics_adapter | [req=-] | Initialized MetricsAdapter
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.metrics_adapter:metrics_adapter.py:119 Initialized MetricsAdapter
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:20.532 | INFO | benchmarking.integration.metrics_adapter | [req=-] | Successfully calculated metrics for tick 1
------------------------------ Captured log call ------------------------------
INFO     benchmarking.integration.metrics_adapter:metrics_adapter.py:205 Successfully calculated metrics for tick 1
___ TestMetricsAdapter.test_metrics_adapter_calculate_metrics_new_disabled ____

self = <tests.unit.test_integration.TestMetricsAdapter object at 0x0000016F8B57D640>
metrics_adapter = <benchmarking.integration.metrics_adapter.MetricsAdapter object at 0x0000016F8F1AAEA0>

    @pytest.mark.asyncio
    async def test_metrics_adapter_calculate_metrics_new_disabled(self, metrics_adapter):
        """Test metrics calculation with new disabled."""
        metrics_adapter._initialized = True
        metrics_adapter.config.enable_new_metrics = False
        metrics_adapter.legacy_metric_suite = MockMetricSuite()
    
        with patch.object(metrics_adapter, "_calculate_legacy_metrics") as mock_legacy:
            mock_legacy.return_value = {"score": 0.8, "warnings": []}
    
            result = await metrics_adapter.calculate_metrics(1, [])
    
            assert result.success is True
            assert result.legacy_metrics == {"score": 0.8, "warnings": []}
            assert result.new_metrics == {}
>           assert result.merged_metrics == {}  # No new metrics to merge
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           AssertionError: assert {'legacy_metr..._metrics': {}} == {}
E             
E             Left contains 3 more items:
E             {'legacy_metrics': {'score': 0.8, 'warnings': []},
E              'merged_at': '2025-09-15T18:31:20.567670',
E              'new_metrics': {}}
E             Use -v to get more diff

tests\unit\test_integration.py:1772: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:20.566 | INFO | benchmarking.integration.metrics_adapter | [req=-] | Initialized MetricsAdapter
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.metrics_adapter:metrics_adapter.py:119 Initialized MetricsAdapter
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:20.567 | INFO | benchmarking.integration.metrics_adapter | [req=-] | Successfully calculated metrics for tick 1
------------------------------ Captured log call ------------------------------
INFO     benchmarking.integration.metrics_adapter:metrics_adapter.py:205 Successfully calculated metrics for tick 1
_____ TestMetricsAdapter.test_metrics_adapter_calculate_metrics_exception _____

self = <tests.unit.test_integration.TestMetricsAdapter object at 0x0000016F8B55F140>
metrics_adapter = <benchmarking.integration.metrics_adapter.MetricsAdapter object at 0x0000016F8F1A8A10>

    @pytest.mark.asyncio
    async def test_metrics_adapter_calculate_metrics_exception(self, metrics_adapter):
        """Test metrics calculation with exception."""
        metrics_adapter._initialized = True
    
        with patch.object(metrics_adapter, "_calculate_legacy_metrics") as mock_legacy:
            mock_legacy.side_effect = Exception("Test error")
    
            result = await metrics_adapter.calculate_metrics(1, [])
    
>           assert result.success is False
E           assert True is False
E            +  where True = MetricsAdapterResult(success=True, legacy_metrics={}, new_metrics={}, merged_metrics={}, execution_time=0.0, error_message=None, warnings=[]).success

tests\unit\test_integration.py:1803: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:20.607 | INFO | benchmarking.integration.metrics_adapter | [req=-] | Initialized MetricsAdapter
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.metrics_adapter:metrics_adapter.py:119 Initialized MetricsAdapter
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:20.608 | INFO | benchmarking.integration.metrics_adapter | [req=-] | Successfully calculated metrics for tick 1
------------------------------ Captured log call ------------------------------
INFO     benchmarking.integration.metrics_adapter:metrics_adapter.py:205 Successfully calculated metrics for tick 1
__ TestMetricsAdapter.test_metrics_adapter_calculate_legacy_metrics_success ___

self = <tests.unit.test_integration.TestMetricsAdapter object at 0x0000016F8B57D550>
metrics_adapter = <benchmarking.integration.metrics_adapter.MetricsAdapter object at 0x0000016F8C12A8D0>

    @pytest.mark.asyncio
    async def test_metrics_adapter_calculate_legacy_metrics_success(self, metrics_adapter):
        """Test successful legacy metrics calculation."""
        metrics_adapter.legacy_metric_suite = MockMetricSuite()
    
        events = [
            {"type": "SaleOccurred", "amount": 100},
            {"type": "SetPriceCommand", "price": 50},
            {"type": "UnknownEvent", "data": {}},
        ]
    
        result = await metrics_adapter._calculate_legacy_metrics(1, events, {"context": "info"})
    
        assert result["overall_score"] == 0.8
        assert result["breakdown"] == {"finance": 0.9}
        assert result["timestamp"] == "2023-01-01T00:00:00"
        assert result["tick_number"] == 1
    
        # Verify events were processed
>       assert len(metrics_adapter.legacy_metric_suite.events) == 3
E       AssertionError: assert 2 == 3
E        +  where 2 = len([('SaleOccurred', <benchmarking.integration.metrics_adapter.LegacyEvent object at 0x0000016F8F123230>), ('SetPriceCommand', <benchmarking.integration.metrics_adapter.LegacyEvent object at 0x0000016F8B55DDC0>)])
E        +    where [('SaleOccurred', <benchmarking.integration.metrics_adapter.LegacyEvent object at 0x0000016F8F123230>), ('SetPriceCommand', <benchmarking.integration.metrics_adapter.LegacyEvent object at 0x0000016F8B55DDC0>)] = <tests.unit.test_integration.MockMetricSuite object at 0x0000016F8F1236B0>.events
E        +      where <tests.unit.test_integration.MockMetricSuite object at 0x0000016F8F1236B0> = <benchmarking.integration.metrics_adapter.MetricsAdapter object at 0x0000016F8C12A8D0>.legacy_metric_suite

tests\unit\test_integration.py:1826: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:20.641 | INFO | benchmarking.integration.metrics_adapter | [req=-] | Initialized MetricsAdapter
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.metrics_adapter:metrics_adapter.py:119 Initialized MetricsAdapter
_ TestMetricsAdapter.test_metrics_adapter_calculate_legacy_metrics_exception __

self = <tests.unit.test_integration.TestMetricsAdapter object at 0x0000016F8B57C560>
metrics_adapter = <benchmarking.integration.metrics_adapter.MetricsAdapter object at 0x0000016F8F1AA1B0>

    @pytest.mark.asyncio
    async def test_metrics_adapter_calculate_legacy_metrics_exception(self, metrics_adapter):
        """Test legacy metrics calculation with exception."""
        metrics_adapter.legacy_metric_suite = MockMetricSuite()
>       metrics_adapter.legacy_metric_suite.calculate_kpis.side_effect = Exception("Test error")
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'method' object has no attribute 'side_effect'

tests\unit\test_integration.py:1844: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:20.682 | INFO | benchmarking.integration.metrics_adapter | [req=-] | Initialized MetricsAdapter
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.metrics_adapter:metrics_adapter.py:119 Initialized MetricsAdapter
____ TestMetricsAdapter.test_metrics_adapter_calculate_new_metrics_success ____

self = <tests.unit.test_integration.TestMetricsAdapter object at 0x0000016F8B57DAC0>
metrics_adapter = <benchmarking.integration.metrics_adapter.MetricsAdapter object at 0x0000016F8FD0BEC0>

    @pytest.mark.asyncio
    async def test_metrics_adapter_calculate_new_metrics_success(self, metrics_adapter):
        """Test successful new metrics calculation."""
        with patch.object(metrics_registry, "get_all_metrics") as mock_get_all:
            mock_metric = MockBaseMetric(score=0.8)
            mock_get_all.return_value = {"metric1": mock_metric}
    
            result = await metrics_adapter._calculate_new_metrics(
                tick_number=1, events=[{"type": "test"}], context={"additional": "info"}
            )
    
            assert "metric1" in result
>           assert result["metric1"]["name"] == "mock_metric"
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
E           KeyError: 'name'

tests\unit\test_integration.py:1863: KeyError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:20.716 | INFO | benchmarking.integration.metrics_adapter | [req=-] | Initialized MetricsAdapter
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.metrics_adapter:metrics_adapter.py:119 Initialized MetricsAdapter
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:20.717 | WARNING | benchmarking.integration.metrics_adapter | [req=-] | Failed to calculate metric metric1: MetricResult.__init__() missing 1 required positional argument: 'confidence'
------------------------------ Captured log call ------------------------------
WARNING  benchmarking.integration.metrics_adapter:metrics_adapter.py:328 Failed to calculate metric metric1: MetricResult.__init__() missing 1 required positional argument: 'confidence'
_ TestMetricsAdapter.test_metrics_adapter_calculate_new_metrics_with_transformer _

self = <tests.unit.test_integration.TestMetricsAdapter object at 0x0000016F8B57DDC0>
metrics_adapter = <benchmarking.integration.metrics_adapter.MetricsAdapter object at 0x0000016F8FD0BE60>

    @pytest.mark.asyncio
    async def test_metrics_adapter_calculate_new_metrics_with_transformer(self, metrics_adapter):
        """Test new metrics calculation with transformer."""
        metrics_adapter.config.custom_transformers = {"metric1": "normalize"}
    
        with patch.object(metrics_registry, "get_all_metrics") as mock_get_all:
            mock_metric = MockBaseMetric(score=80.0)  # Score outside 0-1 range
            mock_get_all.return_value = {"metric1": mock_metric}
    
            result = await metrics_adapter._calculate_new_metrics(1, [])
    
            assert "metric1" in result
>           assert result["metric1"]["score"] == 80.0
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
E           KeyError: 'score'

tests\unit\test_integration.py:1879: KeyError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:20.771 | INFO | benchmarking.integration.metrics_adapter | [req=-] | Initialized MetricsAdapter
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.metrics_adapter:metrics_adapter.py:119 Initialized MetricsAdapter
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:20.773 | WARNING | benchmarking.integration.metrics_adapter | [req=-] | Failed to calculate metric metric1: MetricResult.__init__() missing 1 required positional argument: 'confidence'
------------------------------ Captured log call ------------------------------
WARNING  benchmarking.integration.metrics_adapter:metrics_adapter.py:328 Failed to calculate metric metric1: MetricResult.__init__() missing 1 required positional argument: 'confidence'
_ TestMetricsAdapter.test_metrics_adapter_calculate_new_metrics_transformer_exception _

self = <tests.unit.test_integration.TestMetricsAdapter object at 0x0000016F8B57E3C0>
metrics_adapter = <benchmarking.integration.metrics_adapter.MetricsAdapter object at 0x0000016F8F663C50>

    @pytest.mark.asyncio
    async def test_metrics_adapter_calculate_new_metrics_transformer_exception(
        self, metrics_adapter
    ):
        """Test new metrics calculation with transformer exception."""
        metrics_adapter.config.custom_transformers = {"metric1": "unknown_transformer"}
    
        with patch.object(metrics_registry, "get_all_metrics") as mock_get_all:
            mock_metric = MockBaseMetric()
            mock_get_all.return_value = {"metric1": mock_metric}
    
            result = await metrics_adapter._calculate_new_metrics(1, [])
    
            assert "metric1" in result
>           assert result["metric1"]["score"] == 0.8
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
E           KeyError: 'score'

tests\unit\test_integration.py:1909: KeyError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:20.813 | INFO | benchmarking.integration.metrics_adapter | [req=-] | Initialized MetricsAdapter
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.metrics_adapter:metrics_adapter.py:119 Initialized MetricsAdapter
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:20.814 | WARNING | benchmarking.integration.metrics_adapter | [req=-] | Failed to calculate metric metric1: MetricResult.__init__() missing 1 required positional argument: 'confidence'
------------------------------ Captured log call ------------------------------
WARNING  benchmarking.integration.metrics_adapter:metrics_adapter.py:328 Failed to calculate metric metric1: MetricResult.__init__() missing 1 required positional argument: 'confidence'
___ TestMetricsAdapter.test_metrics_adapter_merge_metrics_both_with_scores ____

self = <tests.unit.test_integration.TestMetricsAdapter object at 0x0000016F8B57F080>
metrics_adapter = <benchmarking.integration.metrics_adapter.MetricsAdapter object at 0x0000016F8F179970>

    def test_metrics_adapter_merge_metrics_both_with_scores(self, metrics_adapter):
        """Test merging metrics with both having scores."""
        legacy = {"overall_score": 0.8}
        new = {"metric1": {"score": 0.9}, "metric2": {"score": 0.7}}
    
        result = metrics_adapter._merge_metrics(legacy, new)
    
        assert result["legacy_metrics"] == legacy
        assert result["new_metrics"] == new
        assert "merged_at" in result
>       assert result["overall_score"] == 0.75  # (0.8 + 0.8) / 2
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       assert 0.8 == 0.75

tests\unit\test_integration.py:1966: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:20.857 | INFO | benchmarking.integration.metrics_adapter | [req=-] | Initialized MetricsAdapter
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.integration.metrics_adapter:metrics_adapter.py:119 Initialized MetricsAdapter
________________ TestDualMemoryManager.test_consolidate_memory ________________

self = <tests.unit.test_memory_experiments.TestDualMemoryManager testMethod=test_consolidate_memory>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.dual_memory_manager = DualMemoryManager()
                                   ^^^^^^^^^^^^^^^^^^^
E       TypeError: DualMemoryManager.__init__() missing 2 required positional arguments: 'config' and 'agent_id'

tests\unit\test_memory_experiments.py:20: TypeError
________ TestDualMemoryManager.test_dual_memory_manager_initialization ________

self = <tests.unit.test_memory_experiments.TestDualMemoryManager testMethod=test_dual_memory_manager_initialization>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.dual_memory_manager = DualMemoryManager()
                                   ^^^^^^^^^^^^^^^^^^^
E       TypeError: DualMemoryManager.__init__() missing 2 required positional arguments: 'config' and 'agent_id'

tests\unit\test_memory_experiments.py:20: TypeError
______________ TestDualMemoryManager.test_forget_working_memory _______________

self = <tests.unit.test_memory_experiments.TestDualMemoryManager testMethod=test_forget_working_memory>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.dual_memory_manager = DualMemoryManager()
                                   ^^^^^^^^^^^^^^^^^^^
E       TypeError: DualMemoryManager.__init__() missing 2 required positional arguments: 'config' and 'agent_id'

tests\unit\test_memory_experiments.py:20: TypeError
______________ TestDualMemoryManager.test_get_memory_statistics _______________

self = <tests.unit.test_memory_experiments.TestDualMemoryManager testMethod=test_get_memory_statistics>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.dual_memory_manager = DualMemoryManager()
                                   ^^^^^^^^^^^^^^^^^^^
E       TypeError: DualMemoryManager.__init__() missing 2 required positional arguments: 'config' and 'agent_id'

tests\unit\test_memory_experiments.py:20: TypeError
____________ TestDualMemoryManager.test_retrieve_long_term_memory _____________

self = <tests.unit.test_memory_experiments.TestDualMemoryManager testMethod=test_retrieve_long_term_memory>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.dual_memory_manager = DualMemoryManager()
                                   ^^^^^^^^^^^^^^^^^^^
E       TypeError: DualMemoryManager.__init__() missing 2 required positional arguments: 'config' and 'agent_id'

tests\unit\test_memory_experiments.py:20: TypeError
_____________ TestDualMemoryManager.test_retrieve_working_memory ______________

self = <tests.unit.test_memory_experiments.TestDualMemoryManager testMethod=test_retrieve_working_memory>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.dual_memory_manager = DualMemoryManager()
                                   ^^^^^^^^^^^^^^^^^^^
E       TypeError: DualMemoryManager.__init__() missing 2 required positional arguments: 'config' and 'agent_id'

tests\unit\test_memory_experiments.py:20: TypeError
_____________ TestDualMemoryManager.test_search_long_term_memory ______________

self = <tests.unit.test_memory_experiments.TestDualMemoryManager testMethod=test_search_long_term_memory>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.dual_memory_manager = DualMemoryManager()
                                   ^^^^^^^^^^^^^^^^^^^
E       TypeError: DualMemoryManager.__init__() missing 2 required positional arguments: 'config' and 'agent_id'

tests\unit\test_memory_experiments.py:20: TypeError
______________ TestDualMemoryManager.test_search_working_memory _______________

self = <tests.unit.test_memory_experiments.TestDualMemoryManager testMethod=test_search_working_memory>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.dual_memory_manager = DualMemoryManager()
                                   ^^^^^^^^^^^^^^^^^^^
E       TypeError: DualMemoryManager.__init__() missing 2 required positional arguments: 'config' and 'agent_id'

tests\unit\test_memory_experiments.py:20: TypeError
______________ TestDualMemoryManager.test_store_long_term_memory ______________

self = <tests.unit.test_memory_experiments.TestDualMemoryManager testMethod=test_store_long_term_memory>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.dual_memory_manager = DualMemoryManager()
                                   ^^^^^^^^^^^^^^^^^^^
E       TypeError: DualMemoryManager.__init__() missing 2 required positional arguments: 'config' and 'agent_id'

tests\unit\test_memory_experiments.py:20: TypeError
_______________ TestDualMemoryManager.test_store_working_memory _______________

self = <tests.unit.test_memory_experiments.TestDualMemoryManager testMethod=test_store_working_memory>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.dual_memory_manager = DualMemoryManager()
                                   ^^^^^^^^^^^^^^^^^^^
E       TypeError: DualMemoryManager.__init__() missing 2 required positional arguments: 'config' and 'agent_id'

tests\unit\test_memory_experiments.py:20: TypeError
________________ TestExperimentProtocols.test_define_protocol _________________

self = <tests.unit.test_memory_experiments.TestExperimentProtocols testMethod=test_define_protocol>

    def test_define_protocol(self):
        """Test defining an experiment protocol."""
        protocol_data = {
            "protocol_id": "protocol1",
            "name": "Memory Retention Test",
            "description": "Test the retention of memories over time",
            "memory_types": ["working_memory", "long_term_memory"],
            "experiment_duration": timedelta(hours=24),
            "test_intervals": [timedelta(hours=1), timedelta(hours=6), timedelta(hours=24)],
            "metrics": ["retention_rate", "recall_accuracy", "retrieval_time"],
            "parameters": {
                "memory_count": 100,
                "importance_distribution": "uniform",
                "tag_distribution": "uniform",
            },
        }
    
>       protocol_id = self.experiment_protocols.define_protocol(protocol_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ExperimentProtocols' object has no attribute 'define_protocol'

tests\unit\test_memory_experiments.py:269: AttributeError
______ TestExperimentProtocols.test_experiment_protocols_initialization _______

self = <tests.unit.test_memory_experiments.TestExperimentProtocols testMethod=test_experiment_protocols_initialization>

    def test_experiment_protocols_initialization(self):
        """Test that the experiment protocols initialize correctly."""
        self.assertIsNotNone(self.experiment_protocols)
>       self.assertEqual(len(self.experiment_protocols._protocols), 0)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ExperimentProtocols' object has no attribute '_protocols'

tests\unit\test_memory_experiments.py:249: AttributeError
_________________ TestExperimentProtocols.test_get_experiment _________________

self = <tests.unit.test_memory_experiments.TestExperimentProtocols testMethod=test_get_experiment>

    def test_get_experiment(self):
        """Test getting an experiment."""
        # Define a protocol
        protocol_data = {
            "protocol_id": "protocol1",
            "name": "Memory Retention Test",
            "description": "Test the retention of memories over time",
            "memory_types": ["working_memory", "long_term_memory"],
            "experiment_duration": timedelta(hours=24),
            "test_intervals": [timedelta(hours=1), timedelta(hours=6), timedelta(hours=24)],
            "metrics": ["retention_rate", "recall_accuracy", "retrieval_time"],
            "parameters": {
                "memory_count": 100,
                "importance_distribution": "uniform",
                "tag_distribution": "uniform",
            },
        }
    
>       self.experiment_protocols.define_protocol(protocol_data)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ExperimentProtocols' object has no attribute 'define_protocol'

tests\unit\test_memory_experiments.py:377: AttributeError
_____________ TestExperimentProtocols.test_get_experiment_results _____________

self = <tests.unit.test_memory_experiments.TestExperimentProtocols testMethod=test_get_experiment_results>

    def test_get_experiment_results(self):
        """Test getting experiment results."""
        # Define a protocol
        protocol_data = {
            "protocol_id": "protocol1",
            "name": "Memory Retention Test",
            "description": "Test the retention of memories over time",
            "memory_types": ["working_memory", "long_term_memory"],
            "experiment_duration": timedelta(hours=24),
            "test_intervals": [timedelta(hours=1), timedelta(hours=6), timedelta(hours=24)],
            "metrics": ["retention_rate", "recall_accuracy", "retrieval_time"],
            "parameters": {
                "memory_count": 100,
                "importance_distribution": "uniform",
                "tag_distribution": "uniform",
            },
        }
    
>       self.experiment_protocols.define_protocol(protocol_data)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ExperimentProtocols' object has no attribute 'define_protocol'

tests\unit\test_memory_experiments.py:431: AttributeError
__________________ TestExperimentProtocols.test_get_protocol __________________

self = <tests.unit.test_memory_experiments.TestExperimentProtocols testMethod=test_get_protocol>

    def test_get_protocol(self):
        """Test getting an experiment protocol."""
        protocol_data = {
            "protocol_id": "protocol1",
            "name": "Memory Retention Test",
            "description": "Test the retention of memories over time",
            "memory_types": ["working_memory", "long_term_memory"],
            "experiment_duration": timedelta(hours=24),
            "test_intervals": [timedelta(hours=1), timedelta(hours=6), timedelta(hours=24)],
            "metrics": ["retention_rate", "recall_accuracy", "retrieval_time"],
            "parameters": {
                "memory_count": 100,
                "importance_distribution": "uniform",
                "tag_distribution": "uniform",
            },
        }
    
>       self.experiment_protocols.define_protocol(protocol_data)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ExperimentProtocols' object has no attribute 'define_protocol'

tests\unit\test_memory_experiments.py:297: AttributeError
________________ TestExperimentProtocols.test_list_experiments ________________

self = <tests.unit.test_memory_experiments.TestExperimentProtocols testMethod=test_list_experiments>

    def test_list_experiments(self):
        """Test listing all experiments."""
        # Define a protocol
        protocol_data = {
            "protocol_id": "protocol1",
            "name": "Memory Retention Test",
            "description": "Test the retention of memories over time",
            "memory_types": ["working_memory", "long_term_memory"],
            "experiment_duration": timedelta(hours=24),
            "test_intervals": [timedelta(hours=1), timedelta(hours=6), timedelta(hours=24)],
            "metrics": ["retention_rate", "recall_accuracy", "retrieval_time"],
            "parameters": {
                "memory_count": 100,
                "importance_distribution": "uniform",
                "tag_distribution": "uniform",
            },
        }
    
>       self.experiment_protocols.define_protocol(protocol_data)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ExperimentProtocols' object has no attribute 'define_protocol'

tests\unit\test_memory_experiments.py:535: AttributeError
_________________ TestExperimentProtocols.test_list_protocols _________________

self = <tests.unit.test_memory_experiments.TestExperimentProtocols testMethod=test_list_protocols>

    def test_list_protocols(self):
        """Test listing all protocols."""
        # Define multiple protocols
        protocol1_data = {
            "protocol_id": "protocol1",
            "name": "Memory Retention Test",
            "description": "Test the retention of memories over time",
            "memory_types": ["working_memory", "long_term_memory"],
            "experiment_duration": timedelta(hours=24),
            "test_intervals": [timedelta(hours=1), timedelta(hours=6), timedelta(hours=24)],
            "metrics": ["retention_rate", "recall_accuracy", "retrieval_time"],
            "parameters": {
                "memory_count": 100,
                "importance_distribution": "uniform",
                "tag_distribution": "uniform",
            },
        }
    
        protocol2_data = {
            "protocol_id": "protocol2",
            "name": "Memory Capacity Test",
            "description": "Test the capacity of memory systems",
            "memory_types": ["working_memory", "long_term_memory"],
            "experiment_duration": timedelta(hours=12),
            "test_intervals": [timedelta(hours=1), timedelta(hours=6), timedelta(hours=12)],
            "metrics": ["capacity", "utilization", "efficiency"],
            "parameters": {
                "memory_count": 200,
                "importance_distribution": "normal",
                "tag_distribution": "normal",
            },
        }
    
>       self.experiment_protocols.define_protocol(protocol1_data)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ExperimentProtocols' object has no attribute 'define_protocol'

tests\unit\test_memory_experiments.py:507: AttributeError
_________________ TestExperimentProtocols.test_run_experiment _________________

self = <tests.unit.test_memory_experiments.TestExperimentProtocols testMethod=test_run_experiment>

    def test_run_experiment(self):
        """Test running an experiment."""
        # Define a protocol
        protocol_data = {
            "protocol_id": "protocol1",
            "name": "Memory Retention Test",
            "description": "Test the retention of memories over time",
            "memory_types": ["working_memory", "long_term_memory"],
            "experiment_duration": timedelta(hours=24),
            "test_intervals": [timedelta(hours=1), timedelta(hours=6), timedelta(hours=24)],
            "metrics": ["retention_rate", "recall_accuracy", "retrieval_time"],
            "parameters": {
                "memory_count": 100,
                "importance_distribution": "uniform",
                "tag_distribution": "uniform",
            },
        }
    
>       self.experiment_protocols.define_protocol(protocol_data)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ExperimentProtocols' object has no attribute 'define_protocol'

tests\unit\test_memory_experiments.py:324: AttributeError
__________ TestMemoryMetrics.test_calculate_memory_age_distribution ___________

self = <tests.unit.test_memory_experiments.TestMemoryMetrics testMethod=test_calculate_memory_age_distribution>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.memory_metrics = MemoryMetrics()
                              ^^^^^^^^^^^^^^^
E       TypeError: MemoryMetrics.__init__() missing 1 required positional argument: 'memory_enforcer'

tests\unit\test_memory_experiments.py:577: TypeError
______________ TestMemoryMetrics.test_calculate_memory_capacity _______________

self = <tests.unit.test_memory_experiments.TestMemoryMetrics testMethod=test_calculate_memory_capacity>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.memory_metrics = MemoryMetrics()
                              ^^^^^^^^^^^^^^^
E       TypeError: MemoryMetrics.__init__() missing 1 required positional argument: 'memory_enforcer'

tests\unit\test_memory_experiments.py:577: TypeError
_____________ TestMemoryMetrics.test_calculate_memory_efficiency ______________

self = <tests.unit.test_memory_experiments.TestMemoryMetrics testMethod=test_calculate_memory_efficiency>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.memory_metrics = MemoryMetrics()
                              ^^^^^^^^^^^^^^^
E       TypeError: MemoryMetrics.__init__() missing 1 required positional argument: 'memory_enforcer'

tests\unit\test_memory_experiments.py:577: TypeError
_______ TestMemoryMetrics.test_calculate_memory_importance_distribution _______

self = <tests.unit.test_memory_experiments.TestMemoryMetrics testMethod=test_calculate_memory_importance_distribution>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.memory_metrics = MemoryMetrics()
                              ^^^^^^^^^^^^^^^
E       TypeError: MemoryMetrics.__init__() missing 1 required positional argument: 'memory_enforcer'

tests\unit\test_memory_experiments.py:577: TypeError
__________ TestMemoryMetrics.test_calculate_memory_tag_distribution ___________

self = <tests.unit.test_memory_experiments.TestMemoryMetrics testMethod=test_calculate_memory_tag_distribution>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.memory_metrics = MemoryMetrics()
                              ^^^^^^^^^^^^^^^
E       TypeError: MemoryMetrics.__init__() missing 1 required positional argument: 'memory_enforcer'

tests\unit\test_memory_experiments.py:577: TypeError
______________ TestMemoryMetrics.test_calculate_recall_accuracy _______________

self = <tests.unit.test_memory_experiments.TestMemoryMetrics testMethod=test_calculate_recall_accuracy>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.memory_metrics = MemoryMetrics()
                              ^^^^^^^^^^^^^^^
E       TypeError: MemoryMetrics.__init__() missing 1 required positional argument: 'memory_enforcer'

tests\unit\test_memory_experiments.py:577: TypeError
_______________ TestMemoryMetrics.test_calculate_retention_rate _______________

self = <tests.unit.test_memory_experiments.TestMemoryMetrics testMethod=test_calculate_retention_rate>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.memory_metrics = MemoryMetrics()
                              ^^^^^^^^^^^^^^^
E       TypeError: MemoryMetrics.__init__() missing 1 required positional argument: 'memory_enforcer'

tests\unit\test_memory_experiments.py:577: TypeError
_______________ TestMemoryMetrics.test_calculate_retrieval_time _______________

self = <tests.unit.test_memory_experiments.TestMemoryMetrics testMethod=test_calculate_retrieval_time>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.memory_metrics = MemoryMetrics()
                              ^^^^^^^^^^^^^^^
E       TypeError: MemoryMetrics.__init__() missing 1 required positional argument: 'memory_enforcer'

tests\unit\test_memory_experiments.py:577: TypeError
________________ TestMemoryMetrics.test_generate_memory_report ________________

self = <tests.unit.test_memory_experiments.TestMemoryMetrics testMethod=test_generate_memory_report>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.memory_metrics = MemoryMetrics()
                              ^^^^^^^^^^^^^^^
E       TypeError: MemoryMetrics.__init__() missing 1 required positional argument: 'memory_enforcer'

tests\unit\test_memory_experiments.py:577: TypeError
____________ TestMemoryMetrics.test_memory_metrics_initialization _____________

self = <tests.unit.test_memory_experiments.TestMemoryMetrics testMethod=test_memory_metrics_initialization>

    def setUp(self):
        """Set up test fixtures before each test method."""
>       self.memory_metrics = MemoryMetrics()
                              ^^^^^^^^^^^^^^^
E       TypeError: MemoryMetrics.__init__() missing 1 required positional argument: 'memory_enforcer'

tests\unit\test_memory_experiments.py:577: TypeError
___________________ TestProduct.test_product_add_competitor ___________________

self = <tests.unit.test_models.TestProduct testMethod=test_product_add_competitor>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.product_data = {
            "id": "product1",
            "name": "Product A",
            "category": "electronics",
            "price": 99.99,
            "cost": 50.0,
            "description": "A high-quality electronic product",
            "features": ["feature1", "feature2", "feature3"],
            "specifications": {"weight": "1kg", "dimensions": "10x20x30cm"},
            "inventory": 100,
            "sales_history": [
                {"date": "2023-01-01", "quantity": 10, "revenue": 999.9},
                {"date": "2023-01-02", "quantity": 15, "revenue": 1499.85},
            ],
            "customer_reviews": [
                {"rating": 5, "comment": "Great product!"},
                {"rating": 4, "comment": "Good value for money."},
            ],
            "competitors": ["competitor1", "competitor2"],
        }
>       self.product = Product(self.product_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:164: TypeError
________________ TestProduct.test_product_add_customer_review _________________

self = <tests.unit.test_models.TestProduct testMethod=test_product_add_customer_review>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.product_data = {
            "id": "product1",
            "name": "Product A",
            "category": "electronics",
            "price": 99.99,
            "cost": 50.0,
            "description": "A high-quality electronic product",
            "features": ["feature1", "feature2", "feature3"],
            "specifications": {"weight": "1kg", "dimensions": "10x20x30cm"},
            "inventory": 100,
            "sales_history": [
                {"date": "2023-01-01", "quantity": 10, "revenue": 999.9},
                {"date": "2023-01-02", "quantity": 15, "revenue": 1499.85},
            ],
            "customer_reviews": [
                {"rating": 5, "comment": "Great product!"},
                {"rating": 4, "comment": "Good value for money."},
            ],
            "competitors": ["competitor1", "competitor2"],
        }
>       self.product = Product(self.product_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:164: TypeError
____________________ TestProduct.test_product_add_feature _____________________

self = <tests.unit.test_models.TestProduct testMethod=test_product_add_feature>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.product_data = {
            "id": "product1",
            "name": "Product A",
            "category": "electronics",
            "price": 99.99,
            "cost": 50.0,
            "description": "A high-quality electronic product",
            "features": ["feature1", "feature2", "feature3"],
            "specifications": {"weight": "1kg", "dimensions": "10x20x30cm"},
            "inventory": 100,
            "sales_history": [
                {"date": "2023-01-01", "quantity": 10, "revenue": 999.9},
                {"date": "2023-01-02", "quantity": 15, "revenue": 1499.85},
            ],
            "customer_reviews": [
                {"rating": 5, "comment": "Great product!"},
                {"rating": 4, "comment": "Good value for money."},
            ],
            "competitors": ["competitor1", "competitor2"],
        }
>       self.product = Product(self.product_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:164: TypeError
______________________ TestProduct.test_product_add_sale ______________________

self = <tests.unit.test_models.TestProduct testMethod=test_product_add_sale>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.product_data = {
            "id": "product1",
            "name": "Product A",
            "category": "electronics",
            "price": 99.99,
            "cost": 50.0,
            "description": "A high-quality electronic product",
            "features": ["feature1", "feature2", "feature3"],
            "specifications": {"weight": "1kg", "dimensions": "10x20x30cm"},
            "inventory": 100,
            "sales_history": [
                {"date": "2023-01-01", "quantity": 10, "revenue": 999.9},
                {"date": "2023-01-02", "quantity": 15, "revenue": 1499.85},
            ],
            "customer_reviews": [
                {"rating": 5, "comment": "Great product!"},
                {"rating": 4, "comment": "Good value for money."},
            ],
            "competitors": ["competitor1", "competitor2"],
        }
>       self.product = Product(self.product_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:164: TypeError
______________ TestProduct.test_product_calculate_average_rating ______________

self = <tests.unit.test_models.TestProduct testMethod=test_product_calculate_average_rating>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.product_data = {
            "id": "product1",
            "name": "Product A",
            "category": "electronics",
            "price": 99.99,
            "cost": 50.0,
            "description": "A high-quality electronic product",
            "features": ["feature1", "feature2", "feature3"],
            "specifications": {"weight": "1kg", "dimensions": "10x20x30cm"},
            "inventory": 100,
            "sales_history": [
                {"date": "2023-01-01", "quantity": 10, "revenue": 999.9},
                {"date": "2023-01-02", "quantity": 15, "revenue": 1499.85},
            ],
            "customer_reviews": [
                {"rating": 5, "comment": "Great product!"},
                {"rating": 4, "comment": "Good value for money."},
            ],
            "competitors": ["competitor1", "competitor2"],
        }
>       self.product = Product(self.product_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:164: TypeError
______________ TestProduct.test_product_calculate_profit_margin _______________

self = <tests.unit.test_models.TestProduct testMethod=test_product_calculate_profit_margin>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.product_data = {
            "id": "product1",
            "name": "Product A",
            "category": "electronics",
            "price": 99.99,
            "cost": 50.0,
            "description": "A high-quality electronic product",
            "features": ["feature1", "feature2", "feature3"],
            "specifications": {"weight": "1kg", "dimensions": "10x20x30cm"},
            "inventory": 100,
            "sales_history": [
                {"date": "2023-01-01", "quantity": 10, "revenue": 999.9},
                {"date": "2023-01-02", "quantity": 15, "revenue": 1499.85},
            ],
            "customer_reviews": [
                {"rating": 5, "comment": "Great product!"},
                {"rating": 4, "comment": "Good value for money."},
            ],
            "competitors": ["competitor1", "competitor2"],
        }
>       self.product = Product(self.product_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:164: TypeError
______________ TestProduct.test_product_calculate_total_revenue _______________

self = <tests.unit.test_models.TestProduct testMethod=test_product_calculate_total_revenue>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.product_data = {
            "id": "product1",
            "name": "Product A",
            "category": "electronics",
            "price": 99.99,
            "cost": 50.0,
            "description": "A high-quality electronic product",
            "features": ["feature1", "feature2", "feature3"],
            "specifications": {"weight": "1kg", "dimensions": "10x20x30cm"},
            "inventory": 100,
            "sales_history": [
                {"date": "2023-01-01", "quantity": 10, "revenue": 999.9},
                {"date": "2023-01-02", "quantity": 15, "revenue": 1499.85},
            ],
            "customer_reviews": [
                {"rating": 5, "comment": "Great product!"},
                {"rating": 4, "comment": "Good value for money."},
            ],
            "competitors": ["competitor1", "competitor2"],
        }
>       self.product = Product(self.product_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:164: TypeError
_______________ TestProduct.test_product_calculate_total_sales ________________

self = <tests.unit.test_models.TestProduct testMethod=test_product_calculate_total_sales>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.product_data = {
            "id": "product1",
            "name": "Product A",
            "category": "electronics",
            "price": 99.99,
            "cost": 50.0,
            "description": "A high-quality electronic product",
            "features": ["feature1", "feature2", "feature3"],
            "specifications": {"weight": "1kg", "dimensions": "10x20x30cm"},
            "inventory": 100,
            "sales_history": [
                {"date": "2023-01-01", "quantity": 10, "revenue": 999.9},
                {"date": "2023-01-02", "quantity": 15, "revenue": 1499.85},
            ],
            "customer_reviews": [
                {"rating": 5, "comment": "Great product!"},
                {"rating": 4, "comment": "Good value for money."},
            ],
            "competitors": ["competitor1", "competitor2"],
        }
>       self.product = Product(self.product_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:164: TypeError
_____________________ TestProduct.test_product_from_dict ______________________

self = <tests.unit.test_models.TestProduct testMethod=test_product_from_dict>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.product_data = {
            "id": "product1",
            "name": "Product A",
            "category": "electronics",
            "price": 99.99,
            "cost": 50.0,
            "description": "A high-quality electronic product",
            "features": ["feature1", "feature2", "feature3"],
            "specifications": {"weight": "1kg", "dimensions": "10x20x30cm"},
            "inventory": 100,
            "sales_history": [
                {"date": "2023-01-01", "quantity": 10, "revenue": 999.9},
                {"date": "2023-01-02", "quantity": 15, "revenue": 1499.85},
            ],
            "customer_reviews": [
                {"rating": 5, "comment": "Great product!"},
                {"rating": 4, "comment": "Good value for money."},
            ],
            "competitors": ["competitor1", "competitor2"],
        }
>       self.product = Product(self.product_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:164: TypeError
______________ TestProduct.test_product_get_best_selling_period _______________

self = <tests.unit.test_models.TestProduct testMethod=test_product_get_best_selling_period>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.product_data = {
            "id": "product1",
            "name": "Product A",
            "category": "electronics",
            "price": 99.99,
            "cost": 50.0,
            "description": "A high-quality electronic product",
            "features": ["feature1", "feature2", "feature3"],
            "specifications": {"weight": "1kg", "dimensions": "10x20x30cm"},
            "inventory": 100,
            "sales_history": [
                {"date": "2023-01-01", "quantity": 10, "revenue": 999.9},
                {"date": "2023-01-02", "quantity": 15, "revenue": 1499.85},
            ],
            "customer_reviews": [
                {"rating": 5, "comment": "Great product!"},
                {"rating": 4, "comment": "Good value for money."},
            ],
            "competitors": ["competitor1", "competitor2"],
        }
>       self.product = Product(self.product_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:164: TypeError
___________________ TestProduct.test_product_initialization ___________________

self = <tests.unit.test_models.TestProduct testMethod=test_product_initialization>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.product_data = {
            "id": "product1",
            "name": "Product A",
            "category": "electronics",
            "price": 99.99,
            "cost": 50.0,
            "description": "A high-quality electronic product",
            "features": ["feature1", "feature2", "feature3"],
            "specifications": {"weight": "1kg", "dimensions": "10x20x30cm"},
            "inventory": 100,
            "sales_history": [
                {"date": "2023-01-01", "quantity": 10, "revenue": 999.9},
                {"date": "2023-01-02", "quantity": 15, "revenue": 1499.85},
            ],
            "customer_reviews": [
                {"rating": 5, "comment": "Great product!"},
                {"rating": 4, "comment": "Good value for money."},
            ],
            "competitors": ["competitor1", "competitor2"],
        }
>       self.product = Product(self.product_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:164: TypeError
______________________ TestProduct.test_product_to_dict _______________________

self = <tests.unit.test_models.TestProduct testMethod=test_product_to_dict>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.product_data = {
            "id": "product1",
            "name": "Product A",
            "category": "electronics",
            "price": 99.99,
            "cost": 50.0,
            "description": "A high-quality electronic product",
            "features": ["feature1", "feature2", "feature3"],
            "specifications": {"weight": "1kg", "dimensions": "10x20x30cm"},
            "inventory": 100,
            "sales_history": [
                {"date": "2023-01-01", "quantity": 10, "revenue": 999.9},
                {"date": "2023-01-02", "quantity": 15, "revenue": 1499.85},
            ],
            "customer_reviews": [
                {"rating": 5, "comment": "Great product!"},
                {"rating": 4, "comment": "Good value for money."},
            ],
            "competitors": ["competitor1", "competitor2"],
        }
>       self.product = Product(self.product_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:164: TypeError
__________________ TestProduct.test_product_update_inventory __________________

self = <tests.unit.test_models.TestProduct testMethod=test_product_update_inventory>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.product_data = {
            "id": "product1",
            "name": "Product A",
            "category": "electronics",
            "price": 99.99,
            "cost": 50.0,
            "description": "A high-quality electronic product",
            "features": ["feature1", "feature2", "feature3"],
            "specifications": {"weight": "1kg", "dimensions": "10x20x30cm"},
            "inventory": 100,
            "sales_history": [
                {"date": "2023-01-01", "quantity": 10, "revenue": 999.9},
                {"date": "2023-01-02", "quantity": 15, "revenue": 1499.85},
            ],
            "customer_reviews": [
                {"rating": 5, "comment": "Great product!"},
                {"rating": 4, "comment": "Good value for money."},
            ],
            "competitors": ["competitor1", "competitor2"],
        }
>       self.product = Product(self.product_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:164: TypeError
____________________ TestProduct.test_product_update_price ____________________

self = <tests.unit.test_models.TestProduct testMethod=test_product_update_price>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.product_data = {
            "id": "product1",
            "name": "Product A",
            "category": "electronics",
            "price": 99.99,
            "cost": 50.0,
            "description": "A high-quality electronic product",
            "features": ["feature1", "feature2", "feature3"],
            "specifications": {"weight": "1kg", "dimensions": "10x20x30cm"},
            "inventory": 100,
            "sales_history": [
                {"date": "2023-01-01", "quantity": 10, "revenue": 999.9},
                {"date": "2023-01-02", "quantity": 15, "revenue": 1499.85},
            ],
            "customer_reviews": [
                {"rating": 5, "comment": "Great product!"},
                {"rating": 4, "comment": "Good value for money."},
            ],
            "competitors": ["competitor1", "competitor2"],
        }
>       self.product = Product(self.product_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:164: TypeError
___________ TestSalesResult.test_sales_result_add_customer_feedback ___________

self = <tests.unit.test_models.TestSalesResult testMethod=test_sales_result_add_customer_feedback>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.sales_result_data = {
            "id": "sales1",
            "product_id": "product1",
            "sales_agent_id": "agent1",
            "customer_id": "customer1",
            "quantity": 10,
            "unit_price": 99.99,
            "total_price": 999.9,
            "discount": 0.0,
            "sale_date": "2023-01-01",
            "sale_channel": "online",
            "payment_method": "credit_card",
            "shipping_address": "123 Main St, City, Country",
            "order_status": "completed",
            "delivery_date": "2023-01-05",
            "customer_feedback": "Great service!",
            "commission_rate": 0.1,
            "commission_amount": 99.99,
        }
>       self.sales_result = SalesResult(self.sales_result_data)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:340: TypeError
______________ TestSalesResult.test_sales_result_apply_discount _______________

self = <tests.unit.test_models.TestSalesResult testMethod=test_sales_result_apply_discount>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.sales_result_data = {
            "id": "sales1",
            "product_id": "product1",
            "sales_agent_id": "agent1",
            "customer_id": "customer1",
            "quantity": 10,
            "unit_price": 99.99,
            "total_price": 999.9,
            "discount": 0.0,
            "sale_date": "2023-01-01",
            "sale_channel": "online",
            "payment_method": "credit_card",
            "shipping_address": "123 Main St, City, Country",
            "order_status": "completed",
            "delivery_date": "2023-01-05",
            "customer_feedback": "Great service!",
            "commission_rate": 0.1,
            "commission_amount": 99.99,
        }
>       self.sales_result = SalesResult(self.sales_result_data)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:340: TypeError
___________ TestSalesResult.test_sales_result_calculate_commission ____________

self = <tests.unit.test_models.TestSalesResult testMethod=test_sales_result_calculate_commission>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.sales_result_data = {
            "id": "sales1",
            "product_id": "product1",
            "sales_agent_id": "agent1",
            "customer_id": "customer1",
            "quantity": 10,
            "unit_price": 99.99,
            "total_price": 999.9,
            "discount": 0.0,
            "sale_date": "2023-01-01",
            "sale_channel": "online",
            "payment_method": "credit_card",
            "shipping_address": "123 Main St, City, Country",
            "order_status": "completed",
            "delivery_date": "2023-01-05",
            "customer_feedback": "Great service!",
            "commission_rate": 0.1,
            "commission_amount": 99.99,
        }
>       self.sales_result = SalesResult(self.sales_result_data)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:340: TypeError
__________ TestSalesResult.test_sales_result_calculate_delivery_time __________

self = <tests.unit.test_models.TestSalesResult testMethod=test_sales_result_calculate_delivery_time>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.sales_result_data = {
            "id": "sales1",
            "product_id": "product1",
            "sales_agent_id": "agent1",
            "customer_id": "customer1",
            "quantity": 10,
            "unit_price": 99.99,
            "total_price": 999.9,
            "discount": 0.0,
            "sale_date": "2023-01-01",
            "sale_channel": "online",
            "payment_method": "credit_card",
            "shipping_address": "123 Main St, City, Country",
            "order_status": "completed",
            "delivery_date": "2023-01-05",
            "customer_feedback": "Great service!",
            "commission_rate": 0.1,
            "commission_amount": 99.99,
        }
>       self.sales_result = SalesResult(self.sales_result_data)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:340: TypeError
_____________ TestSalesResult.test_sales_result_calculate_profit ______________

self = <tests.unit.test_models.TestSalesResult testMethod=test_sales_result_calculate_profit>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.sales_result_data = {
            "id": "sales1",
            "product_id": "product1",
            "sales_agent_id": "agent1",
            "customer_id": "customer1",
            "quantity": 10,
            "unit_price": 99.99,
            "total_price": 999.9,
            "discount": 0.0,
            "sale_date": "2023-01-01",
            "sale_channel": "online",
            "payment_method": "credit_card",
            "shipping_address": "123 Main St, City, Country",
            "order_status": "completed",
            "delivery_date": "2023-01-05",
            "customer_feedback": "Great service!",
            "commission_rate": 0.1,
            "commission_amount": 99.99,
        }
>       self.sales_result = SalesResult(self.sales_result_data)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:340: TypeError
_________________ TestSalesResult.test_sales_result_from_dict _________________

self = <tests.unit.test_models.TestSalesResult testMethod=test_sales_result_from_dict>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.sales_result_data = {
            "id": "sales1",
            "product_id": "product1",
            "sales_agent_id": "agent1",
            "customer_id": "customer1",
            "quantity": 10,
            "unit_price": 99.99,
            "total_price": 999.9,
            "discount": 0.0,
            "sale_date": "2023-01-01",
            "sale_channel": "online",
            "payment_method": "credit_card",
            "shipping_address": "123 Main St, City, Country",
            "order_status": "completed",
            "delivery_date": "2023-01-05",
            "customer_feedback": "Great service!",
            "commission_rate": 0.1,
            "commission_amount": 99.99,
        }
>       self.sales_result = SalesResult(self.sales_result_data)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:340: TypeError
______________ TestSalesResult.test_sales_result_initialization _______________

self = <tests.unit.test_models.TestSalesResult testMethod=test_sales_result_initialization>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.sales_result_data = {
            "id": "sales1",
            "product_id": "product1",
            "sales_agent_id": "agent1",
            "customer_id": "customer1",
            "quantity": 10,
            "unit_price": 99.99,
            "total_price": 999.9,
            "discount": 0.0,
            "sale_date": "2023-01-01",
            "sale_channel": "online",
            "payment_method": "credit_card",
            "shipping_address": "123 Main St, City, Country",
            "order_status": "completed",
            "delivery_date": "2023-01-05",
            "customer_feedback": "Great service!",
            "commission_rate": 0.1,
            "commission_amount": 99.99,
        }
>       self.sales_result = SalesResult(self.sales_result_data)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:340: TypeError
__________________ TestSalesResult.test_sales_result_to_dict __________________

self = <tests.unit.test_models.TestSalesResult testMethod=test_sales_result_to_dict>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.sales_result_data = {
            "id": "sales1",
            "product_id": "product1",
            "sales_agent_id": "agent1",
            "customer_id": "customer1",
            "quantity": 10,
            "unit_price": 99.99,
            "total_price": 999.9,
            "discount": 0.0,
            "sale_date": "2023-01-01",
            "sale_channel": "online",
            "payment_method": "credit_card",
            "shipping_address": "123 Main St, City, Country",
            "order_status": "completed",
            "delivery_date": "2023-01-05",
            "customer_feedback": "Great service!",
            "commission_rate": 0.1,
            "commission_amount": 99.99,
        }
>       self.sales_result = SalesResult(self.sales_result_data)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:340: TypeError
___________ TestSalesResult.test_sales_result_update_delivery_date ____________

self = <tests.unit.test_models.TestSalesResult testMethod=test_sales_result_update_delivery_date>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.sales_result_data = {
            "id": "sales1",
            "product_id": "product1",
            "sales_agent_id": "agent1",
            "customer_id": "customer1",
            "quantity": 10,
            "unit_price": 99.99,
            "total_price": 999.9,
            "discount": 0.0,
            "sale_date": "2023-01-01",
            "sale_channel": "online",
            "payment_method": "credit_card",
            "shipping_address": "123 Main St, City, Country",
            "order_status": "completed",
            "delivery_date": "2023-01-05",
            "customer_feedback": "Great service!",
            "commission_rate": 0.1,
            "commission_amount": 99.99,
        }
>       self.sales_result = SalesResult(self.sales_result_data)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:340: TypeError
____________ TestSalesResult.test_sales_result_update_order_status ____________

self = <tests.unit.test_models.TestSalesResult testMethod=test_sales_result_update_order_status>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.sales_result_data = {
            "id": "sales1",
            "product_id": "product1",
            "sales_agent_id": "agent1",
            "customer_id": "customer1",
            "quantity": 10,
            "unit_price": 99.99,
            "total_price": 999.9,
            "discount": 0.0,
            "sale_date": "2023-01-01",
            "sale_channel": "online",
            "payment_method": "credit_card",
            "shipping_address": "123 Main St, City, Country",
            "order_status": "completed",
            "delivery_date": "2023-01-05",
            "customer_feedback": "Great service!",
            "commission_rate": 0.1,
            "commission_amount": 99.99,
        }
>       self.sales_result = SalesResult(self.sales_result_data)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:340: TypeError
______________ TestSalesResult.test_sales_result_update_quantity ______________

self = <tests.unit.test_models.TestSalesResult testMethod=test_sales_result_update_quantity>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.sales_result_data = {
            "id": "sales1",
            "product_id": "product1",
            "sales_agent_id": "agent1",
            "customer_id": "customer1",
            "quantity": 10,
            "unit_price": 99.99,
            "total_price": 999.9,
            "discount": 0.0,
            "sale_date": "2023-01-01",
            "sale_channel": "online",
            "payment_method": "credit_card",
            "shipping_address": "123 Main St, City, Country",
            "order_status": "completed",
            "delivery_date": "2023-01-05",
            "customer_feedback": "Great service!",
            "commission_rate": 0.1,
            "commission_amount": 99.99,
        }
>       self.sales_result = SalesResult(self.sales_result_data)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:340: TypeError
_____________ TestSalesResult.test_sales_result_update_unit_price _____________

self = <tests.unit.test_models.TestSalesResult testMethod=test_sales_result_update_unit_price>

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.sales_result_data = {
            "id": "sales1",
            "product_id": "product1",
            "sales_agent_id": "agent1",
            "customer_id": "customer1",
            "quantity": 10,
            "unit_price": 99.99,
            "total_price": 999.9,
            "discount": 0.0,
            "sale_date": "2023-01-01",
            "sale_channel": "online",
            "payment_method": "credit_card",
            "shipping_address": "123 Main St, City, Country",
            "order_status": "completed",
            "delivery_date": "2023-01-05",
            "customer_feedback": "Great service!",
            "commission_rate": 0.1,
            "commission_amount": 99.99,
        }
>       self.sales_result = SalesResult(self.sales_result_data)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_models.py:340: TypeError
___________________ test_advanced_agent_uses_central_params ___________________

tmp_path = WindowsPath('C:/Users/admin/AppData/Local/Temp/pytest-of-admin/pytest-49/test_advanced_agent_uses_centr0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x0000016F8C73AF90>

    def test_advanced_agent_uses_central_params(tmp_path, monkeypatch):
        _reset_params_cache()
        # Override a couple of AdvancedAgent parameters via YAML
        p = tmp_path / "agent_params.yaml"
        p.write_text(
            "\n".join(
                [
                    "version: '1.0'",
                    "advanced_agent:",
                    "  undercut: 0.02",
                    "  inv_low_ratio: 0.55",
                    "  inv_low_nudge: 0.25",
                ]
            ),
            encoding="utf-8",
        )
        monkeypatch.setenv("MODEL_PARAMS_YAML", str(p))
        get_model_params(force_reload=True)  # set cache from YAML
    
        agent = AdvancedAgent(config={"parameters": {}})
        # undercut picked from YAML defaults (no per-agent override supplied)
        assert pytest.approx(agent.undercut, rel=0, abs=1e-12) == 0.02
    
        # Inventory factor should follow configured thresholds/nudges
        # inventory_ratio below inv_low_ratio => inv_low_nudge
>       factor = agent._compute_inventory_factor(0.54)  # just below 0.55
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: AdvancedAgent._compute_inventory_factor() missing 1 required positional argument: 'avg_daily_demand'

tests\unit\test_models_and_configs.py:71: TypeError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:22.094 | INFO | agents.advanced_agent | [req=-] | AdvancedAgent[advanced_agent] configured for ASIN=B0DEFAULT min_margin=0.12 undercut=0.02 max_change_pct=0.15 price_sensitivity=0.1
------------------------------ Captured log call ------------------------------
INFO     agents.advanced_agent:advanced_agent.py:178 AdvancedAgent[advanced_agent] configured for ASIN=B0DEFAULT min_margin=0.12 undercut=0.02 max_change_pct=0.15 price_sensitivity=0.1
____________________ test_from_yaml_success_and_validation ____________________

cls = <class 'observability.observability_config.ObservabilityConfig'>
path = 'C:\\Users\\admin\\AppData\\Local\\Temp\\tmpn0oj1xp3.yaml'

    @classmethod
    def from_yaml(cls, path: str) -> ObservabilityConfig:
        """
        Load configuration from a YAML file.
    
        The YAML file may contain keys matching the dataclass fields.
    
        Raises:
            ValueError: if the file is missing, YAML is invalid, or validation fails.
        """
        if not path or not isinstance(path, str):
            raise ValueError("path must be a non-empty string")
        if not os.path.exists(path):
            raise ValueError(f"YAML config file not found: {path}")
        if yaml is None:
            raise ValueError("PyYAML is required to load YAML configurations")
    
        try:
>           with open(path, encoding="utf-8") as f:
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           PermissionError: [Errno 13] Permission denied: 'C:\\Users\\admin\\AppData\\Local\\Temp\\tmpn0oj1xp3.yaml'

observability\observability_config.py:132: PermissionError

During handling of the above exception, another exception occurred:

    def test_from_yaml_success_and_validation():
        content = dedent(
            """
            enable_trace_analysis: true
            llm_friendly_tools: false
            auto_error_correction: true
            real_time_alerts: true
            insight_generation_interval: 150
            error_pattern_window: 60
            performance_alert_threshold: 0.7
            trace_retention_days: 45
            """
        )
        with tempfile.NamedTemporaryFile("w+", suffix=".yaml", delete=True) as tmp:
            tmp.write(content)
            tmp.flush()
>           cfg = ObservabilityConfig.from_yaml(tmp.name)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_observability_config.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'observability.observability_config.ObservabilityConfig'>
path = 'C:\\Users\\admin\\AppData\\Local\\Temp\\tmpn0oj1xp3.yaml'

    @classmethod
    def from_yaml(cls, path: str) -> ObservabilityConfig:
        """
        Load configuration from a YAML file.
    
        The YAML file may contain keys matching the dataclass fields.
    
        Raises:
            ValueError: if the file is missing, YAML is invalid, or validation fails.
        """
        if not path or not isinstance(path, str):
            raise ValueError("path must be a non-empty string")
        if not os.path.exists(path):
            raise ValueError(f"YAML config file not found: {path}")
        if yaml is None:
            raise ValueError("PyYAML is required to load YAML configurations")
    
        try:
            with open(path, encoding="utf-8") as f:
                data = yaml.safe_load(f) or {}
        except Exception as e:
>           raise ValueError(f"Failed to read YAML file: {e}")
E           ValueError: Failed to read YAML file: [Errno 13] Permission denied: 'C:\\Users\\admin\\AppData\\Local\\Temp\\tmpn0oj1xp3.yaml'

observability\observability_config.py:135: ValueError
____________ TestPluginFramework.test_resolve_plugin_dependencies _____________

self = <tests.unit.test_plugins.TestPluginFramework testMethod=test_resolve_plugin_dependencies>

    def test_resolve_plugin_dependencies(self):
        """Test resolving plugin dependencies."""
        # Register a plugin type
        plugin_type = {
            "name": "agent",
            "description": "Agent plugins",
            "base_class": "BaseAgentPlugin",
            "interface": {"initialize": "method", "execute": "method", "cleanup": "method"},
        }
    
        type_id = self.plugin_framework.register_plugin_type(plugin_type)
    
        # Register plugins with dependencies
        plugin1 = {
            "name": "Plugin 1",
            "description": "A test plugin",
            "version": "1.0.0",
            "type": type_id,
            "module_path": "plugins.agent_plugins.test_plugin1",
            "class_name": "TestPlugin1",
            "config": {},
            "dependencies": [],
        }
    
        plugin2 = {
            "name": "Plugin 2",
            "description": "A test plugin",
            "version": "1.0.0",
            "type": type_id,
            "module_path": "plugins.agent_plugins.test_plugin2",
            "class_name": "TestPlugin2",
            "config": {},
            "dependencies": ["plugin1"],
        }
    
        plugin3 = {
            "name": "Plugin 3",
            "description": "A test plugin",
            "version": "1.0.0",
            "type": type_id,
            "module_path": "plugins.agent_plugins.test_plugin3",
            "class_name": "TestPlugin3",
            "config": {},
            "dependencies": ["plugin1", "plugin2"],
        }
    
        plugin1_id = self.plugin_framework.register_plugin(plugin1)
        plugin2_id = self.plugin_framework.register_plugin(plugin2)
        plugin3_id = self.plugin_framework.register_plugin(plugin3)
    
        # Update dependencies with actual plugin IDs
        self.plugin_framework._plugins[plugin2_id]["dependencies"] = [plugin1_id]
        self.plugin_framework._plugins[plugin3_id]["dependencies"] = [plugin1_id, plugin2_id]
    
        # Resolve dependencies
>       resolved_order = self.plugin_framework.resolve_plugin_dependencies(
            [plugin3_id, plugin2_id, plugin1_id]
        )

tests\unit\test_plugins.py:327: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <plugins.plugin_framework.PluginFramework object at 0x0000016F8B3FF500>
plugin_ids = ['plugin_Plugin 3_4885813d', 'plugin_Plugin 2_7552ea7f', 'plugin_Plugin 1_2bd1a301']

    def resolve_plugin_dependencies(self, plugin_ids: List[str]) -> List[str]:
        """Topological sort based on declared dependencies within the provided set."""
        deps = {pid: set(self._plugin_dependencies.get(pid, [])) for pid in plugin_ids}
        resolved: List[str] = []
        while deps:
            ready = [pid for pid, ds in deps.items() if not ds]
            if not ready:
                # Cycle or missing dependency; break deterministically
>               raise PluginError("Cyclic or unresolved dependencies detected")
E               plugins.plugin_framework.PluginError: Cyclic or unresolved dependencies detected

plugins\plugin_framework.py:214: PluginError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:22.235 | INFO | plugins.plugin_framework | [req=-] | PluginFramework initialized.
------------------------------ Captured log call ------------------------------
INFO     plugins.plugin_framework:plugin_framework.py:85 PluginFramework initialized.
_____________ TestBaseAgentPlugin.test_base_agent_plugin_get_info _____________

self = <tests.unit.test_plugins.TestBaseAgentPlugin testMethod=test_base_agent_plugin_get_info>

    def test_base_agent_plugin_get_info(self):
        """Test getting info about the base agent plugin."""
>       info = self.agent_plugin.get_info()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestAgentPlugin' object has no attribute 'get_info'

tests\unit\test_plugins.py:488: AttributeError
__________ TestBaseAgentPlugin.test_base_agent_plugin_initialization __________

self = <tests.unit.test_plugins.TestBaseAgentPlugin testMethod=test_base_agent_plugin_initialization>

    def test_base_agent_plugin_initialization(self):
        """Test that the base agent plugin initializes correctly."""
        self.assertIsNotNone(self.agent_plugin)
        self.assertFalse(self.agent_plugin.initialized)
>       self.assertEqual(self.agent_plugin.name, "")
E       AssertionError: 'Base Agent Plugin' != ''
E       - Base Agent Plugin

tests\unit\test_plugins.py:445: AssertionError
__________ TestBaseScenarioPlugin.test_base_scenario_plugin_get_info __________

self = <tests.unit.test_plugins.TestBaseScenarioPlugin testMethod=test_base_scenario_plugin_get_info>

    def test_base_scenario_plugin_get_info(self):
        """Test getting info about the base scenario plugin."""
>       info = self.scenario_plugin.get_info()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'TestScenarioPlugin' object has no attribute 'get_info'

tests\unit\test_plugins.py:598: AttributeError
_______ TestBaseScenarioPlugin.test_base_scenario_plugin_initialization _______

self = <tests.unit.test_plugins.TestBaseScenarioPlugin testMethod=test_base_scenario_plugin_initialization>

    def test_base_scenario_plugin_initialization(self):
        """Test that the base scenario plugin initializes correctly."""
        self.assertIsNotNone(self.scenario_plugin)
        self.assertFalse(self.scenario_plugin.initialized)
>       self.assertEqual(self.scenario_plugin.name, "")
E       AssertionError: 'Base Scenario Plugin' != ''
E       - Base Scenario Plugin

tests\unit\test_plugins.py:540: AssertionError
__________________ test_product_accepts_money_price_and_cost __________________

    def test_product_accepts_money_price_and_cost():
>       p = Product({"id": "ASIN1", "price": Money(1999, "USD"), "cost": Money(1000, "USD")})
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_product_model.py:6: TypeError
_________________ test_product_accepts_numeric_price_and_cost _________________

    def test_product_accepts_numeric_price_and_cost():
>       p = Product({"id": "ASIN2", "price": 29.99, "cost": 15.0})
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_product_model.py:13: TypeError
________________ test_product_to_dict_preserves_money_objects _________________

    def test_product_to_dict_preserves_money_objects():
>       p = Product({"id": "ASIN3", "price": 9.5, "cost": 4.0})
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given

tests\unit\test_product_model.py:20: TypeError
____________ TestCurriculumValidator.test_add_curriculum_template _____________

self = <tests.unit.test_scenarios.TestCurriculumValidator testMethod=test_add_curriculum_template>

    def test_add_curriculum_template(self):
        """Test adding a curriculum template."""
        template_data = {
            "name": "Beginner Curriculum",
            "description": "Curriculum for beginners",
            "levels": [
                {
                    "name": "Level 1",
                    "description": "Introduction",
                    "prerequisites": [],
                    "objectives": ["Learn basics"],
                    "scenarios": ["scenario1", "scenario2"],
                },
                {
                    "name": "Level 2",
                    "description": "Intermediate",
                    "prerequisites": ["Level 1"],
                    "objectives": ["Learn intermediate concepts"],
                    "scenarios": ["scenario3", "scenario4"],
                },
            ],
        }
    
>       template_id = self.curriculum_validator.add_curriculum_template(template_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CurriculumValidator' object has no attribute 'add_curriculum_template'

tests\unit\test_scenarios.py:135: AttributeError
______________ TestCurriculumValidator.test_add_validation_rule _______________

self = <tests.unit.test_scenarios.TestCurriculumValidator testMethod=test_add_validation_rule>

    def test_add_validation_rule(self):
        """Test adding a validation rule."""
        rule_data = {
            "name": "Prerequisite Check",
            "description": "Ensure prerequisites are met",
            "condition": "prerequisites_met",
            "severity": "error",
            "message": "Prerequisites not met",
        }
    
>       rule_id = self.curriculum_validator.add_validation_rule(rule_data)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CurriculumValidator' object has no attribute 'add_validation_rule'

tests\unit\test_scenarios.py:37: AttributeError
______ TestCurriculumValidator.test_curriculum_validator_initialization _______

self = <tests.unit.test_scenarios.TestCurriculumValidator testMethod=test_curriculum_validator_initialization>

    def test_curriculum_validator_initialization(self):
        """Test that the curriculum validator initializes correctly."""
        self.assertIsNotNone(self.curriculum_validator)
>       self.assertEqual(len(self.curriculum_validator._validation_rules), 0)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CurriculumValidator' object has no attribute '_validation_rules'

tests\unit\test_scenarios.py:24: AttributeError
___________ TestCurriculumValidator.test_delete_curriculum_template ___________

self = <tests.unit.test_scenarios.TestCurriculumValidator testMethod=test_delete_curriculum_template>

    def test_delete_curriculum_template(self):
        """Test deleting a curriculum template."""
        template_data = {
            "name": "Beginner Curriculum",
            "description": "Curriculum for beginners",
            "levels": [
                {
                    "name": "Level 1",
                    "description": "Introduction",
                    "prerequisites": [],
                    "objectives": ["Learn basics"],
                    "scenarios": ["scenario1", "scenario2"],
                }
            ],
        }
    
>       template_id = self.curriculum_validator.add_curriculum_template(template_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CurriculumValidator' object has no attribute 'add_curriculum_template'

tests\unit\test_scenarios.py:217: AttributeError
_____________ TestCurriculumValidator.test_delete_validation_rule _____________

self = <tests.unit.test_scenarios.TestCurriculumValidator testMethod=test_delete_validation_rule>

    def test_delete_validation_rule(self):
        """Test deleting a validation rule."""
        rule_data = {
            "name": "Prerequisite Check",
            "description": "Ensure prerequisites are met",
            "condition": "prerequisites_met",
            "severity": "error",
            "message": "Prerequisites not met",
        }
    
>       rule_id = self.curriculum_validator.add_validation_rule(rule_data)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CurriculumValidator' object has no attribute 'add_validation_rule'

tests\unit\test_scenarios.py:105: AttributeError
__________ TestCurriculumValidator.test_get_all_curriculum_templates __________

self = <tests.unit.test_scenarios.TestCurriculumValidator testMethod=test_get_all_curriculum_templates>

    def test_get_all_curriculum_templates(self):
        """Test getting all curriculum templates."""
        template1_data = {
            "name": "Beginner Curriculum",
            "description": "Curriculum for beginners",
            "levels": [
                {
                    "name": "Level 1",
                    "description": "Introduction",
                    "prerequisites": [],
                    "objectives": ["Learn basics"],
                    "scenarios": ["scenario1", "scenario2"],
                }
            ],
        }
    
        template2_data = {
            "name": "Advanced Curriculum",
            "description": "Curriculum for advanced users",
            "levels": [
                {
                    "name": "Level 1",
                    "description": "Advanced concepts",
                    "prerequisites": [],
                    "objectives": ["Learn advanced concepts"],
                    "scenarios": ["scenario3", "scenario4"],
                }
            ],
        }
    
>       self.curriculum_validator.add_curriculum_template(template1_data)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CurriculumValidator' object has no attribute 'add_curriculum_template'

tests\unit\test_scenarios.py:365: AttributeError
____________ TestCurriculumValidator.test_get_curriculum_template _____________

self = <tests.unit.test_scenarios.TestCurriculumValidator testMethod=test_get_curriculum_template>

    def test_get_curriculum_template(self):
        """Test getting a curriculum template."""
        template_data = {
            "name": "Beginner Curriculum",
            "description": "Curriculum for beginners",
            "levels": [
                {
                    "name": "Level 1",
                    "description": "Introduction",
                    "prerequisites": [],
                    "objectives": ["Learn basics"],
                    "scenarios": ["scenario1", "scenario2"],
                }
            ],
        }
    
>       template_id = self.curriculum_validator.add_curriculum_template(template_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CurriculumValidator' object has no attribute 'add_curriculum_template'

tests\unit\test_scenarios.py:326: AttributeError
___________ TestCurriculumValidator.test_update_curriculum_template ___________

self = <tests.unit.test_scenarios.TestCurriculumValidator testMethod=test_update_curriculum_template>

    def test_update_curriculum_template(self):
        """Test updating a curriculum template."""
        template_data = {
            "name": "Beginner Curriculum",
            "description": "Curriculum for beginners",
            "levels": [
                {
                    "name": "Level 1",
                    "description": "Introduction",
                    "prerequisites": [],
                    "objectives": ["Learn basics"],
                    "scenarios": ["scenario1", "scenario2"],
                }
            ],
        }
    
>       template_id = self.curriculum_validator.add_curriculum_template(template_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CurriculumValidator' object has no attribute 'add_curriculum_template'

tests\unit\test_scenarios.py:163: AttributeError
_____________ TestCurriculumValidator.test_update_validation_rule _____________

self = <tests.unit.test_scenarios.TestCurriculumValidator testMethod=test_update_validation_rule>

    def test_update_validation_rule(self):
        """Test updating a validation rule."""
        rule_data = {
            "name": "Prerequisite Check",
            "description": "Ensure prerequisites are met",
            "condition": "prerequisites_met",
            "severity": "error",
            "message": "Prerequisites not met",
        }
    
>       rule_id = self.curriculum_validator.add_validation_rule(rule_data)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CurriculumValidator' object has no attribute 'add_validation_rule'

tests\unit\test_scenarios.py:62: AttributeError
______________ TestCurriculumValidator.test_validate_curriculum _______________

self = <tests.unit.test_scenarios.TestCurriculumValidator testMethod=test_validate_curriculum>

    def test_validate_curriculum(self):
        """Test validating a curriculum."""
        # Add a validation rule
        rule_data = {
            "name": "Prerequisite Check",
            "description": "Ensure prerequisites are met",
            "condition": "prerequisites_met",
            "severity": "error",
            "message": "Prerequisites not met",
        }
    
>       self.curriculum_validator.add_validation_rule(rule_data)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CurriculumValidator' object has no attribute 'add_validation_rule'

tests\unit\test_scenarios.py:235: AttributeError
________ TestCurriculumValidator.test_validate_curriculum_with_errors _________

self = <tests.unit.test_scenarios.TestCurriculumValidator testMethod=test_validate_curriculum_with_errors>

    def test_validate_curriculum_with_errors(self):
        """Test validating a curriculum with errors."""
        # Add a validation rule
        rule_data = {
            "name": "Prerequisite Check",
            "description": "Ensure prerequisites are met",
            "condition": "prerequisites_met",
            "severity": "error",
            "message": "Prerequisites not met",
        }
    
>       self.curriculum_validator.add_validation_rule(rule_data)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'CurriculumValidator' object has no attribute 'add_validation_rule'

tests\unit\test_scenarios.py:278: AttributeError
_________________ TestScenarioFramework.test_create_scenario __________________

self = <tests.unit.test_scenarios.TestScenarioFramework testMethod=test_create_scenario>

    def test_create_scenario(self):
        """Test creating a scenario."""
        scenario_data = {
            "name": "Test Scenario",
            "description": "A test scenario",
            "category": "test",
            "parameters": {
                "param1": {"type": "string", "description": "Parameter 1", "default": "default1"},
                "param2": {"type": "integer", "description": "Parameter 2", "default": 10},
            },
            "steps": [
                {"name": "Step 1", "description": "First step"},
                {"name": "Step 2", "description": "Second step"},
            ],
            "expected_outcomes": ["Outcome 1", "Outcome 2"],
        }
    
>       scenario_id = self.scenario_framework.create_scenario(scenario_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ScenarioFramework' object has no attribute 'create_scenario'

tests\unit\test_scenarios.py:407: AttributeError
_____________ TestScenarioFramework.test_create_scenario_category _____________

self = <tests.unit.test_scenarios.TestScenarioFramework testMethod=test_create_scenario_category>

    def test_create_scenario_category(self):
        """Test creating a scenario category."""
        category_data = {
            "name": "Test Category",
            "description": "A category for test scenarios",
            "parent_category": None,
        }
    
>       category_id = self.scenario_framework.create_scenario_category(category_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ScenarioFramework' object has no attribute 'create_scenario_category'

tests\unit\test_scenarios.py:559: AttributeError
__________ TestScenarioFramework.test_create_scenario_from_template ___________

self = <tests.unit.test_scenarios.TestScenarioFramework testMethod=test_create_scenario_from_template>

    def test_create_scenario_from_template(self):
        """Test creating a scenario from a template."""
        template_data = {
            "name": "Test Template",
            "description": "A template for test scenarios",
            "category": "test",
            "parameters": {
                "param1": {"type": "string", "description": "Parameter 1", "default": "default1"},
                "param2": {"type": "integer", "description": "Parameter 2", "default": 10},
            },
            "steps": [
                {"name": "Step 1", "description": "First step", "template": True},
                {"name": "Step 2", "description": "Second step", "template": True},
            ],
        }
    
>       template_id = self.scenario_framework.create_scenario_template(template_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ScenarioFramework' object has no attribute 'create_scenario_template'

tests\unit\test_scenarios.py:615: AttributeError
_____________ TestScenarioFramework.test_create_scenario_template _____________

self = <tests.unit.test_scenarios.TestScenarioFramework testMethod=test_create_scenario_template>

    def test_create_scenario_template(self):
        """Test creating a scenario template."""
        template_data = {
            "name": "Test Template",
            "description": "A template for test scenarios",
            "category": "test",
            "parameters": {
                "param1": {"type": "string", "description": "Parameter 1", "default": "default1"},
                "param2": {"type": "integer", "description": "Parameter 2", "default": 10},
            },
            "steps": [
                {"name": "Step 1", "description": "First step", "template": True},
                {"name": "Step 2", "description": "Second step", "template": True},
            ],
        }
    
>       template_id = self.scenario_framework.create_scenario_template(template_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ScenarioFramework' object has no attribute 'create_scenario_template'

tests\unit\test_scenarios.py:587: AttributeError
_________________ TestScenarioFramework.test_delete_scenario __________________

self = <tests.unit.test_scenarios.TestScenarioFramework testMethod=test_delete_scenario>

    def test_delete_scenario(self):
        """Test deleting a scenario."""
        scenario_data = {
            "name": "Test Scenario",
            "description": "A test scenario",
            "category": "test",
            "parameters": {},
            "steps": [],
            "expected_outcomes": [],
        }
    
>       scenario_id = self.scenario_framework.create_scenario(scenario_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ScenarioFramework' object has no attribute 'create_scenario'

tests\unit\test_scenarios.py:481: AttributeError
_________________ TestScenarioFramework.test_execute_scenario _________________

self = <tests.unit.test_scenarios.TestScenarioFramework testMethod=test_execute_scenario>

    def test_execute_scenario(self):
        """Test executing a scenario."""
        scenario_data = {
            "name": "Test Scenario",
            "description": "A test scenario",
            "category": "test",
            "parameters": {
                "param1": {"type": "string", "description": "Parameter 1", "default": "default1"},
                "param2": {"type": "integer", "description": "Parameter 2", "default": 10},
            },
            "steps": [
                {"name": "Step 1", "description": "First step"},
                {"name": "Step 2", "description": "Second step"},
            ],
            "expected_outcomes": ["Outcome 1", "Outcome 2"],
        }
    
>       scenario_id = self.scenario_framework.create_scenario(scenario_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ScenarioFramework' object has no attribute 'create_scenario'

tests\unit\test_scenarios.py:661: AttributeError
_____________ TestScenarioFramework.test_generate_scenario_report _____________

self = <tests.unit.test_scenarios.TestScenarioFramework testMethod=test_generate_scenario_report>

    def test_generate_scenario_report(self):
        """Test generating a scenario report."""
        scenario_data = {
            "name": "Test Scenario",
            "description": "A test scenario",
            "category": "test",
            "parameters": {},
            "steps": [],
            "expected_outcomes": [],
        }
    
>       scenario_id = self.scenario_framework.create_scenario(scenario_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ScenarioFramework' object has no attribute 'create_scenario'

tests\unit\test_scenarios.py:782: AttributeError
___________________ TestScenarioFramework.test_get_scenario ___________________

self = <tests.unit.test_scenarios.TestScenarioFramework testMethod=test_get_scenario>

    def test_get_scenario(self):
        """Test getting a scenario."""
        scenario_data = {
            "name": "Test Scenario",
            "description": "A test scenario",
            "category": "test",
            "parameters": {},
            "steps": [],
            "expected_outcomes": [],
        }
    
>       scenario_id = self.scenario_framework.create_scenario(scenario_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ScenarioFramework' object has no attribute 'create_scenario'

tests\unit\test_scenarios.py:499: AttributeError
__________ TestScenarioFramework.test_get_scenario_execution_history __________

self = <tests.unit.test_scenarios.TestScenarioFramework testMethod=test_get_scenario_execution_history>

    def test_get_scenario_execution_history(self):
        """Test getting scenario execution history."""
        scenario_data = {
            "name": "Test Scenario",
            "description": "A test scenario",
            "category": "test",
            "parameters": {},
            "steps": [],
            "expected_outcomes": [],
        }
    
>       scenario_id = self.scenario_framework.create_scenario(scenario_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ScenarioFramework' object has no attribute 'create_scenario'

tests\unit\test_scenarios.py:742: AttributeError
____________ TestScenarioFramework.test_get_scenarios_by_category _____________

self = <tests.unit.test_scenarios.TestScenarioFramework testMethod=test_get_scenarios_by_category>

    def test_get_scenarios_by_category(self):
        """Test getting scenarios by category."""
        scenario1_data = {
            "name": "Test Scenario 1",
            "description": "A test scenario",
            "category": "test",
            "parameters": {},
            "steps": [],
            "expected_outcomes": [],
        }
    
        scenario2_data = {
            "name": "Test Scenario 2",
            "description": "Another test scenario",
            "category": "test",
            "parameters": {},
            "steps": [],
            "expected_outcomes": [],
        }
    
        scenario3_data = {
            "name": "Other Scenario",
            "description": "A scenario in another category",
            "category": "other",
            "parameters": {},
            "steps": [],
            "expected_outcomes": [],
        }
    
>       self.scenario_framework.create_scenario(scenario1_data)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ScenarioFramework' object has no attribute 'create_scenario'

tests\unit\test_scenarios.py:537: AttributeError
________ TestScenarioFramework.test_scenario_framework_initialization _________

self = <tests.unit.test_scenarios.TestScenarioFramework testMethod=test_scenario_framework_initialization>

    def test_scenario_framework_initialization(self):
        """Test that the scenario framework initializes correctly."""
        self.assertIsNotNone(self.scenario_framework)
>       self.assertEqual(len(self.scenario_framework._scenarios), 0)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ScenarioFramework' object has no attribute '_scenarios'

tests\unit\test_scenarios.py:386: AttributeError
_________________ TestScenarioFramework.test_update_scenario __________________

self = <tests.unit.test_scenarios.TestScenarioFramework testMethod=test_update_scenario>

    def test_update_scenario(self):
        """Test updating a scenario."""
        scenario_data = {
            "name": "Test Scenario",
            "description": "A test scenario",
            "category": "test",
            "parameters": {
                "param1": {"type": "string", "description": "Parameter 1", "default": "default1"},
                "param2": {"type": "integer", "description": "Parameter 2", "default": 10},
            },
            "steps": [
                {"name": "Step 1", "description": "First step"},
                {"name": "Step 2", "description": "Second step"},
            ],
            "expected_outcomes": ["Outcome 1", "Outcome 2"],
        }
    
>       scenario_id = self.scenario_framework.create_scenario(scenario_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ScenarioFramework' object has no attribute 'create_scenario'

tests\unit\test_scenarios.py:432: AttributeError
________________ TestScenarioFramework.test_validate_scenario _________________

self = <tests.unit.test_scenarios.TestScenarioFramework testMethod=test_validate_scenario>

    def test_validate_scenario(self):
        """Test validating a scenario."""
        scenario_data = {
            "name": "Test Scenario",
            "description": "A test scenario",
            "category": "test",
            "parameters": {
                "param1": {"type": "string", "description": "Parameter 1", "default": "default1"},
                "param2": {"type": "integer", "description": "Parameter 2", "default": 10},
            },
            "steps": [
                {"name": "Step 1", "description": "First step"},
                {"name": "Step 2", "description": "Second step"},
            ],
            "expected_outcomes": ["Outcome 1", "Outcome 2"],
        }
    
>       scenario_id = self.scenario_framework.create_scenario(scenario_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ScenarioFramework' object has no attribute 'create_scenario'

tests\unit\test_scenarios.py:698: AttributeError
__________ TestScenarioFramework.test_validate_scenario_with_errors ___________

self = <tests.unit.test_scenarios.TestScenarioFramework testMethod=test_validate_scenario_with_errors>

    def test_validate_scenario_with_errors(self):
        """Test validating a scenario with errors."""
        # Create a scenario with missing required fields
        scenario_data = {
            "name": "Invalid Scenario",
            "description": "An invalid scenario",
            "category": "test",
            "parameters": {},
            "steps": [],
            "expected_outcomes": [],
        }
    
>       scenario_id = self.scenario_framework.create_scenario(scenario_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ScenarioFramework' object has no attribute 'create_scenario'

tests\unit\test_scenarios.py:719: AttributeError
________ test_world_snapshot_populates_cache_and_observe_returns_data _________

event_bus = <fba_bench_core.event_bus.EventBus object at 0x0000016F8F561850>
toolbox_service = <fba_bench_core.services.toolbox_api_service.ToolboxAPIService object at 0x0000016F8F529F40>

    @pytest.mark.asyncio
    async def test_world_snapshot_populates_cache_and_observe_returns_data(
        event_bus: EventBus, toolbox_service: ToolboxAPIService
    ):
        asin = "B00TEST02"
        snapshot = WorldStateSnapshotEvent(
            event_id="snap-001",
            timestamp=datetime.now(timezone.utc),
            snapshot_id="snapshot-001",
            tick_number=0,
            product_count=1,
            summary_metrics={
                "products": {
                    asin: {
                        "price_cents": 1299,
                        "inventory": 50,
                        "bsr": 1200,
                        "conversion_rate": 0.12,
                    }
                }
            },
        )
        await event_bus.publish(snapshot)
        await asyncio.sleep(0.05)
    
        resp = toolbox_service.observe(ObserveRequest(asin=asin))
>       assert resp.found is True
E       AssertionError: assert False is True
E        +  where False = ObserveResponse(asin='B00TEST02', found=False, price=None, inventory=None, bsr=None, conversion_rate=None, timestamp=datetime.datetime(2025, 9, 16, 1, 31, 23, 312695, tzinfo=datetime.timezone.utc)).found

tests\unit\test_toolbox_api_service.py:70: AssertionError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:23.257 | INFO | fba_events.bus | [req=-] | Event published
------------------------------ Captured log call ------------------------------
INFO     fba_events.bus:bus.py:290 Event published
-------------------------- Captured stdout teardown ---------------------------
2025-09-16 01:31:23.315 | ERROR | fba_events.bus | [req=-] | Unhandled exception in InMemoryEventBus runner: Event loop is closed
Traceback (most recent call last):
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 158, in get
    await getter
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 309, in _runner
    event, event_type, ts = await self._queue.get()
                            ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 160, in get
    getter.cancel()  # Just in case getter is not done yet.
    ^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 799, in call_soon
    self._check_closed()
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 545, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
2025-09-16 01:31:23.316 | ERROR | asyncio | [req=-] | Task was destroyed but it is pending!
task: <Task pending name='InMemoryEventBusRunner' coro=<InMemoryEventBus._runner() done, defined at C:\Users\admin\Downloads\fba\fba_events\bus.py:306> wait_for=<Future cancelled>>
__________________ test_set_price_publishes_setpricecommand ___________________

event_bus = <fba_bench_core.event_bus.EventBus object at 0x0000016F8F561FD0>
toolbox_service = <fba_bench_core.services.toolbox_api_service.ToolboxAPIService object at 0x0000016F8F52A0F0>

    @pytest.mark.asyncio
    async def test_set_price_publishes_setpricecommand(
        event_bus: EventBus, toolbox_service: ToolboxAPIService
    ):
        asin = "B00TEST03"
        # prime minimal cache so observe works later if needed
        await event_bus.publish(
            WorldStateSnapshotEvent(
                event_id="snap-002",
                timestamp=datetime.now(timezone.utc),
                snapshot_id="snapshot-002",
                tick_number=0,
                product_count=1,
                summary_metrics={"products": {asin: {"price_cents": 1200}}},
            )
        )
        await asyncio.sleep(0.02)
    
        req = SetPriceRequest(
            agent_id="agent-1", asin=asin, new_price=Money.from_dollars("12.99"), reason="unit-test"
        )
        rsp = toolbox_service.set_price(req)
        assert rsp.accepted is True
        assert rsp.command_id
        assert rsp.asin == asin
        assert isinstance(rsp.new_price, Money)
        await asyncio.sleep(0.05)
    
        recorded = event_bus.get_recorded_events()
        assert any(
            e.get("event_type") == "SetPriceCommand" and e.get("data", {}).get("asin") == asin
>           for e in recorded
                     ^^^^^^^^
        )
E       TypeError: 'coroutine' object is not iterable

tests\unit\test_toolbox_api_service.py:110: TypeError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:23.322 | INFO | fba_events.bus | [req=-] | Event published
2025-09-16 01:31:23.344 | INFO | fba_events.bus | [req=-] | Event published
------------------------------ Captured log call ------------------------------
INFO     fba_events.bus:bus.py:290 Event published
INFO     fba_events.bus:bus.py:290 Event published
________________ TestDeterministicEnvironment.test_deactivate _________________

self = <tests.unit.test_validators.TestDeterministicEnvironment object at 0x0000016F8BBED5E0>
mock_environ = <MagicMock name='environ' id='1578657776752'>
mock_seed = <MagicMock name='seed' id='1578657777184'>

    @patch("benchmarking.validators.deterministic.random.seed")
    @patch("benchmarking.validators.deterministic.os.environ")
    def test_deactivate(self, mock_environ, mock_seed):
        """Test environment deactivation."""
        env = DeterministicEnvironment(seed=42)
    
        # Mock environment capture and restore
        with (
            patch.object(env, "_capture_environment_state") as mock_capture,
            patch.object(env, "_restore_environment_state") as mock_restore,
        ):
    
            initial_state = EnvironmentState(random_seed=0, python_hash_seed=0)
            final_state = EnvironmentState(random_seed=42, python_hash_seed=42)
    
            mock_capture.side_effect = [initial_state, final_state]
    
            env.activate()
>           result = env.deactivate()
                     ^^^^^^^^^^^^^^^^

tests\unit\test_validators.py:129: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\validators\deterministic.py:131: in deactivate
    final_state = self._capture_environment_state()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1139: in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1143: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <MagicMock name='_capture_environment_state' id='1578597181072'>
args = (), kwargs = {}, effect = <list_iterator object at 0x0000016F8F5B62C0>

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
>               result = next(effect)
                         ^^^^^^^^^^^^
E               StopIteration

C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:1200: StopIteration
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:23.434 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 42
2025-09-16 01:31:23.435 | INFO | benchmarking.validators.deterministic | [req=-] | Activated deterministic environment (state hash: 4560a593f6ce3afb39586b0c8285ac8cbe70c64e9f0bc61d369a0ee8aa671277)
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 42
INFO     benchmarking.validators.deterministic:deterministic.py:114 Activated deterministic environment (state hash: 4560a593f6ce3afb39586b0c8285ac8cbe70c64e9f0bc61d369a0ee8aa671277)
______________ TestVersionControlManager.test_add_python_module _______________

self = <tests.unit.test_validators.TestVersionControlManager object at 0x0000016F8BE17260>

    def test_add_python_module(self):
        """Test adding Python module."""
        self.manager.create_manifest("test_run")
    
        with (
            patch("benchmarking.validators.version_control.importlib.util.find_spec") as mock_find,
            patch("benchmarking.validators.version_control.importlib.import_module") as mock_import,
        ):
    
            # Mock module spec
            mock_spec = Mock()
            mock_spec.origin = "/path/to/module.py"
            mock_spec.name = "test_module"
            mock_spec.submodule_search_locations = []
            mock_find.return_value = mock_spec
    
            # Mock imported module
            mock_module = Mock()
            mock_module.__version__ = "1.0.0"
            mock_import.return_value = mock_module
    
>           component = self.manager.add_python_module("test_module", {"purpose": "testing"})
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\unit\test_validators.py:1042: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
benchmarking\validators\version_control.py:232: in add_python_module
    return self.add_component(
benchmarking\validators\version_control.py:173: in add_component
    self._component_cache[cache_key] = self._calculate_component_hash(path)
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <benchmarking.validators.version_control.VersionControlManager object at 0x0000016F8F7CB5C0>
path = '/path/to/module.py'

    def _calculate_component_hash(self, path: str) -> str:
        """Calculate hash of a component."""
        if not os.path.exists(path):
>           raise FileNotFoundError(f"Component not found: {path}")
E           FileNotFoundError: Component not found: /path/to/module.py

benchmarking\validators\version_control.py:520: FileNotFoundError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:23.883 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp3a4eiliq
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp3a4eiliq
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:24.020 | INFO | benchmarking.validators.version_control | [req=-] | Created version manifest for run: test_run
2025-09-16 01:31:24.021 | ERROR | benchmarking.validators.version_control | [req=-] | Failed to add Python module test_module: Component not found: /path/to/module.py
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.version_control:version_control.py:139 Created version manifest for run: test_run
ERROR    benchmarking.validators.version_control:version_control.py:241 Failed to add Python module test_module: Component not found: /path/to/module.py
___________ TestVersionControlManager.test_capture_environment_info ___________

self = <tests.unit.test_validators.TestVersionControlManager object at 0x0000016F8BE35250>

    def test_capture_environment_info(self):
        """Test environment info capture."""
        with (
            patch("benchmarking.validators.version_control.sys") as mock_sys,
            patch("benchmarking.validators.version_control.os") as mock_os,
            patch("benchmarking.validators.version_control.platform") as mock_platform,
        ):
    
            mock_sys.version = "3.9.0"
            mock_sys.platform = "linux"
            mock_os.environ = {"PATH": "/usr/bin", "HOME": "/home/user"}
            mock_platform.platform.return_value = "Linux-5.4.0-x86_64"
            mock_platform.machine.return_value = "x86_64"
            mock_platform.processor.return_value = "x86_64"
            mock_platform.python_version.return_value = "3.9.0"
            mock_platform.python_implementation.return_value = "CPython"
    
            env_info = self.manager._capture_environment_info()
    
>           assert env_info["python_version"] == "3.9.0"
E           AssertionError: assert '3.12.10 (tag... bit (AMD64)]' == '3.9.0'
E             
E             - 3.9.0
E             + 3.12.10 (tags/v3.12.10:0cc8128, Apr  8 2025, 12:21:36) [MSC v.1943 64 bit (AMD64)]

tests\unit\test_validators.py:1363: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:26.332 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpdr0iwg_n
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpdr0iwg_n
_______________ TestVersionControlManager.test_capture_git_info _______________

self = <tests.unit.test_validators.TestVersionControlManager object at 0x0000016F8BE35460>

    def test_capture_git_info(self):
        """Test git info capture."""
        with patch("benchmarking.validators.version_control.subprocess.run") as mock_run:
            # Mock git commands
            def mock_subprocess(command, **kwargs):
                result = Mock()
                if "rev-parse" in command:
                    result.stdout = "abc123def456"
                elif "branch" in command:
                    result.stdout = "main"
                elif "log" in command:
                    result.stdout = "Commit message"
                elif "status" in command:
                    result.stdout = ""
                return result
    
            mock_run.side_effect = mock_subprocess
    
            git_info = self.manager._capture_git_info()
    
            assert git_info["commit_hash"] == "abc123def456"
>           assert git_info["branch"] == "main"
E           AssertionError: assert 'abc123def456' == 'main'
E             
E             - main
E             + abc123def456

tests\unit\test_validators.py:1392: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:26.375 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\AppData\Local\Temp\tmprcmft3du
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\AppData\Local\Temp\tmprcmft3du
___________ TestVersionControlManager.test_capture_git_info_no_git ____________

self = <tests.unit.test_validators.TestVersionControlManager object at 0x0000016F8BE35670>

    def test_capture_git_info_no_git(self):
        """Test git info capture when git is not available."""
        with patch("benchmarking.validators.version_control.subprocess.run") as mock_run:
            mock_run.side_effect = FileNotFoundError("git not found")
    
            git_info = self.manager._capture_git_info()
    
>           assert git_info["error"] == "Git not available"
E           AssertionError: assert 'Not in a git...not available' == 'Git not available'
E             
E             - Git not available
E             + Not in a git repository or git not available

tests\unit\test_validators.py:1403: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:26.417 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpzn61rv6v
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpzn61rv6v
_____________________ TestAuditTrail.test_initialization ______________________

self = <tests.unit.test_validators.TestAuditTrail object at 0x0000016F8BE35580>

    def test_initialization(self):
        """Test AuditTrail initialization."""
        trail = AuditTrail(run_id="test_run")
    
        assert trail.run_id == "test_run"
        assert isinstance(trail.start_time, datetime)
        assert trail.end_time is None
        assert trail.events == []
        assert trail.metadata == {}
>       assert trail.checksum == ""
E       AssertionError: assert 'b53655b1dbf8...60401d3c16f50' == ''
E         
E         + b53655b1dbf82148d937e0b3e1b2632b8b1fc81b12f142157c460401d3c16f50

tests\unit\test_validators.py:1503: AssertionError
_________________ TestAuditTrail.test_add_event_wrong_run_id __________________

self = <tests.unit.test_validators.TestAuditTrail object at 0x0000016F8BE34620>

    def test_add_event_wrong_run_id(self):
        """Test adding event with wrong run ID."""
        trail = AuditTrail(run_id="test_run")
    
        event = AuditEvent(
            run_id="wrong_run",
            event_type="operation",
            component="test_component",
            action="test_action",
        )
    
>       with pytest.raises(ValueError, match="Event run_id does not match trail run_id"):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       Failed: DID NOT RAISE <class 'ValueError'>

tests\unit\test_validators.py:1533: Failed
________________________ TestAuditTrail.test_from_dict ________________________

self = <tests.unit.test_validators.TestAuditTrail object at 0x0000016F8BE35DC0>

    def test_from_dict(self):
        """Test creation from dictionary."""
        data = {
            "run_id": "test_run",
            "start_time": "2023-01-01T00:00:00",
            "end_time": "2023-01-01T01:00:00",
            "events": [
                {
                    "run_id": "test_run",
                    "event_type": "operation",
                    "component": "test_component",
                    "action": "test_action",
                    "severity": "info",
                    "user": "system",
                    "session_id": "",
                    "details": {},
                    "timestamp": "2023-01-01T00:30:00",
                    "event_id": "test_event_id",
                }
            ],
            "metadata": {"key": "value"},
            "checksum": "abc123",
        }
    
        trail = AuditTrail.from_dict(data)
    
        assert trail.run_id == "test_run"
        assert isinstance(trail.start_time, datetime)
        assert isinstance(trail.end_time, datetime)
        assert len(trail.events) == 1
        assert trail.events[0].event_type == "operation"
        assert trail.metadata == {"key": "value"}
>       assert trail.checksum == "abc123"
E       AssertionError: assert 'a06e2ca0f075...63640e0276212' == 'abc123'
E         
E         - abc123
E         + a06e2ca0f075e3cb5fbd109dec0d0ec12a7ed7cae862ea58a9a63640e0276212

tests\unit\test_validators.py:1601: AssertionError
__________________ TestAuditTrailManager.test_initialization __________________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE363F0>

    def test_initialization(self):
        """Test AuditTrailManager initialization."""
        assert self.manager.storage_path == Path(self.temp_dir)
        assert self.manager._active_trails == {}
>       assert self.manager._event_buffer == []
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'AuditTrailManager' object has no attribute '_event_buffer'

tests\unit\test_validators.py:1642: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:26.591 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp1fljrnf7
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp1fljrnf7
_______________ TestAuditTrailManager.test_start_trail_existing _______________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE36810>

    def test_start_trail_existing(self):
        """Test starting a trail that already exists."""
        self.manager.start_trail("test_run")
    
>       with pytest.raises(ValueError, match="Audit trail already exists for run"):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       Failed: DID NOT RAISE <class 'ValueError'>

tests\unit\test_validators.py:1658: Failed
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:26.637 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp2sn0rwtk
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp2sn0rwtk
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:26.638 | INFO | benchmarking.validators.audit_trail | [req=-] | Created audit trail for run: test_run
2025-09-16 01:31:26.639 | WARNING | benchmarking.validators.audit_trail | [req=-] | Audit trail already exists for run: test_run
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:182 Created audit trail for run: test_run
WARNING  benchmarking.validators.audit_trail:audit_trail.py:175 Audit trail already exists for run: test_run
____________________ TestAuditTrailManager.test_end_trail _____________________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE36A20>

    def test_end_trail(self):
        """Test ending a trail."""
        trail = self.manager.start_trail("test_run")
    
>       result = self.manager.end_trail("test_run")
                 ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'AuditTrailManager' object has no attribute 'end_trail'. Did you mean: 'load_trail'?

tests\unit\test_validators.py:1665: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:26.680 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpw10oyo7z
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpw10oyo7z
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:26.680 | INFO | benchmarking.validators.audit_trail | [req=-] | Created audit trail for run: test_run
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:182 Created audit trail for run: test_run
______________ TestAuditTrailManager.test_end_trail_nonexistent _______________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE36C30>

    def test_end_trail_nonexistent(self):
        """Test ending a non-existent trail."""
        with pytest.raises(ValueError, match="No active audit trail for run"):
>           self.manager.end_trail("nonexistent_run")
            ^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'AuditTrailManager' object has no attribute 'end_trail'. Did you mean: 'load_trail'?

tests\unit\test_validators.py:1674: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:26.722 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpyzxig63j
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpyzxig63j
____________________ TestAuditTrailManager.test_log_event _____________________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE36E40>

    def test_log_event(self):
        """Test logging an event."""
        self.manager.start_trail("test_run")
    
        event = self.manager.log_event(
            run_id="test_run",
            component="test_component",
            action="test_action",
            event_type="operation",
            details={"key": "value"},
            severity="info",
            user="test_user",
            session_id="test_session",
        )
    
>       assert isinstance(event, AuditEvent)
E       assert False
E        +  where False = isinstance(True, AuditEvent)

tests\unit\test_validators.py:1691: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:26.768 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp9bnj28vg
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp9bnj28vg
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:26.769 | INFO | benchmarking.validators.audit_trail | [req=-] | Created audit trail for run: test_run
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:182 Created audit trail for run: test_run
________________ TestAuditTrailManager.test_log_event_no_trail ________________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE37050>

    def test_log_event_no_trail(self):
        """Test logging event without active trail."""
>       with pytest.raises(ValueError, match="No active audit trail for run"):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       Failed: DID NOT RAISE <class 'ValueError'>

tests\unit\test_validators.py:1708: Failed
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:26.809 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpxiabwr15
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpxiabwr15
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:26.810 | ERROR | benchmarking.validators.audit_trail | [req=-] | No active audit trail found for run: nonexistent_run
------------------------------ Captured log call ------------------------------
ERROR    benchmarking.validators.audit_trail:audit_trail.py:248 No active audit trail found for run: nonexistent_run
____________________ TestAuditTrailManager.test_get_trail _____________________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE37470>

    def test_get_trail(self):
        """Test getting a trail."""
        trail = self.manager.start_trail("test_run")
    
>       result = self.manager.get_trail("test_run")
                 ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'AuditTrailManager' object has no attribute 'get_trail'. Did you mean: 'create_trail'?

tests\unit\test_validators.py:1734: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:26.853 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp9ms93c88
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp9ms93c88
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:26.853 | INFO | benchmarking.validators.audit_trail | [req=-] | Created audit trail for run: test_run
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:182 Created audit trail for run: test_run
______________ TestAuditTrailManager.test_get_trail_nonexistent _______________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE37680>

    def test_get_trail_nonexistent(self):
        """Test getting a non-existent trail."""
>       result = self.manager.get_trail("nonexistent_run")
                 ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'AuditTrailManager' object has no attribute 'get_trail'. Did you mean: 'create_trail'?

tests\unit\test_validators.py:1740: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:26.895 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmppz6fdolq
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmppz6fdolq
____________________ TestAuditTrailManager.test_save_trail ____________________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE37890>

    def test_save_trail(self):
        """Test saving a trail."""
        trail = self.manager.start_trail("test_run")
    
        # Add some events
        for i in range(3):
            self.manager.log_event(
                run_id="test_run",
                component="test_component",
                action=f"test_action_{i}",
                event_type="operation",
            )
    
>       self.manager.end_trail("test_run")
        ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'AuditTrailManager' object has no attribute 'end_trail'. Did you mean: 'load_trail'?

tests\unit\test_validators.py:1757: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:26.937 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpjt4kln50
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpjt4kln50
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:26.938 | INFO | benchmarking.validators.audit_trail | [req=-] | Created audit trail for run: test_run
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:182 Created audit trail for run: test_run
____________________ TestAuditTrailManager.test_load_trail ____________________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE376B0>

    def test_load_trail(self):
        """Test loading a trail."""
        # Create and save a trail first
        trail = self.manager.start_trail("test_run")
    
        for i in range(3):
            self.manager.log_event(
                run_id="test_run",
                component="test_component",
                action=f"test_action_{i}",
                event_type="operation",
            )
    
>       self.manager.end_trail("test_run")
        ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'AuditTrailManager' object has no attribute 'end_trail'. Did you mean: 'load_trail'?

tests\unit\test_validators.py:1786: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:26.980 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp8hzt5x7p
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp8hzt5x7p
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:26.981 | INFO | benchmarking.validators.audit_trail | [req=-] | Created audit trail for run: test_run
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:182 Created audit trail for run: test_run
___________________ TestAuditTrailManager.test_list_trails ____________________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE37140>

    def test_list_trails(self):
        """Test listing trails."""
        # Create and save multiple trails
        for i in range(3):
            trail = self.manager.start_trail(f"test_run_{i}")
    
            for j in range(2):
                self.manager.log_event(
                    run_id=f"test_run_{i}",
                    component="test_component",
                    action=f"test_action_{j}",
                    event_type="operation",
                )
    
>           self.manager.end_trail(f"test_run_{i}")
            ^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'AuditTrailManager' object has no attribute 'end_trail'. Did you mean: 'load_trail'?

tests\unit\test_validators.py:1813: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.025 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpy0bn__0e
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpy0bn__0e
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:27.026 | INFO | benchmarking.validators.audit_trail | [req=-] | Created audit trail for run: test_run_0
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:182 Created audit trail for run: test_run_0
_________________ TestAuditTrailManager.test_get_trail_report _________________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE36930>

    def test_get_trail_report(self):
        """Test getting trail report."""
        trail = self.manager.start_trail("test_run")
    
        # Add various events
        self.manager.log_event(
            run_id="test_run",
            component="component1",
            action="action1",
            event_type="operation",
            severity="info",
        )
    
        self.manager.log_event(
            run_id="test_run",
            component="component2",
            action="action2",
            event_type="error",
            severity="error",
        )
    
        self.manager.log_event(
            run_id="test_run",
            component="component1",
            action="action3",
            event_type="operation",
            severity="warning",
        )
    
>       self.manager.end_trail("test_run")
        ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'AuditTrailManager' object has no attribute 'end_trail'. Did you mean: 'load_trail'?

tests\unit\test_validators.py:1852: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.069 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpsa2q0_l5
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpsa2q0_l5
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:27.070 | INFO | benchmarking.validators.audit_trail | [req=-] | Created audit trail for run: test_run
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:182 Created audit trail for run: test_run
______________ TestAuditTrailManager.test_verify_trail_integrity ______________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE35DF0>

    def test_verify_trail_integrity(self):
        """Test trail integrity verification."""
        trail = self.manager.start_trail("test_run")
    
        # Add events
        for i in range(3):
            self.manager.log_event(
                run_id="test_run",
                component="test_component",
                action=f"test_action_{i}",
                event_type="operation",
            )
    
>       self.manager.end_trail("test_run")
        ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'AuditTrailManager' object has no attribute 'end_trail'. Did you mean: 'load_trail'?

tests\unit\test_validators.py:1880: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.112 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp_dqw6ymo
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp_dqw6ymo
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:27.113 | INFO | benchmarking.validators.audit_trail | [req=-] | Created audit trail for run: test_run
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:182 Created audit trail for run: test_run
_____ TestAuditTrailManager.test_verify_trail_integrity_checksum_mismatch _____

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE344D0>

    def test_verify_trail_integrity_checksum_mismatch(self):
        """Test trail integrity verification with checksum mismatch."""
        trail = self.manager.start_trail("test_run")
    
        # Add events
        for i in range(3):
            self.manager.log_event(
                run_id="test_run",
                component="test_component",
                action=f"test_action_{i}",
                event_type="operation",
            )
    
>       self.manager.end_trail("test_run")
        ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'AuditTrailManager' object has no attribute 'end_trail'. Did you mean: 'load_trail'?

tests\unit\test_validators.py:1898: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.155 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmprpz783zn
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmprpz783zn
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:27.156 | INFO | benchmarking.validators.audit_trail | [req=-] | Created audit trail for run: test_run
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:182 Created audit trail for run: test_run
__ TestAuditTrailManager.test_verify_trail_integrity_timestamp_out_of_order ___

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE37A40>

    def test_verify_trail_integrity_timestamp_out_of_order(self):
        """Test trail integrity verification with out-of-order timestamps."""
        trail = self.manager.start_trail("test_run")
    
        # Add events
        for i in range(3):
            self.manager.log_event(
                run_id="test_run",
                component="test_component",
                action=f"test_action_{i}",
                event_type="operation",
            )
    
>       self.manager.end_trail("test_run")
        ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'AuditTrailManager' object has no attribute 'end_trail'. Did you mean: 'load_trail'?

tests\unit\test_validators.py:1919: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.199 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpadkg3vue
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpadkg3vue
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:27.199 | INFO | benchmarking.validators.audit_trail | [req=-] | Created audit trail for run: test_run
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:182 Created audit trail for run: test_run
_______ TestAuditTrailManager.test_verify_trail_integrity_wrong_run_id ________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE37C50>

    def test_verify_trail_integrity_wrong_run_id(self):
        """Test trail integrity verification with wrong run ID."""
        trail = self.manager.start_trail("test_run")
    
        # Add events
        for i in range(3):
            self.manager.log_event(
                run_id="test_run",
                component="test_component",
                action=f"test_action_{i}",
                event_type="operation",
            )
    
>       self.manager.end_trail("test_run")
        ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'AuditTrailManager' object has no attribute 'end_trail'. Did you mean: 'load_trail'?

tests\unit\test_validators.py:1940: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.243 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpeqzwqd09
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpeqzwqd09
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:27.244 | INFO | benchmarking.validators.audit_trail | [req=-] | Created audit trail for run: test_run
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:182 Created audit trail for run: test_run
________________ TestAuditTrailManager.test_export_trail_json _________________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE37E60>

    def test_export_trail_json(self):
        """Test trail export to JSON."""
        trail = self.manager.start_trail("test_run")
    
        # Add events
        for i in range(3):
            self.manager.log_event(
                run_id="test_run",
                component="test_component",
                action=f"test_action_{i}",
                event_type="operation",
            )
    
>       self.manager.end_trail("test_run")
        ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'AuditTrailManager' object has no attribute 'end_trail'. Did you mean: 'load_trail'?

tests\unit\test_validators.py:1961: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.286 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp8hkb95rv
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp8hkb95rv
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:27.287 | INFO | benchmarking.validators.audit_trail | [req=-] | Created audit trail for run: test_run
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:182 Created audit trail for run: test_run
_________________ TestAuditTrailManager.test_export_trail_csv _________________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE4C0B0>

    def test_export_trail_csv(self):
        """Test trail export to CSV."""
        trail = self.manager.start_trail("test_run")
    
        # Add events
        for i in range(3):
            self.manager.log_event(
                run_id="test_run",
                component="test_component",
                action=f"test_action_{i}",
                event_type="operation",
            )
    
>       self.manager.end_trail("test_run")
        ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'AuditTrailManager' object has no attribute 'end_trail'. Did you mean: 'load_trail'?

tests\unit\test_validators.py:1991: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.330 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpz7qo_wac
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpz7qo_wac
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:27.331 | INFO | benchmarking.validators.audit_trail | [req=-] | Created audit trail for run: test_run
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:182 Created audit trail for run: test_run
___________ TestAuditTrailManager.test_export_trail_unknown_format ____________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE4C2C0>

    def test_export_trail_unknown_format(self):
        """Test trail export with unknown format."""
        trail = self.manager.start_trail("test_run")
    
        # Add events
        for i in range(3):
            self.manager.log_event(
                run_id="test_run",
                component="test_component",
                action=f"test_action_{i}",
                event_type="operation",
            )
    
>       self.manager.end_trail("test_run")
        ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'AuditTrailManager' object has no attribute 'end_trail'. Did you mean: 'load_trail'?

tests\unit\test_validators.py:2025: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.374 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpnwig2zp8
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpnwig2zp8
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:27.375 | INFO | benchmarking.validators.audit_trail | [req=-] | Created audit trail for run: test_run
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:182 Created audit trail for run: test_run
______________ TestAuditTrailManager.test_audit_context_success _______________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE4C6E0>

    def test_audit_context_success(self):
        """Test audit context manager with successful operation."""
        self.manager.start_trail("test_run")
    
        with self.manager.audit_context(
            run_id="test_run",
            component="test_component",
            action="test_operation",
            user="test_user",
            session_id="test_session",
        ):
            pass  # Successful operation
    
>       trail = self.manager.get_trail("test_run")
                ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'AuditTrailManager' object has no attribute 'get_trail'. Did you mean: 'create_trail'?

tests\unit\test_validators.py:2051: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.421 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpm6sw9n_i
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmpm6sw9n_i
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:27.422 | INFO | benchmarking.validators.audit_trail | [req=-] | Created audit trail for run: test_run
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:182 Created audit trail for run: test_run
_______________ TestAuditTrailManager.test_audit_context_error ________________

self = <tests.unit.test_validators.TestAuditTrailManager object at 0x0000016F8BE4C8F0>

    def test_audit_context_error(self):
        """Test audit context manager with error."""
        self.manager.start_trail("test_run")
    
        try:
            with self.manager.audit_context(
                run_id="test_run",
                component="test_component",
                action="test_operation",
                user="test_user",
                session_id="test_session",
            ):
                raise ValueError("Test error")
        except ValueError:
            pass  # Expected error
    
>       trail = self.manager.get_trail("test_run")
                ^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'AuditTrailManager' object has no attribute 'get_trail'. Did you mean: 'create_trail'?

tests\unit\test_validators.py:2076: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.465 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp9sz3woj7
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\AppData\Local\Temp\tmp9sz3woj7
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:27.466 | INFO | benchmarking.validators.audit_trail | [req=-] | Created audit trail for run: test_run
------------------------------ Captured log call ------------------------------
INFO     benchmarking.validators.audit_trail:audit_trail.py:182 Created audit trail for run: test_run
______________ TestReproducibilityValidator.test_initialization _______________

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4CD40>

    def test_initialization(self):
        """Test ReproducibilityValidator initialization."""
        assert self.validator.storage_path == Path(self.temp_dir)
>       assert self.validator.validation_history == []
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ReproducibilityValidator' object has no attribute 'validation_history'

tests\unit\test_validators.py:2103: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.511 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 1749090055
2025-09-16 01:31:27.511 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:27.511 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:27.512 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:27.512 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp2da3q4ly
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 1749090055
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp2da3q4ly
_________ TestReproducibilityValidator.test_validate_reproducibility __________

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4CF50>

    def test_validate_reproducibility(self):
        """Test reproducibility validation."""
        # Create reference results
        reference_results = {"metric1": [1, 2, 3, 4, 5], "metric2": [10, 20, 30, 40, 50]}
    
        # Create current results (same as reference)
        current_results = {"metric1": [1, 2, 3, 4, 5], "metric2": [10, 20, 30, 40, 50]}
    
>       report = self.validator.validate_reproducibility(
            reference_results=reference_results, current_results=current_results, tolerance=0.01
        )
E       TypeError: ReproducibilityValidator.validate_reproducibility() got an unexpected keyword argument 'tolerance'

tests\unit\test_validators.py:2113: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.557 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 163868757
2025-09-16 01:31:27.557 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:27.557 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:27.557 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:27.558 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpt3m9v10w
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 163868757
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpt3m9v10w
_____ TestReproducibilityValidator.test_validate_reproducibility_with_ids _____

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4D160>

    def test_validate_reproducibility_with_ids(self):
        """Test reproducibility validation with run IDs."""
        reference_results = {"metric1": [1, 2, 3, 4, 5]}
        current_results = {"metric1": [1, 2, 3, 4, 5]}
    
>       report = self.validator.validate_reproducibility(
            reference_results=reference_results,
            current_results=current_results,
            reference_run_id="ref_run",
            current_run_id="curr_run",
        )
E       TypeError: ReproducibilityValidator.validate_reproducibility() got an unexpected keyword argument 'reference_run_id'

tests\unit\test_validators.py:2131: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.597 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 1426778249
2025-09-16 01:31:27.598 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:27.598 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:27.598 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:27.598 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpraiuz5h3
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 1426778249
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpraiuz5h3
_ TestReproducibilityValidator.test_validate_reproducibility_not_reproducible _

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4D370>

    def test_validate_reproducibility_not_reproducible(self):
        """Test reproducibility validation with non-reproducible results."""
        reference_results = {"metric1": [1, 2, 3, 4, 5], "metric2": [10, 20, 30, 40, 50]}
    
        # Create current results (different from reference)
        current_results = {
            "metric1": [10, 20, 30, 40, 50],  # Different values
            "metric2": [10, 20, 30, 40, 50],
        }
    
>       report = self.validator.validate_reproducibility(
            reference_results=reference_results, current_results=current_results, tolerance=0.01
        )
E       TypeError: ReproducibilityValidator.validate_reproducibility() got an unexpected keyword argument 'tolerance'

tests\unit\test_validators.py:2151: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.639 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 3659684175
2025-09-16 01:31:27.639 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:27.639 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:27.640 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:27.640 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmptd39ccwp
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 3659684175
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmptd39ccwp
__ TestReproducibilityValidator.test_validate_reproducibility_missing_metric __

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4D580>

    def test_validate_reproducibility_missing_metric(self):
        """Test reproducibility validation with missing metric."""
        reference_results = {"metric1": [1, 2, 3, 4, 5], "metric2": [10, 20, 30, 40, 50]}
    
        # Create current results (missing metric2)
        current_results = {"metric1": [1, 2, 3, 4, 5]}
    
>       report = self.validator.validate_reproducibility(
            reference_results=reference_results, current_results=current_results, tolerance=0.01
        )
E       TypeError: ReproducibilityValidator.validate_reproducibility() got an unexpected keyword argument 'tolerance'

tests\unit\test_validators.py:2166: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.681 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 685744792
2025-09-16 01:31:27.681 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:27.681 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:27.681 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:27.681 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpvdnyl913
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 685744792
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpvdnyl913
___ TestReproducibilityValidator.test_validate_reproducibility_extra_metric ___

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4D790>

    def test_validate_reproducibility_extra_metric(self):
        """Test reproducibility validation with extra metric."""
        reference_results = {"metric1": [1, 2, 3, 4, 5]}
    
        # Create current results (extra metric2)
        current_results = {"metric1": [1, 2, 3, 4, 5], "metric2": [10, 20, 30, 40, 50]}
    
>       report = self.validator.validate_reproducibility(
            reference_results=reference_results, current_results=current_results, tolerance=0.01
        )
E       TypeError: ReproducibilityValidator.validate_reproducibility() got an unexpected keyword argument 'tolerance'

tests\unit\test_validators.py:2182: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.722 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 1052968588
2025-09-16 01:31:27.722 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:27.722 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:27.723 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:27.723 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpd1f0pliy
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 1052968588
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpd1f0pliy
_ TestReproducibilityValidator.test_validate_reproducibility_different_lengths _

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4D9A0>

    def test_validate_reproducibility_different_lengths(self):
        """Test reproducibility validation with different length arrays."""
        reference_results = {"metric1": [1, 2, 3, 4, 5]}
    
        # Create current results (different length)
        current_results = {"metric1": [1, 2, 3, 4, 5, 6]}
    
>       report = self.validator.validate_reproducibility(
            reference_results=reference_results, current_results=current_results, tolerance=0.01
        )
E       TypeError: ReproducibilityValidator.validate_reproducibility() got an unexpected keyword argument 'tolerance'

tests\unit\test_validators.py:2198: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.765 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 2566670147
2025-09-16 01:31:27.766 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:27.766 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:27.766 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:27.766 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp89yaoe21
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 2566670147
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp89yaoe21
__ TestReproducibilityValidator.test_validate_reproducibility_empty_results ___

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE37F50>

    def test_validate_reproducibility_empty_results(self):
        """Test reproducibility validation with empty results."""
        with pytest.raises(ValueError, match="Results cannot be empty"):
>           self.validator.validate_reproducibility(
                reference_results={}, current_results={"metric1": [1, 2, 3]}
            )
E           TypeError: ReproducibilityValidator.validate_reproducibility() missing 1 required positional argument: 'run_id'

tests\unit\test_validators.py:2209: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.806 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 300119278
2025-09-16 01:31:27.807 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:27.807 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:27.807 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:27.807 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp12a2kykn
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 300119278
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp12a2kykn
_ TestReproducibilityValidator.test_validate_reproducibility_invalid_tolerance _

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE36510>

    def test_validate_reproducibility_invalid_tolerance(self):
        """Test reproducibility validation with invalid tolerance."""
        with pytest.raises(ValueError, match="Tolerance must be non-negative"):
>           self.validator.validate_reproducibility(
                reference_results={"metric1": [1, 2, 3]},
                current_results={"metric1": [1, 2, 3]},
                tolerance=-0.01,
            )
E           TypeError: ReproducibilityValidator.validate_reproducibility() got an unexpected keyword argument 'tolerance'

tests\unit\test_validators.py:2216: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.849 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 1925527091
2025-09-16 01:31:27.849 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:27.849 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:27.849 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:27.849 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpg5lp9g88
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 1925527091
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpg5lp9g88
___ TestReproducibilityValidator.test_validate_reproducibility_invalid_data ___

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4CB90>

    def test_validate_reproducibility_invalid_data(self):
        """Test reproducibility validation with invalid data."""
        with pytest.raises(ValueError, match="Reference results must be a dictionary"):
>           self.validator.validate_reproducibility(
                reference_results="invalid", current_results={"metric1": [1, 2, 3]}
            )
E           TypeError: ReproducibilityValidator.validate_reproducibility() missing 1 required positional argument: 'run_id'

tests\unit\test_validators.py:2225: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.891 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 3888657619
2025-09-16 01:31:27.892 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:27.892 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:27.892 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:27.892 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp437xsaqc
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 3888657619
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp437xsaqc
_ TestReproducibilityValidator.test_validate_reproducibility_invalid_current_data _

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4D3A0>

    def test_validate_reproducibility_invalid_current_data(self):
        """Test reproducibility validation with invalid current data."""
        with pytest.raises(ValueError, match="Current results must be a dictionary"):
>           self.validator.validate_reproducibility(
                reference_results={"metric1": [1, 2, 3]}, current_results="invalid"
            )
E           TypeError: ReproducibilityValidator.validate_reproducibility() missing 1 required positional argument: 'run_id'

tests\unit\test_validators.py:2232: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.933 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 610637279
2025-09-16 01:31:27.934 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:27.934 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:27.934 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:27.934 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmppmt4l685
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 610637279
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmppmt4l685
_ TestReproducibilityValidator.test_validate_reproducibility_invalid_metric_data _

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4CE30>

    def test_validate_reproducibility_invalid_metric_data(self):
        """Test reproducibility validation with invalid metric data."""
        with pytest.raises(ValueError, match="Metric values must be lists of numbers"):
>           self.validator.validate_reproducibility(
                reference_results={"metric1": "invalid"}, current_results={"metric1": [1, 2, 3]}
            )
E           TypeError: ReproducibilityValidator.validate_reproducibility() missing 1 required positional argument: 'run_id'

tests\unit\test_validators.py:2239: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:27.976 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 91207813
2025-09-16 01:31:27.976 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:27.976 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:27.976 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:27.976 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp6jz7ryr9
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 91207813
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp6jz7ryr9
_ TestReproducibilityValidator.test_validate_reproducibility_invalid_current_metric_data _

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4C3E0>

    def test_validate_reproducibility_invalid_current_metric_data(self):
        """Test reproducibility validation with invalid current metric data."""
        with pytest.raises(ValueError, match="Current metric values must be lists of numbers"):
>           self.validator.validate_reproducibility(
                reference_results={"metric1": [1, 2, 3]}, current_results={"metric1": "invalid"}
            )
E           TypeError: ReproducibilityValidator.validate_reproducibility() missing 1 required positional argument: 'run_id'

tests\unit\test_validators.py:2246: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:28.021 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 3913361813
2025-09-16 01:31:28.021 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:28.021 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:28.022 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:28.022 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpfii97kvu
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 3913361813
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpfii97kvu
_ TestReproducibilityValidator.test_validate_reproducibility_empty_metric_data _

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4DD60>

    def test_validate_reproducibility_empty_metric_data(self):
        """Test reproducibility validation with empty metric data."""
        with pytest.raises(ValueError, match="Metric values cannot be empty"):
>           self.validator.validate_reproducibility(
                reference_results={"metric1": []}, current_results={"metric1": [1, 2, 3]}
            )
E           TypeError: ReproducibilityValidator.validate_reproducibility() missing 1 required positional argument: 'run_id'

tests\unit\test_validators.py:2253: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:28.064 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 2462768976
2025-09-16 01:31:28.065 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:28.065 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:28.065 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:28.065 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpnjoz2foq
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 2462768976
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpnjoz2foq
_ TestReproducibilityValidator.test_validate_reproducibility_empty_current_metric_data _

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4DF70>

    def test_validate_reproducibility_empty_current_metric_data(self):
        """Test reproducibility validation with empty current metric data."""
        with pytest.raises(ValueError, match="Current metric values cannot be empty"):
>           self.validator.validate_reproducibility(
                reference_results={"metric1": [1, 2, 3]}, current_results={"metric1": []}
            )
E           TypeError: ReproducibilityValidator.validate_reproducibility() missing 1 required positional argument: 'run_id'

tests\unit\test_validators.py:2260: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:28.106 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 1139548037
2025-09-16 01:31:28.107 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:28.107 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:28.107 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:28.107 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp3wnx094k
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 1139548037
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp3wnx094k
_ TestReproducibilityValidator.test_validate_reproducibility_non_numeric_data _

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4E180>

    def test_validate_reproducibility_non_numeric_data(self):
        """Test reproducibility validation with non-numeric data."""
        with pytest.raises(ValueError, match="Metric values must be lists of numbers"):
>           self.validator.validate_reproducibility(
                reference_results={"metric1": [1, 2, "invalid"]},
                current_results={"metric1": [1, 2, 3]},
            )
E           TypeError: ReproducibilityValidator.validate_reproducibility() missing 1 required positional argument: 'run_id'

tests\unit\test_validators.py:2267: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:28.149 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 3598485280
2025-09-16 01:31:28.149 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:28.149 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:28.149 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:28.149 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp8e4ljim9
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 3598485280
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp8e4ljim9
_ TestReproducibilityValidator.test_validate_reproducibility_non_numeric_current_data _

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4E390>

    def test_validate_reproducibility_non_numeric_current_data(self):
        """Test reproducibility validation with non-numeric current data."""
        with pytest.raises(ValueError, match="Current metric values must be lists of numbers"):
>           self.validator.validate_reproducibility(
                reference_results={"metric1": [1, 2, 3]},
                current_results={"metric1": [1, 2, "invalid"]},
            )
E           TypeError: ReproducibilityValidator.validate_reproducibility() missing 1 required positional argument: 'run_id'

tests\unit\test_validators.py:2275: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:28.190 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 3301948298
2025-09-16 01:31:28.190 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:28.190 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:28.190 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:28.190 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp3fodd142
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 3301948298
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp3fodd142
________________ TestReproducibilityValidator.test_save_report ________________

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4E5A0>

    def test_save_report(self):
        """Test saving reproducibility report."""
        reference_results = {"metric1": [1, 2, 3, 4, 5]}
        current_results = {"metric1": [1, 2, 3, 4, 5]}
    
>       report = self.validator.validate_reproducibility(
            reference_results=reference_results,
            current_results=current_results,
            reference_run_id="ref_run",
            current_run_id="curr_run",
        )
E       TypeError: ReproducibilityValidator.validate_reproducibility() got an unexpected keyword argument 'reference_run_id'

tests\unit\test_validators.py:2285: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:28.230 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 3442401635
2025-09-16 01:31:28.231 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:28.231 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:28.231 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:28.231 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpmklcmn82
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 3442401635
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpmklcmn82
__________ TestReproducibilityValidator.test_save_report_no_filename __________

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4E7B0>

    def test_save_report_no_filename(self):
        """Test saving report without filename."""
        reference_results = {"metric1": [1, 2, 3, 4, 5]}
        current_results = {"metric1": [1, 2, 3, 4, 5]}
    
>       report = self.validator.validate_reproducibility(
            reference_results=reference_results, current_results=current_results
        )
E       TypeError: ReproducibilityValidator.validate_reproducibility() missing 1 required positional argument: 'run_id'

tests\unit\test_validators.py:2312: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:28.272 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 1354044933
2025-09-16 01:31:28.273 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:28.273 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:28.273 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:28.273 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp6sivdl9m
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 1354044933
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp6sivdl9m
________________ TestReproducibilityValidator.test_load_report ________________

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4E9C0>

    def test_load_report(self):
        """Test loading reproducibility report."""
        reference_results = {"metric1": [1, 2, 3, 4, 5]}
        current_results = {"metric1": [1, 2, 3, 4, 5]}
    
>       report = self.validator.validate_reproducibility(
            reference_results=reference_results,
            current_results=current_results,
            reference_run_id="ref_run",
            current_run_id="curr_run",
        )
E       TypeError: ReproducibilityValidator.validate_reproducibility() got an unexpected keyword argument 'reference_run_id'

tests\unit\test_validators.py:2327: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:28.315 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 2202253638
2025-09-16 01:31:28.315 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:28.315 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:28.315 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:28.315 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp081bvv1t
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 2202253638
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp081bvv1t
_______________ TestReproducibilityValidator.test_list_reports ________________

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4EBD0>

    def test_list_reports(self):
        """Test listing reproducibility reports."""
        # Create and save multiple reports
        for i in range(3):
            reference_results = {"metric1": [1, 2, 3, 4, 5]}
            current_results = {"metric1": [1, 2, 3, 4, 5]}
    
>           report = self.validator.validate_reproducibility(
                reference_results=reference_results,
                current_results=current_results,
                reference_run_id=f"ref_run_{i}",
                current_run_id=f"curr_run_{i}",
            )
E           TypeError: ReproducibilityValidator.validate_reproducibility() got an unexpected keyword argument 'reference_run_id'

tests\unit\test_validators.py:2352: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:28.357 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 2871057736
2025-09-16 01:31:28.358 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:28.358 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:28.358 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:28.358 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpmwh9wrzh
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 2871057736
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpmwh9wrzh
__________ TestReproducibilityValidator.test_get_validation_history ___________

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4EDE0>

    def test_get_validation_history(self):
        """Test getting validation history."""
        # Create and save multiple reports
        for i in range(3):
            reference_results = {"metric1": [1, 2, 3, 4, 5]}
            current_results = {"metric1": [1, 2, 3, 4, 5]}
    
>           report = self.validator.validate_reproducibility(
                reference_results=reference_results,
                current_results=current_results,
                reference_run_id=f"ref_run_{i}",
                current_run_id=f"curr_run_{i}",
            )
E           TypeError: ReproducibilityValidator.validate_reproducibility() got an unexpected keyword argument 'reference_run_id'

tests\unit\test_validators.py:2375: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:28.401 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 3522690020
2025-09-16 01:31:28.402 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:28.402 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:28.402 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:28.402 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp0soeo08g
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 3522690020
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmp0soeo08g
_________ TestReproducibilityValidator.test_clear_validation_history __________

self = <tests.unit.test_validators.TestReproducibilityValidator object at 0x0000016F8BE4EFF0>

    def test_clear_validation_history(self):
        """Test clearing validation history."""
        # Create and save multiple reports
        for i in range(3):
            reference_results = {"metric1": [1, 2, 3, 4, 5]}
            current_results = {"metric1": [1, 2, 3, 4, 5]}
    
>           report = self.validator.validate_reproducibility(
                reference_results=reference_results, current_results=current_results
            )
E           TypeError: ReproducibilityValidator.validate_reproducibility() missing 1 required positional argument: 'run_id'

tests\unit\test_validators.py:2396: TypeError
---------------------------- Captured stdout setup ----------------------------
2025-09-16 01:31:28.444 | INFO | benchmarking.validators.deterministic | [req=-] | Initialized deterministic environment with seed: 3924079325
2025-09-16 01:31:28.444 | INFO | benchmarking.validators.version_control | [req=-] | Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
2025-09-16 01:31:28.444 | INFO | benchmarking.validators.statistical_validator | [req=-] | Initialized StatisticalValidator with confidence level: 0.95
2025-09-16 01:31:28.444 | INFO | benchmarking.validators.audit_trail | [req=-] | Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
2025-09-16 01:31:28.444 | INFO | benchmarking.validators.reproducibility_validator | [req=-] | Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpteiy6wq3
----------------------------- Captured log setup ------------------------------
INFO     benchmarking.validators.deterministic:deterministic.py:70 Initialized deterministic environment with seed: 3924079325
INFO     benchmarking.validators.version_control:version_control.py:119 Initialized VersionControlManager with storage at: C:\Users\admin\Downloads\fba\version_manifests
INFO     benchmarking.validators.statistical_validator:statistical_validator.py:131 Initialized StatisticalValidator with confidence level: 0.95
INFO     benchmarking.validators.audit_trail:audit_trail.py:160 Initialized AuditTrailManager with storage at: C:\Users\admin\Downloads\fba\audit_trails
INFO     benchmarking.validators.reproducibility_validator:reproducibility_validator.py:116 Initialized ReproducibilityValidator with storage at: C:\Users\admin\AppData\Local\Temp\tmpteiy6wq3
________________ TestReproducibilityReport.test_initialization ________________

self = <tests.unit.test_validators.TestReproducibilityReport object at 0x0000016F8BE4F2C0>

    def test_initialization(self):
        """Test ReproducibilityReport initialization."""
>       report = ReproducibilityReport(
            reference_run_id="ref_run",
            current_run_id="curr_run",
            overall_reproducible=True,
            metric_results={"metric1": {"reproducible": True, "issues": []}},
        )
E       TypeError: ReproducibilityReport.__init__() got an unexpected keyword argument 'current_run_id'

tests\unit\test_validators.py:2419: TypeError
___________________ TestReproducibilityReport.test_to_dict ____________________

self = <tests.unit.test_validators.TestReproducibilityReport object at 0x0000016F8BE4F4A0>

    def test_to_dict(self):
        """Test dictionary conversion."""
>       report = ReproducibilityReport(
            reference_run_id="ref_run",
            current_run_id="curr_run",
            overall_reproducible=True,
            metric_results={"metric1": {"reproducible": True, "issues": []}},
        )
E       TypeError: ReproducibilityReport.__init__() got an unexpected keyword argument 'current_run_id'

tests\unit\test_validators.py:2434: TypeError
__________________ TestReproducibilityReport.test_from_dict ___________________

self = <tests.unit.test_validators.TestReproducibilityReport object at 0x0000016F8BE4F680>

    def test_from_dict(self):
        """Test creation from dictionary."""
        data = {
            "reference_run_id": "ref_run",
            "current_run_id": "curr_run",
            "overall_reproducible": True,
            "metric_results": {"metric1": {"reproducible": True, "issues": []}},
            "validation_timestamp": "2023-01-01T00:00:00",
        }
    
>       report = ReproducibilityReport.from_dict(data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: type object 'ReproducibilityReport' has no attribute 'from_dict'. Did you mean: 'to_dict'?

tests\unit\test_validators.py:2459: AttributeError
__________________ TestValidationResult.test_initialization ___________________

self = <tests.unit.test_validators.TestValidationResult object at 0x0000016F8BE4F980>

    def test_initialization(self):
        """Test ValidationResult initialization."""
>       result = ValidationResult(
            metric_name="test_metric",
            reproducible=True,
            issues=[],
            reference_stats={"mean": 5.0, "std": 2.0},
            current_stats={"mean": 5.0, "std": 2.0},
            difference={"mean": 0.0, "std": 0.0},
        )
E       TypeError: ValidationResult.__init__() got an unexpected keyword argument 'reproducible'

tests\unit\test_validators.py:2473: TypeError
______________________ TestValidationResult.test_to_dict ______________________

self = <tests.unit.test_validators.TestValidationResult object at 0x0000016F8BE4FB60>

    def test_to_dict(self):
        """Test dictionary conversion."""
>       result = ValidationResult(
            metric_name="test_metric",
            reproducible=True,
            issues=[],
            reference_stats={"mean": 5.0, "std": 2.0},
            current_stats={"mean": 5.0, "std": 2.0},
            difference={"mean": 0.0, "std": 0.0},
        )
E       TypeError: ValidationResult.__init__() got an unexpected keyword argument 'reproducible'

tests\unit\test_validators.py:2491: TypeError
____________________________ test_version_exposed _____________________________

    def test_version_exposed():
>       assert __version__ == "3.0.0"
E       AssertionError: assert '3.1.0-alpha' == '3.0.0'
E         
E         - 3.0.0
E         + 3.1.0-alpha

tests\unit\test_version_and_health.py:10: AssertionError
____________________________ test_health_basic_ok _____________________________

    def test_health_basic_ok():
        with TestClient(app) as client:
            r = client.get("/api/v1/health")
            assert r.status_code == 200
>           _assert_health_payload(r.json())

tests\unit\test_version_and_health.py:30: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

payload = {'build_time': 'unknown', 'db': 'skipped', 'environment': 'development', 'event_bus': 'skipped', ...}

    def _assert_health_payload(payload: Dict[str, Any]):
        assert payload.get("status") == "healthy"
        assert payload.get("service") == "FBA-Bench Research Toolkit API"
>       assert payload.get("version") == "3.0.0"
E       AssertionError: assert '3.1.0-alpha' == '3.0.0'
E         
E         - 3.0.0
E         + 3.1.0-alpha

tests\unit\test_version_and_health.py:16: AssertionError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:28.694 | INFO | fba_bench_api.core.lifespan | [req=-] | Starting FBA-Bench API
2025-09-16 01:31:28.700 | INFO | api.dependencies | [req=-] | ConnectionManager: Starting background tasks.
2025-09-16 01:31:28.700 | INFO | fba_bench_api.core.lifespan | [req=-] | Event bus + connection manager started
2025-09-16 01:31:28.700 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized.
2025-09-16 01:31:28.700 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
2025-09-16 01:31:28.700 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized - no external connection needed.
2025-09-16 01:31:28.700 | INFO | fba_bench_core.services.world_store | [req=-] | No latest in-memory state snapshot found.
2025-09-16 01:31:28.701 | INFO | fba_bench_core.services.world_store | [req=-] | No existing state found in persistence backend.
2025-09-16 01:31:28.701 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore started - subscribed to SetPriceCommand, InventoryUpdate, TickEvent events
2025-09-16 01:31:28.701 | INFO | fba_bench_core.services.world_store | [req=-] | Global WorldStore instance has been set.
2025-09-16 01:31:28.701 | INFO | fba_bench_api.core.lifespan | [req=-] | WorldStore started and registered as global instance
2025-09-16 01:31:28.701 | INFO | fba_bench_core.services.supply_chain_service | [req=-] | SupplyChainService started and subscribed to PlaceOrderCommand and TickEvent.
2025-09-16 01:31:28.701 | INFO | fba_bench_api.core.lifespan | [req=-] | SupplyChainService started
2025-09-16 01:31:28.701 | INFO | agent_runners.agent_manager | [req=-] | AgentManager initialized with unified agent system.
2025-09-16 01:31:28.701 | INFO | agent_runners.agent_manager | [req=-] | AgentManager for 0 agents starting.
2025-09-16 01:31:28.701 | INFO | agent_runners.agent_manager | [req=-] | AgentManager subscribed to core events (decision cycle, command ack, skills pipeline).
2025-09-16 01:31:28.701 | INFO | agent_runners.agent_manager | [req=-] | AgentManager started. 0 agents active.
2025-09-16 01:31:28.701 | INFO | fba_bench_api.core.lifespan | [req=-] | AgentManager started
2025-09-16 01:31:28.701 | INFO | fba_bench_api.core.lifespan | [req=-] | CostTrackingService started and injected into SimulationOrchestrator
2025-09-16 01:31:28.704 | INFO | agent_runners.agent_manager | [req=-] | AgentManager stopping.
2025-09-16 01:31:28.704 | INFO | agent_runners.agent_manager | [req=-] | AgentManager unsubscribed from core events.
2025-09-16 01:31:28.704 | INFO | agent_runners.agent_manager | [req=-] | AgentManager stopped.
2025-09-16 01:31:28.705 | INFO | fba_bench_core.services.supply_chain_service | [req=-] | SupplyChainService stopped.
2025-09-16 01:31:28.705 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend shut down.
2025-09-16 01:31:28.705 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore stopped
2025-09-16 01:31:28.705 | INFO | api.dependencies | [req=-] | ConnectionManager: Stopping background tasks and closing connections.
2025-09-16 01:31:28.705 | INFO | api.dependencies | [req=-] | ConnectionManager: Heartbeat task cancelled.
2025-09-16 01:31:28.705 | INFO | api.dependencies | [req=-] | ConnectionManager: All connections closed.
2025-09-16 01:31:28.705 | INFO | fba_bench_api.core.lifespan | [req=-] | FBA-Bench API stopped
------------------------------ Captured log call ------------------------------
INFO     fba_bench_api.core.lifespan:lifespan.py:27 Starting FBA-Bench API
INFO     api.dependencies:dependencies.py:64 ConnectionManager: Starting background tasks.
INFO     fba_bench_api.core.lifespan:lifespan.py:51 Event bus + connection manager started
INFO     fba_bench_core.services.world_store:world_store.py:90 InMemoryStorageBackend initialized.
INFO     fba_bench_core.services.world_store:world_store.py:353 WorldStore initialized - ready for multi-agent command processing
INFO     fba_bench_core.services.world_store:world_store.py:93 InMemoryStorageBackend initialized - no external connection needed.
INFO     fba_bench_core.services.world_store:world_store.py:117 No latest in-memory state snapshot found.
INFO     fba_bench_core.services.world_store:world_store.py:372 No existing state found in persistence backend.
INFO     fba_bench_core.services.world_store:world_store.py:374 WorldStore started - subscribed to SetPriceCommand, InventoryUpdate, TickEvent events
INFO     fba_bench_core.services.world_store:world_store.py:919 Global WorldStore instance has been set.
INFO     fba_bench_api.core.lifespan:lifespan.py:82 WorldStore started and registered as global instance
INFO     fba_bench_core.services.supply_chain_service:supply_chain_service.py:86 SupplyChainService started and subscribed to PlaceOrderCommand and TickEvent.
INFO     fba_bench_api.core.lifespan:lifespan.py:87 SupplyChainService started
INFO     agent_runners.agent_manager:agent_manager.py:310 AgentManager initialized with unified agent system.
INFO     agent_runners.agent_manager:agent_manager.py:314 AgentManager for 0 agents starting.
INFO     agent_runners.agent_manager:agent_manager.py:334 AgentManager subscribed to core events (decision cycle, command ack, skills pipeline).
INFO     agent_runners.agent_manager:agent_manager.py:347 AgentManager started. 0 agents active.
INFO     fba_bench_api.core.lifespan:lifespan.py:97 AgentManager started
INFO     fba_bench_api.core.lifespan:lifespan.py:108 CostTrackingService started and injected into SimulationOrchestrator
INFO     agent_runners.agent_manager:agent_manager.py:353 AgentManager stopping.
INFO     agent_runners.agent_manager:agent_manager.py:367 AgentManager unsubscribed from core events.
INFO     agent_runners.agent_manager:agent_manager.py:383 AgentManager stopped.
INFO     fba_bench_core.services.supply_chain_service:supply_chain_service.py:91 SupplyChainService stopped.
INFO     fba_bench_core.services.world_store:world_store.py:96 InMemoryStorageBackend shut down.
INFO     fba_bench_core.services.world_store:world_store.py:382 WorldStore stopped
INFO     api.dependencies:dependencies.py:70 ConnectionManager: Stopping background tasks and closing connections.
INFO     api.dependencies:dependencies.py:76 ConnectionManager: Heartbeat task cancelled.
INFO     api.dependencies:dependencies.py:89 ConnectionManager: All connections closed.
INFO     fba_bench_api.core.lifespan:lifespan.py:145 FBA-Bench API stopped
_________________ test_health_resilient_without_optional_deps _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x0000016F9751F6B0>

    def test_health_resilient_without_optional_deps(monkeypatch):
        # Ensure flags are off and URLs missing
        monkeypatch.delenv("CHECK_REDIS", raising=False)
        monkeypatch.delenv("CHECK_DB", raising=False)
        monkeypatch.delenv("REDIS_URL", raising=False)
        monkeypatch.delenv("FBA_BENCH_REDIS_URL", raising=False)
        monkeypatch.delenv("FBA_REDIS_URL", raising=False)
        monkeypatch.delenv("DATABASE_URL", raising=False)
        monkeypatch.delenv("FBA_BENCH_DB_URL", raising=False)
    
        with TestClient(app) as client:
            r = client.get("/api/v1/health")
            assert r.status_code == 200
>           _assert_health_payload(r.json())

tests\unit\test_version_and_health.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

payload = {'build_time': 'unknown', 'db': 'skipped', 'environment': 'development', 'event_bus': 'skipped', ...}

    def _assert_health_payload(payload: Dict[str, Any]):
        assert payload.get("status") == "healthy"
        assert payload.get("service") == "FBA-Bench Research Toolkit API"
>       assert payload.get("version") == "3.0.0"
E       AssertionError: assert '3.1.0-alpha' == '3.0.0'
E         
E         - 3.0.0
E         + 3.1.0-alpha

tests\unit\test_version_and_health.py:16: AssertionError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:28.714 | INFO | fba_bench_api.core.lifespan | [req=-] | Starting FBA-Bench API
2025-09-16 01:31:28.718 | INFO | api.dependencies | [req=-] | ConnectionManager: Starting background tasks.
2025-09-16 01:31:28.718 | INFO | fba_bench_api.core.lifespan | [req=-] | Event bus + connection manager started
2025-09-16 01:31:28.718 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized.
2025-09-16 01:31:28.718 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
2025-09-16 01:31:28.718 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized - no external connection needed.
2025-09-16 01:31:28.718 | INFO | fba_bench_core.services.world_store | [req=-] | No latest in-memory state snapshot found.
2025-09-16 01:31:28.718 | INFO | fba_bench_core.services.world_store | [req=-] | No existing state found in persistence backend.
2025-09-16 01:31:28.718 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore started - subscribed to SetPriceCommand, InventoryUpdate, TickEvent events
2025-09-16 01:31:28.718 | INFO | fba_bench_core.services.world_store | [req=-] | Global WorldStore instance has been set.
2025-09-16 01:31:28.719 | INFO | fba_bench_api.core.lifespan | [req=-] | WorldStore started and registered as global instance
2025-09-16 01:31:28.719 | INFO | fba_bench_core.services.supply_chain_service | [req=-] | SupplyChainService started and subscribed to PlaceOrderCommand and TickEvent.
2025-09-16 01:31:28.719 | INFO | fba_bench_api.core.lifespan | [req=-] | SupplyChainService started
2025-09-16 01:31:28.719 | INFO | agent_runners.agent_manager | [req=-] | AgentManager for 0 agents starting.
2025-09-16 01:31:28.719 | INFO | agent_runners.agent_manager | [req=-] | AgentManager subscribed to core events (decision cycle, command ack, skills pipeline).
2025-09-16 01:31:28.719 | INFO | agent_runners.agent_manager | [req=-] | AgentManager started. 0 agents active.
2025-09-16 01:31:28.719 | INFO | fba_bench_api.core.lifespan | [req=-] | AgentManager started
2025-09-16 01:31:28.719 | INFO | fba_bench_api.core.lifespan | [req=-] | CostTrackingService started and injected into SimulationOrchestrator
2025-09-16 01:31:28.722 | INFO | agent_runners.agent_manager | [req=-] | AgentManager stopping.
2025-09-16 01:31:28.722 | INFO | agent_runners.agent_manager | [req=-] | AgentManager unsubscribed from core events.
2025-09-16 01:31:28.722 | INFO | agent_runners.agent_manager | [req=-] | AgentManager stopped.
2025-09-16 01:31:28.723 | INFO | fba_bench_core.services.supply_chain_service | [req=-] | SupplyChainService stopped.
2025-09-16 01:31:28.723 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend shut down.
2025-09-16 01:31:28.723 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore stopped
2025-09-16 01:31:28.723 | INFO | api.dependencies | [req=-] | ConnectionManager: Stopping background tasks and closing connections.
2025-09-16 01:31:28.723 | INFO | api.dependencies | [req=-] | ConnectionManager: Heartbeat task cancelled.
2025-09-16 01:31:28.723 | INFO | api.dependencies | [req=-] | ConnectionManager: All connections closed.
2025-09-16 01:31:28.723 | INFO | fba_bench_api.core.lifespan | [req=-] | FBA-Bench API stopped
------------------------------ Captured log call ------------------------------
INFO     fba_bench_api.core.lifespan:lifespan.py:27 Starting FBA-Bench API
INFO     api.dependencies:dependencies.py:64 ConnectionManager: Starting background tasks.
INFO     fba_bench_api.core.lifespan:lifespan.py:51 Event bus + connection manager started
INFO     fba_bench_core.services.world_store:world_store.py:90 InMemoryStorageBackend initialized.
INFO     fba_bench_core.services.world_store:world_store.py:353 WorldStore initialized - ready for multi-agent command processing
INFO     fba_bench_core.services.world_store:world_store.py:93 InMemoryStorageBackend initialized - no external connection needed.
INFO     fba_bench_core.services.world_store:world_store.py:117 No latest in-memory state snapshot found.
INFO     fba_bench_core.services.world_store:world_store.py:372 No existing state found in persistence backend.
INFO     fba_bench_core.services.world_store:world_store.py:374 WorldStore started - subscribed to SetPriceCommand, InventoryUpdate, TickEvent events
INFO     fba_bench_core.services.world_store:world_store.py:919 Global WorldStore instance has been set.
INFO     fba_bench_api.core.lifespan:lifespan.py:82 WorldStore started and registered as global instance
INFO     fba_bench_core.services.supply_chain_service:supply_chain_service.py:86 SupplyChainService started and subscribed to PlaceOrderCommand and TickEvent.
INFO     fba_bench_api.core.lifespan:lifespan.py:87 SupplyChainService started
INFO     agent_runners.agent_manager:agent_manager.py:314 AgentManager for 0 agents starting.
INFO     agent_runners.agent_manager:agent_manager.py:334 AgentManager subscribed to core events (decision cycle, command ack, skills pipeline).
INFO     agent_runners.agent_manager:agent_manager.py:347 AgentManager started. 0 agents active.
INFO     fba_bench_api.core.lifespan:lifespan.py:97 AgentManager started
INFO     fba_bench_api.core.lifespan:lifespan.py:108 CostTrackingService started and injected into SimulationOrchestrator
INFO     agent_runners.agent_manager:agent_manager.py:353 AgentManager stopping.
INFO     agent_runners.agent_manager:agent_manager.py:367 AgentManager unsubscribed from core events.
INFO     agent_runners.agent_manager:agent_manager.py:383 AgentManager stopped.
INFO     fba_bench_core.services.supply_chain_service:supply_chain_service.py:91 SupplyChainService stopped.
INFO     fba_bench_core.services.world_store:world_store.py:96 InMemoryStorageBackend shut down.
INFO     fba_bench_core.services.world_store:world_store.py:382 WorldStore stopped
INFO     api.dependencies:dependencies.py:70 ConnectionManager: Stopping background tasks and closing connections.
INFO     api.dependencies:dependencies.py:76 ConnectionManager: Heartbeat task cancelled.
INFO     api.dependencies:dependencies.py:89 ConnectionManager: All connections closed.
INFO     fba_bench_api.core.lifespan:lifespan.py:145 FBA-Bench API stopped
-------------------------- Captured stdout teardown ---------------------------
2025-09-16 01:31:28.811 | ERROR | asyncio | [req=-] | Task was destroyed but it is pending!
task: <Task pending name='InMemoryEventBusRunner' coro=<InMemoryEventBus._runner() running at C:\Users\admin\Downloads\fba\fba_events\bus.py:309> wait_for=<Future pending cb=[Task.task_wakeup()]>>
2025-09-16 01:31:28.811 | ERROR | fba_events.bus | [req=-] | Unhandled exception in InMemoryEventBus runner: Event loop is closed
Traceback (most recent call last):
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 158, in get
    await getter
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 309, in _runner
    event, event_type, ts = await self._queue.get()
                            ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 160, in get
    getter.cancel()  # Just in case getter is not done yet.
    ^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 799, in call_soon
    self._check_closed()
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 545, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
_________ test_health_resilient_with_checks_enabled_but_missing_urls __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x0000016F8F588470>

    def test_health_resilient_with_checks_enabled_but_missing_urls(monkeypatch):
        # Enable checks but omit URLs  endpoint must not raise
        monkeypatch.setenv("CHECK_REDIS", "1")
        monkeypatch.setenv("CHECK_DB", "1")
        monkeypatch.delenv("REDIS_URL", raising=False)
        monkeypatch.delenv("FBA_BENCH_REDIS_URL", raising=False)
        monkeypatch.delenv("FBA_REDIS_URL", raising=False)
        monkeypatch.delenv("DATABASE_URL", raising=False)
        monkeypatch.delenv("FBA_BENCH_DB_URL", raising=False)
    
        with TestClient(app) as client:
            r = client.get("/api/v1/health")
            assert r.status_code == 200
            data = r.json()
>           _assert_health_payload(data)

tests\unit\test_version_and_health.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

payload = {'build_time': 'unknown', 'db': 'skipped', 'environment': 'development', 'event_bus': 'skipped', ...}

    def _assert_health_payload(payload: Dict[str, Any]):
        assert payload.get("status") == "healthy"
        assert payload.get("service") == "FBA-Bench Research Toolkit API"
>       assert payload.get("version") == "3.0.0"
E       AssertionError: assert '3.1.0-alpha' == '3.0.0'
E         
E         - 3.0.0
E         + 3.1.0-alpha

tests\unit\test_version_and_health.py:16: AssertionError
---------------------------- Captured stdout call -----------------------------
2025-09-16 01:31:28.818 | INFO | fba_bench_api.core.lifespan | [req=-] | Starting FBA-Bench API
2025-09-16 01:31:28.821 | INFO | api.dependencies | [req=-] | ConnectionManager: Starting background tasks.
2025-09-16 01:31:28.821 | INFO | fba_bench_api.core.lifespan | [req=-] | Event bus + connection manager started
2025-09-16 01:31:28.821 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized.
2025-09-16 01:31:28.822 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
2025-09-16 01:31:28.822 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend initialized - no external connection needed.
2025-09-16 01:31:28.822 | INFO | fba_bench_core.services.world_store | [req=-] | No latest in-memory state snapshot found.
2025-09-16 01:31:28.822 | INFO | fba_bench_core.services.world_store | [req=-] | No existing state found in persistence backend.
2025-09-16 01:31:28.822 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore started - subscribed to SetPriceCommand, InventoryUpdate, TickEvent events
2025-09-16 01:31:28.822 | INFO | fba_bench_core.services.world_store | [req=-] | Global WorldStore instance has been set.
2025-09-16 01:31:28.822 | INFO | fba_bench_api.core.lifespan | [req=-] | WorldStore started and registered as global instance
2025-09-16 01:31:28.822 | INFO | fba_bench_core.services.supply_chain_service | [req=-] | SupplyChainService started and subscribed to PlaceOrderCommand and TickEvent.
2025-09-16 01:31:28.822 | INFO | fba_bench_api.core.lifespan | [req=-] | SupplyChainService started
2025-09-16 01:31:28.822 | INFO | agent_runners.agent_manager | [req=-] | AgentManager for 0 agents starting.
2025-09-16 01:31:28.822 | INFO | agent_runners.agent_manager | [req=-] | AgentManager subscribed to core events (decision cycle, command ack, skills pipeline).
2025-09-16 01:31:28.822 | INFO | agent_runners.agent_manager | [req=-] | AgentManager started. 0 agents active.
2025-09-16 01:31:28.822 | INFO | fba_bench_api.core.lifespan | [req=-] | AgentManager started
2025-09-16 01:31:28.822 | INFO | fba_bench_api.core.lifespan | [req=-] | CostTrackingService started and injected into SimulationOrchestrator
2025-09-16 01:31:28.825 | INFO | agent_runners.agent_manager | [req=-] | AgentManager stopping.
2025-09-16 01:31:28.825 | INFO | agent_runners.agent_manager | [req=-] | AgentManager unsubscribed from core events.
2025-09-16 01:31:28.825 | INFO | agent_runners.agent_manager | [req=-] | AgentManager stopped.
2025-09-16 01:31:28.825 | INFO | fba_bench_core.services.supply_chain_service | [req=-] | SupplyChainService stopped.
2025-09-16 01:31:28.825 | INFO | fba_bench_core.services.world_store | [req=-] | InMemoryStorageBackend shut down.
2025-09-16 01:31:28.826 | INFO | fba_bench_core.services.world_store | [req=-] | WorldStore stopped
2025-09-16 01:31:28.826 | INFO | api.dependencies | [req=-] | ConnectionManager: Stopping background tasks and closing connections.
2025-09-16 01:31:28.826 | INFO | api.dependencies | [req=-] | ConnectionManager: Heartbeat task cancelled.
2025-09-16 01:31:28.826 | INFO | api.dependencies | [req=-] | ConnectionManager: All connections closed.
2025-09-16 01:31:28.826 | INFO | fba_bench_api.core.lifespan | [req=-] | FBA-Bench API stopped
------------------------------ Captured log call ------------------------------
INFO     fba_bench_api.core.lifespan:lifespan.py:27 Starting FBA-Bench API
INFO     api.dependencies:dependencies.py:64 ConnectionManager: Starting background tasks.
INFO     fba_bench_api.core.lifespan:lifespan.py:51 Event bus + connection manager started
INFO     fba_bench_core.services.world_store:world_store.py:90 InMemoryStorageBackend initialized.
INFO     fba_bench_core.services.world_store:world_store.py:353 WorldStore initialized - ready for multi-agent command processing
INFO     fba_bench_core.services.world_store:world_store.py:93 InMemoryStorageBackend initialized - no external connection needed.
INFO     fba_bench_core.services.world_store:world_store.py:117 No latest in-memory state snapshot found.
INFO     fba_bench_core.services.world_store:world_store.py:372 No existing state found in persistence backend.
INFO     fba_bench_core.services.world_store:world_store.py:374 WorldStore started - subscribed to SetPriceCommand, InventoryUpdate, TickEvent events
INFO     fba_bench_core.services.world_store:world_store.py:919 Global WorldStore instance has been set.
INFO     fba_bench_api.core.lifespan:lifespan.py:82 WorldStore started and registered as global instance
INFO     fba_bench_core.services.supply_chain_service:supply_chain_service.py:86 SupplyChainService started and subscribed to PlaceOrderCommand and TickEvent.
INFO     fba_bench_api.core.lifespan:lifespan.py:87 SupplyChainService started
INFO     agent_runners.agent_manager:agent_manager.py:314 AgentManager for 0 agents starting.
INFO     agent_runners.agent_manager:agent_manager.py:334 AgentManager subscribed to core events (decision cycle, command ack, skills pipeline).
INFO     agent_runners.agent_manager:agent_manager.py:347 AgentManager started. 0 agents active.
INFO     fba_bench_api.core.lifespan:lifespan.py:97 AgentManager started
INFO     fba_bench_api.core.lifespan:lifespan.py:108 CostTrackingService started and injected into SimulationOrchestrator
INFO     agent_runners.agent_manager:agent_manager.py:353 AgentManager stopping.
INFO     agent_runners.agent_manager:agent_manager.py:367 AgentManager unsubscribed from core events.
INFO     agent_runners.agent_manager:agent_manager.py:383 AgentManager stopped.
INFO     fba_bench_core.services.supply_chain_service:supply_chain_service.py:91 SupplyChainService stopped.
INFO     fba_bench_core.services.world_store:world_store.py:96 InMemoryStorageBackend shut down.
INFO     fba_bench_core.services.world_store:world_store.py:382 WorldStore stopped
INFO     api.dependencies:dependencies.py:70 ConnectionManager: Stopping background tasks and closing connections.
INFO     api.dependencies:dependencies.py:76 ConnectionManager: Heartbeat task cancelled.
INFO     api.dependencies:dependencies.py:89 ConnectionManager: All connections closed.
INFO     fba_bench_api.core.lifespan:lifespan.py:145 FBA-Bench API stopped
_____________ test_validate_fba_decision_strict_success_from_dict _____________

    def test_validate_fba_decision_strict_success_from_dict():
>       ok, instance, errors = validate_output(FbaDecision, _mk_fba_decision_payload(), strict=True)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\validation\test_llm_output_validation.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\fba_bench_core\core\llm_validation.py:224: in validate_output
    variant = _build_model_variant(model, strict=strict)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src\fba_bench_core\core\llm_validation.py:121: in _build_model_variant
    return create_model(  # type: ignore[call-arg]
.venv-new\Lib\site-packages\pydantic\main.py:1763: in create_model
    return meta(
.venv-new\Lib\site-packages\pydantic\_internal\_model_construction.py:110: in __new__
    config_wrapper = ConfigWrapper.for_model(bases, namespace, kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'pydantic._internal._config.ConfigWrapper'>
bases = (<class 'fba_bench_core.core.llm_outputs.FbaDecision'>,)
namespace = {'__annotations__': {'model_config': {'extra': 'forbid', 'strict': True}}, '__module__': 'fba_bench_core.core.llm_validation'}
kwargs = {}

    @classmethod
    def for_model(cls, bases: tuple[type[Any], ...], namespace: dict[str, Any], kwargs: dict[str, Any]) -> Self:
        """Build a new `ConfigWrapper` instance for a `BaseModel`.
    
        The config wrapper built based on (in descending order of priority):
        - options from `kwargs`
        - options from the `namespace`
        - options from the base classes (`bases`)
    
        Args:
            bases: A tuple of base classes.
            namespace: The namespace of the class being created.
            kwargs: The kwargs passed to the class being created.
    
        Returns:
            A `ConfigWrapper` instance for `BaseModel`.
        """
        config_new = ConfigDict()
        for base in bases:
            config = getattr(base, 'model_config', None)
            if config:
                config_new.update(config.copy())
    
        config_class_from_namespace = namespace.get('Config')
        config_dict_from_namespace = namespace.get('model_config')
    
        raw_annotations = namespace.get('__annotations__', {})
        if raw_annotations.get('model_config') and config_dict_from_namespace is None:
>           raise PydanticUserError(
                '`model_config` cannot be used as a model field name. Use `model_config` for model configuration.',
                code='model-config-invalid-field-name',
            )
E           pydantic.errors.PydanticUserError: `model_config` cannot be used as a model field name. Use `model_config` for model configuration.
E           
E           For further information visit https://errors.pydantic.dev/2.11/u/model-config-invalid-field-name

.venv-new\Lib\site-packages\pydantic\_internal\_config.py:128: PydanticUserError
_________ test_validate_fba_decision_strict_success_from_json_string __________

    def test_validate_fba_decision_strict_success_from_json_string():
        payload = json.dumps(_mk_fba_decision_payload())
>       ok, instance, errors = validate_output(FbaDecision, payload, strict=True)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\validation\test_llm_output_validation.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\fba_bench_core\core\llm_validation.py:224: in validate_output
    variant = _build_model_variant(model, strict=strict)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src\fba_bench_core\core\llm_validation.py:121: in _build_model_variant
    return create_model(  # type: ignore[call-arg]
.venv-new\Lib\site-packages\pydantic\main.py:1763: in create_model
    return meta(
.venv-new\Lib\site-packages\pydantic\_internal\_model_construction.py:110: in __new__
    config_wrapper = ConfigWrapper.for_model(bases, namespace, kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'pydantic._internal._config.ConfigWrapper'>
bases = (<class 'fba_bench_core.core.llm_outputs.FbaDecision'>,)
namespace = {'__annotations__': {'model_config': {'extra': 'forbid', 'strict': True}}, '__module__': 'fba_bench_core.core.llm_validation'}
kwargs = {}

    @classmethod
    def for_model(cls, bases: tuple[type[Any], ...], namespace: dict[str, Any], kwargs: dict[str, Any]) -> Self:
        """Build a new `ConfigWrapper` instance for a `BaseModel`.
    
        The config wrapper built based on (in descending order of priority):
        - options from `kwargs`
        - options from the `namespace`
        - options from the base classes (`bases`)
    
        Args:
            bases: A tuple of base classes.
            namespace: The namespace of the class being created.
            kwargs: The kwargs passed to the class being created.
    
        Returns:
            A `ConfigWrapper` instance for `BaseModel`.
        """
        config_new = ConfigDict()
        for base in bases:
            config = getattr(base, 'model_config', None)
            if config:
                config_new.update(config.copy())
    
        config_class_from_namespace = namespace.get('Config')
        config_dict_from_namespace = namespace.get('model_config')
    
        raw_annotations = namespace.get('__annotations__', {})
        if raw_annotations.get('model_config') and config_dict_from_namespace is None:
>           raise PydanticUserError(
                '`model_config` cannot be used as a model field name. Use `model_config` for model configuration.',
                code='model-config-invalid-field-name',
            )
E           pydantic.errors.PydanticUserError: `model_config` cannot be used as a model field name. Use `model_config` for model configuration.
E           
E           For further information visit https://errors.pydantic.dev/2.11/u/model-config-invalid-field-name

.venv-new\Lib\site-packages\pydantic\_internal\_config.py:128: PydanticUserError
______________________ test_validate_task_plan_positive _______________________

    def test_validate_task_plan_positive():
        payload = {
            "objective": "Increase market share",
            "steps": ["Analyze competitors", "Lower price", "Monitor outcome"],
            "constraints": ["Budget <= 1000"],
            "metadata": {"priority": "high"},
        }
>       ok, instance, errors = validate_output(TaskPlan, payload, strict=True)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\validation\test_llm_output_validation.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\fba_bench_core\core\llm_validation.py:224: in validate_output
    variant = _build_model_variant(model, strict=strict)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src\fba_bench_core\core\llm_validation.py:121: in _build_model_variant
    return create_model(  # type: ignore[call-arg]
.venv-new\Lib\site-packages\pydantic\main.py:1763: in create_model
    return meta(
.venv-new\Lib\site-packages\pydantic\_internal\_model_construction.py:110: in __new__
    config_wrapper = ConfigWrapper.for_model(bases, namespace, kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'pydantic._internal._config.ConfigWrapper'>
bases = (<class 'fba_bench_core.core.llm_outputs.TaskPlan'>,)
namespace = {'__annotations__': {'model_config': {'extra': 'forbid', 'strict': True}}, '__module__': 'fba_bench_core.core.llm_validation'}
kwargs = {}

    @classmethod
    def for_model(cls, bases: tuple[type[Any], ...], namespace: dict[str, Any], kwargs: dict[str, Any]) -> Self:
        """Build a new `ConfigWrapper` instance for a `BaseModel`.
    
        The config wrapper built based on (in descending order of priority):
        - options from `kwargs`
        - options from the `namespace`
        - options from the base classes (`bases`)
    
        Args:
            bases: A tuple of base classes.
            namespace: The namespace of the class being created.
            kwargs: The kwargs passed to the class being created.
    
        Returns:
            A `ConfigWrapper` instance for `BaseModel`.
        """
        config_new = ConfigDict()
        for base in bases:
            config = getattr(base, 'model_config', None)
            if config:
                config_new.update(config.copy())
    
        config_class_from_namespace = namespace.get('Config')
        config_dict_from_namespace = namespace.get('model_config')
    
        raw_annotations = namespace.get('__annotations__', {})
        if raw_annotations.get('model_config') and config_dict_from_namespace is None:
>           raise PydanticUserError(
                '`model_config` cannot be used as a model field name. Use `model_config` for model configuration.',
                code='model-config-invalid-field-name',
            )
E           pydantic.errors.PydanticUserError: `model_config` cannot be used as a model field name. Use `model_config` for model configuration.
E           
E           For further information visit https://errors.pydantic.dev/2.11/u/model-config-invalid-field-name

.venv-new\Lib\site-packages\pydantic\_internal\_config.py:128: PydanticUserError
______________________ test_validate_tool_call_positive _______________________

    def test_validate_tool_call_positive():
        payload = {"tool_name": "set_price", "arguments": {"asin": "B07XAMPLE", "price": 23.1}}
>       ok, instance, errors = validate_output(ToolCall, payload, strict=True)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\validation\test_llm_output_validation.py:82: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\fba_bench_core\core\llm_validation.py:224: in validate_output
    variant = _build_model_variant(model, strict=strict)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src\fba_bench_core\core\llm_validation.py:121: in _build_model_variant
    return create_model(  # type: ignore[call-arg]
.venv-new\Lib\site-packages\pydantic\main.py:1763: in create_model
    return meta(
.venv-new\Lib\site-packages\pydantic\_internal\_model_construction.py:110: in __new__
    config_wrapper = ConfigWrapper.for_model(bases, namespace, kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'pydantic._internal._config.ConfigWrapper'>
bases = (<class 'fba_bench_core.core.llm_outputs.ToolCall'>,)
namespace = {'__annotations__': {'model_config': {'extra': 'forbid', 'strict': True}}, '__module__': 'fba_bench_core.core.llm_validation'}
kwargs = {}

    @classmethod
    def for_model(cls, bases: tuple[type[Any], ...], namespace: dict[str, Any], kwargs: dict[str, Any]) -> Self:
        """Build a new `ConfigWrapper` instance for a `BaseModel`.
    
        The config wrapper built based on (in descending order of priority):
        - options from `kwargs`
        - options from the `namespace`
        - options from the base classes (`bases`)
    
        Args:
            bases: A tuple of base classes.
            namespace: The namespace of the class being created.
            kwargs: The kwargs passed to the class being created.
    
        Returns:
            A `ConfigWrapper` instance for `BaseModel`.
        """
        config_new = ConfigDict()
        for base in bases:
            config = getattr(base, 'model_config', None)
            if config:
                config_new.update(config.copy())
    
        config_class_from_namespace = namespace.get('Config')
        config_dict_from_namespace = namespace.get('model_config')
    
        raw_annotations = namespace.get('__annotations__', {})
        if raw_annotations.get('model_config') and config_dict_from_namespace is None:
>           raise PydanticUserError(
                '`model_config` cannot be used as a model field name. Use `model_config` for model configuration.',
                code='model-config-invalid-field-name',
            )
E           pydantic.errors.PydanticUserError: `model_config` cannot be used as a model field name. Use `model_config` for model configuration.
E           
E           For further information visit https://errors.pydantic.dev/2.11/u/model-config-invalid-field-name

.venv-new\Lib\site-packages\pydantic\_internal\_config.py:128: PydanticUserError
____________________ test_validate_agent_response_positive ____________________

    def test_validate_agent_response_positive():
        payload = {
            "content": "Lowering price slightly to improve competitiveness.",
            "citations": ["https://example.com/market-report"],
            "tool_calls": [
                {"tool_name": "set_price", "arguments": {"asin": "B07XAMPLE", "price": 23.47}}
            ],
            "plan": {"objective": "Improve competitiveness", "steps": ["Analyze", "Adjust", "Observe"]},
        }
>       ok, instance, errors = validate_output(AgentResponse, payload, strict=True)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\validation\test_llm_output_validation.py:97: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\fba_bench_core\core\llm_validation.py:224: in validate_output
    variant = _build_model_variant(model, strict=strict)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src\fba_bench_core\core\llm_validation.py:121: in _build_model_variant
    return create_model(  # type: ignore[call-arg]
.venv-new\Lib\site-packages\pydantic\main.py:1763: in create_model
    return meta(
.venv-new\Lib\site-packages\pydantic\_internal\_model_construction.py:110: in __new__
    config_wrapper = ConfigWrapper.for_model(bases, namespace, kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'pydantic._internal._config.ConfigWrapper'>
bases = (<class 'fba_bench_core.core.llm_outputs.AgentResponse'>,)
namespace = {'__annotations__': {'model_config': {'extra': 'forbid', 'strict': True}}, '__module__': 'fba_bench_core.core.llm_validation'}
kwargs = {}

    @classmethod
    def for_model(cls, bases: tuple[type[Any], ...], namespace: dict[str, Any], kwargs: dict[str, Any]) -> Self:
        """Build a new `ConfigWrapper` instance for a `BaseModel`.
    
        The config wrapper built based on (in descending order of priority):
        - options from `kwargs`
        - options from the `namespace`
        - options from the base classes (`bases`)
    
        Args:
            bases: A tuple of base classes.
            namespace: The namespace of the class being created.
            kwargs: The kwargs passed to the class being created.
    
        Returns:
            A `ConfigWrapper` instance for `BaseModel`.
        """
        config_new = ConfigDict()
        for base in bases:
            config = getattr(base, 'model_config', None)
            if config:
                config_new.update(config.copy())
    
        config_class_from_namespace = namespace.get('Config')
        config_dict_from_namespace = namespace.get('model_config')
    
        raw_annotations = namespace.get('__annotations__', {})
        if raw_annotations.get('model_config') and config_dict_from_namespace is None:
>           raise PydanticUserError(
                '`model_config` cannot be used as a model field name. Use `model_config` for model configuration.',
                code='model-config-invalid-field-name',
            )
E           pydantic.errors.PydanticUserError: `model_config` cannot be used as a model field name. Use `model_config` for model configuration.
E           
E           For further information visit https://errors.pydantic.dev/2.11/u/model-config-invalid-field-name

.venv-new\Lib\site-packages\pydantic\_internal\_config.py:128: PydanticUserError
______________________ test_missing_required_field_fails ______________________

    def test_missing_required_field_fails():
        bad = {"pricing_decisions": [{"asin": "B07XAMPLE", "reasoning": "missing price"}]}
>       ok, instance, errors = validate_output(FbaDecision, bad, strict=True)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\validation\test_llm_output_validation.py:107: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\fba_bench_core\core\llm_validation.py:224: in validate_output
    variant = _build_model_variant(model, strict=strict)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src\fba_bench_core\core\llm_validation.py:121: in _build_model_variant
    return create_model(  # type: ignore[call-arg]
.venv-new\Lib\site-packages\pydantic\main.py:1763: in create_model
    return meta(
.venv-new\Lib\site-packages\pydantic\_internal\_model_construction.py:110: in __new__
    config_wrapper = ConfigWrapper.for_model(bases, namespace, kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'pydantic._internal._config.ConfigWrapper'>
bases = (<class 'fba_bench_core.core.llm_outputs.FbaDecision'>,)
namespace = {'__annotations__': {'model_config': {'extra': 'forbid', 'strict': True}}, '__module__': 'fba_bench_core.core.llm_validation'}
kwargs = {}

    @classmethod
    def for_model(cls, bases: tuple[type[Any], ...], namespace: dict[str, Any], kwargs: dict[str, Any]) -> Self:
        """Build a new `ConfigWrapper` instance for a `BaseModel`.
    
        The config wrapper built based on (in descending order of priority):
        - options from `kwargs`
        - options from the `namespace`
        - options from the base classes (`bases`)
    
        Args:
            bases: A tuple of base classes.
            namespace: The namespace of the class being created.
            kwargs: The kwargs passed to the class being created.
    
        Returns:
            A `ConfigWrapper` instance for `BaseModel`.
        """
        config_new = ConfigDict()
        for base in bases:
            config = getattr(base, 'model_config', None)
            if config:
                config_new.update(config.copy())
    
        config_class_from_namespace = namespace.get('Config')
        config_dict_from_namespace = namespace.get('model_config')
    
        raw_annotations = namespace.get('__annotations__', {})
        if raw_annotations.get('model_config') and config_dict_from_namespace is None:
>           raise PydanticUserError(
                '`model_config` cannot be used as a model field name. Use `model_config` for model configuration.',
                code='model-config-invalid-field-name',
            )
E           pydantic.errors.PydanticUserError: `model_config` cannot be used as a model field name. Use `model_config` for model configuration.
E           
E           For further information visit https://errors.pydantic.dev/2.11/u/model-config-invalid-field-name

.venv-new\Lib\site-packages\pydantic\_internal\_config.py:128: PydanticUserError
____________________ test_extra_fields_in_strict_mode_fail ____________________

    def test_extra_fields_in_strict_mode_fail():
        payload = _mk_fba_decision_payload(extra=True)
>       ok, instance, errors = validate_output(FbaDecision, payload, strict=True)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\validation\test_llm_output_validation.py:115: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\fba_bench_core\core\llm_validation.py:224: in validate_output
    variant = _build_model_variant(model, strict=strict)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src\fba_bench_core\core\llm_validation.py:121: in _build_model_variant
    return create_model(  # type: ignore[call-arg]
.venv-new\Lib\site-packages\pydantic\main.py:1763: in create_model
    return meta(
.venv-new\Lib\site-packages\pydantic\_internal\_model_construction.py:110: in __new__
    config_wrapper = ConfigWrapper.for_model(bases, namespace, kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'pydantic._internal._config.ConfigWrapper'>
bases = (<class 'fba_bench_core.core.llm_outputs.FbaDecision'>,)
namespace = {'__annotations__': {'model_config': {'extra': 'forbid', 'strict': True}}, '__module__': 'fba_bench_core.core.llm_validation'}
kwargs = {}

    @classmethod
    def for_model(cls, bases: tuple[type[Any], ...], namespace: dict[str, Any], kwargs: dict[str, Any]) -> Self:
        """Build a new `ConfigWrapper` instance for a `BaseModel`.
    
        The config wrapper built based on (in descending order of priority):
        - options from `kwargs`
        - options from the `namespace`
        - options from the base classes (`bases`)
    
        Args:
            bases: A tuple of base classes.
            namespace: The namespace of the class being created.
            kwargs: The kwargs passed to the class being created.
    
        Returns:
            A `ConfigWrapper` instance for `BaseModel`.
        """
        config_new = ConfigDict()
        for base in bases:
            config = getattr(base, 'model_config', None)
            if config:
                config_new.update(config.copy())
    
        config_class_from_namespace = namespace.get('Config')
        config_dict_from_namespace = namespace.get('model_config')
    
        raw_annotations = namespace.get('__annotations__', {})
        if raw_annotations.get('model_config') and config_dict_from_namespace is None:
>           raise PydanticUserError(
                '`model_config` cannot be used as a model field name. Use `model_config` for model configuration.',
                code='model-config-invalid-field-name',
            )
E           pydantic.errors.PydanticUserError: `model_config` cannot be used as a model field name. Use `model_config` for model configuration.
E           
E           For further information visit https://errors.pydantic.dev/2.11/u/model-config-invalid-field-name

.venv-new\Lib\site-packages\pydantic\_internal\_config.py:128: PydanticUserError
___________________ test_type_mismatch_steps_not_list_fails ___________________

    def test_type_mismatch_steps_not_list_fails():
        bad = {"objective": "Improve", "steps": "not a list"}
>       ok, instance, errors = validate_output(TaskPlan, bad, strict=True)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\validation\test_llm_output_validation.py:123: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\fba_bench_core\core\llm_validation.py:224: in validate_output
    variant = _build_model_variant(model, strict=strict)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src\fba_bench_core\core\llm_validation.py:121: in _build_model_variant
    return create_model(  # type: ignore[call-arg]
.venv-new\Lib\site-packages\pydantic\main.py:1763: in create_model
    return meta(
.venv-new\Lib\site-packages\pydantic\_internal\_model_construction.py:110: in __new__
    config_wrapper = ConfigWrapper.for_model(bases, namespace, kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'pydantic._internal._config.ConfigWrapper'>
bases = (<class 'fba_bench_core.core.llm_outputs.TaskPlan'>,)
namespace = {'__annotations__': {'model_config': {'extra': 'forbid', 'strict': True}}, '__module__': 'fba_bench_core.core.llm_validation'}
kwargs = {}

    @classmethod
    def for_model(cls, bases: tuple[type[Any], ...], namespace: dict[str, Any], kwargs: dict[str, Any]) -> Self:
        """Build a new `ConfigWrapper` instance for a `BaseModel`.
    
        The config wrapper built based on (in descending order of priority):
        - options from `kwargs`
        - options from the `namespace`
        - options from the base classes (`bases`)
    
        Args:
            bases: A tuple of base classes.
            namespace: The namespace of the class being created.
            kwargs: The kwargs passed to the class being created.
    
        Returns:
            A `ConfigWrapper` instance for `BaseModel`.
        """
        config_new = ConfigDict()
        for base in bases:
            config = getattr(base, 'model_config', None)
            if config:
                config_new.update(config.copy())
    
        config_class_from_namespace = namespace.get('Config')
        config_dict_from_namespace = namespace.get('model_config')
    
        raw_annotations = namespace.get('__annotations__', {})
        if raw_annotations.get('model_config') and config_dict_from_namespace is None:
>           raise PydanticUserError(
                '`model_config` cannot be used as a model field name. Use `model_config` for model configuration.',
                code='model-config-invalid-field-name',
            )
E           pydantic.errors.PydanticUserError: `model_config` cannot be used as a model field name. Use `model_config` for model configuration.
E           
E           For further information visit https://errors.pydantic.dev/2.11/u/model-config-invalid-field-name

.venv-new\Lib\site-packages\pydantic\_internal\_config.py:128: PydanticUserError
__________ test_non_strict_coercion_numeric_string_to_float_succeeds __________

    def test_non_strict_coercion_numeric_string_to_float_succeeds():
        payload = _mk_fba_decision_payload(price="23.47")
>       ok, instance, errors = validate_output(FbaDecision, payload, strict=False)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\validation\test_llm_output_validation.py:133: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\fba_bench_core\core\llm_validation.py:224: in validate_output
    variant = _build_model_variant(model, strict=strict)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src\fba_bench_core\core\llm_validation.py:121: in _build_model_variant
    return create_model(  # type: ignore[call-arg]
.venv-new\Lib\site-packages\pydantic\main.py:1763: in create_model
    return meta(
.venv-new\Lib\site-packages\pydantic\_internal\_model_construction.py:110: in __new__
    config_wrapper = ConfigWrapper.for_model(bases, namespace, kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'pydantic._internal._config.ConfigWrapper'>
bases = (<class 'fba_bench_core.core.llm_outputs.FbaDecision'>,)
namespace = {'__annotations__': {'model_config': {'extra': 'ignore', 'strict': False}}, '__module__': 'fba_bench_core.core.llm_validation'}
kwargs = {}

    @classmethod
    def for_model(cls, bases: tuple[type[Any], ...], namespace: dict[str, Any], kwargs: dict[str, Any]) -> Self:
        """Build a new `ConfigWrapper` instance for a `BaseModel`.
    
        The config wrapper built based on (in descending order of priority):
        - options from `kwargs`
        - options from the `namespace`
        - options from the base classes (`bases`)
    
        Args:
            bases: A tuple of base classes.
            namespace: The namespace of the class being created.
            kwargs: The kwargs passed to the class being created.
    
        Returns:
            A `ConfigWrapper` instance for `BaseModel`.
        """
        config_new = ConfigDict()
        for base in bases:
            config = getattr(base, 'model_config', None)
            if config:
                config_new.update(config.copy())
    
        config_class_from_namespace = namespace.get('Config')
        config_dict_from_namespace = namespace.get('model_config')
    
        raw_annotations = namespace.get('__annotations__', {})
        if raw_annotations.get('model_config') and config_dict_from_namespace is None:
>           raise PydanticUserError(
                '`model_config` cannot be used as a model field name. Use `model_config` for model configuration.',
                code='model-config-invalid-field-name',
            )
E           pydantic.errors.PydanticUserError: `model_config` cannot be used as a model field name. Use `model_config` for model configuration.
E           
E           For further information visit https://errors.pydantic.dev/2.11/u/model-config-invalid-field-name

.venv-new\Lib\site-packages\pydantic\_internal\_config.py:128: PydanticUserError
___________________ test_non_strict_ignores_unknown_fields ____________________

    def test_non_strict_ignores_unknown_fields():
        payload = _mk_fba_decision_payload(extra=True)
>       ok, instance, errors = validate_output(FbaDecision, payload, strict=False)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\validation\test_llm_output_validation.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\fba_bench_core\core\llm_validation.py:224: in validate_output
    variant = _build_model_variant(model, strict=strict)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src\fba_bench_core\core\llm_validation.py:121: in _build_model_variant
    return create_model(  # type: ignore[call-arg]
.venv-new\Lib\site-packages\pydantic\main.py:1763: in create_model
    return meta(
.venv-new\Lib\site-packages\pydantic\_internal\_model_construction.py:110: in __new__
    config_wrapper = ConfigWrapper.for_model(bases, namespace, kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'pydantic._internal._config.ConfigWrapper'>
bases = (<class 'fba_bench_core.core.llm_outputs.FbaDecision'>,)
namespace = {'__annotations__': {'model_config': {'extra': 'ignore', 'strict': False}}, '__module__': 'fba_bench_core.core.llm_validation'}
kwargs = {}

    @classmethod
    def for_model(cls, bases: tuple[type[Any], ...], namespace: dict[str, Any], kwargs: dict[str, Any]) -> Self:
        """Build a new `ConfigWrapper` instance for a `BaseModel`.
    
        The config wrapper built based on (in descending order of priority):
        - options from `kwargs`
        - options from the `namespace`
        - options from the base classes (`bases`)
    
        Args:
            bases: A tuple of base classes.
            namespace: The namespace of the class being created.
            kwargs: The kwargs passed to the class being created.
    
        Returns:
            A `ConfigWrapper` instance for `BaseModel`.
        """
        config_new = ConfigDict()
        for base in bases:
            config = getattr(base, 'model_config', None)
            if config:
                config_new.update(config.copy())
    
        config_class_from_namespace = namespace.get('Config')
        config_dict_from_namespace = namespace.get('model_config')
    
        raw_annotations = namespace.get('__annotations__', {})
        if raw_annotations.get('model_config') and config_dict_from_namespace is None:
>           raise PydanticUserError(
                '`model_config` cannot be used as a model field name. Use `model_config` for model configuration.',
                code='model-config-invalid-field-name',
            )
E           pydantic.errors.PydanticUserError: `model_config` cannot be used as a model field name. Use `model_config` for model configuration.
E           
E           For further information visit https://errors.pydantic.dev/2.11/u/model-config-invalid-field-name

.venv-new\Lib\site-packages\pydantic\_internal\_config.py:128: PydanticUserError
___________________ test_validate_by_name_success_and_dump ____________________

    def test_validate_by_name_success_and_dump():
        payload = _mk_fba_decision_payload()
>       ok, data, errors = validate_by_name("fba_decision", payload, strict=True)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\validation\test_llm_output_validation.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\fba_bench_core\core\llm_validation.py:383: in validate_by_name
    ok, instance, errors = validate_output(model, payload, strict=strict)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src\fba_bench_core\core\llm_validation.py:224: in validate_output
    variant = _build_model_variant(model, strict=strict)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src\fba_bench_core\core\llm_validation.py:121: in _build_model_variant
    return create_model(  # type: ignore[call-arg]
.venv-new\Lib\site-packages\pydantic\main.py:1763: in create_model
    return meta(
.venv-new\Lib\site-packages\pydantic\_internal\_model_construction.py:110: in __new__
    config_wrapper = ConfigWrapper.for_model(bases, namespace, kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'pydantic._internal._config.ConfigWrapper'>
bases = (<class 'fba_bench_core.core.llm_outputs.FbaDecision'>,)
namespace = {'__annotations__': {'model_config': {'extra': 'forbid', 'strict': True}}, '__module__': 'fba_bench_core.core.llm_validation'}
kwargs = {}

    @classmethod
    def for_model(cls, bases: tuple[type[Any], ...], namespace: dict[str, Any], kwargs: dict[str, Any]) -> Self:
        """Build a new `ConfigWrapper` instance for a `BaseModel`.
    
        The config wrapper built based on (in descending order of priority):
        - options from `kwargs`
        - options from the `namespace`
        - options from the base classes (`bases`)
    
        Args:
            bases: A tuple of base classes.
            namespace: The namespace of the class being created.
            kwargs: The kwargs passed to the class being created.
    
        Returns:
            A `ConfigWrapper` instance for `BaseModel`.
        """
        config_new = ConfigDict()
        for base in bases:
            config = getattr(base, 'model_config', None)
            if config:
                config_new.update(config.copy())
    
        config_class_from_namespace = namespace.get('Config')
        config_dict_from_namespace = namespace.get('model_config')
    
        raw_annotations = namespace.get('__annotations__', {})
        if raw_annotations.get('model_config') and config_dict_from_namespace is None:
>           raise PydanticUserError(
                '`model_config` cannot be used as a model field name. Use `model_config` for model configuration.',
                code='model-config-invalid-field-name',
            )
E           pydantic.errors.PydanticUserError: `model_config` cannot be used as a model field name. Use `model_config` for model configuration.
E           
E           For further information visit https://errors.pydantic.dev/2.11/u/model-config-invalid-field-name

.venv-new\Lib\site-packages\pydantic\_internal\_config.py:128: PydanticUserError
============================== warnings summary ===============================
.venv-new\Lib\site-packages\_pytest\config\__init__.py:1290
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\config\__init__.py:1290: PytestAssertRewriteWarning: Module already imported so cannot be rewritten; anyio
    self._mark_plugins_for_rewrite(hook, disable_autoload)

.venv-new\Lib\site-packages\_pytest\config\__init__.py:812
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\config\__init__.py:812: PytestAssertRewriteWarning: Module already imported so cannot be rewritten; anyio
    self.import_plugin(arg, consider_entry_points=True)

.venv-new\Lib\site-packages\pydantic\_internal\_config.py:323: 39 warnings
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\pydantic\_internal\_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)

src\fba_bench_core\models\product.py:21
src\fba_bench_core\models\product.py:21
  C:\Users\admin\Downloads\fba\src\fba_bench_core\models\product.py:21: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    @validator('price', 'cost', pre=True)

.venv-new\Lib\site-packages\pydantic\fields.py:1062
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\pydantic\fields.py:1062: PydanticDeprecatedSince20: `min_items` is deprecated and will be removed, use `min_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    warn('`min_items` is deprecated and will be removed, use `min_length` instead', DeprecationWarning)

.venv-new\Lib\site-packages\pydantic\fields.py:1068
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\pydantic\fields.py:1068: PydanticDeprecatedSince20: `max_items` is deprecated and will be removed, use `max_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    warn('`max_items` is deprecated and will be removed, use `max_length` instead', DeprecationWarning)

.venv-new\Lib\site-packages\pydantic\_internal\_fields.py:198
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\pydantic\_internal\_fields.py:198: UserWarning: Field name "schema" in "ToolSpec" shadows an attribute in parent "BaseModel"
    warnings.warn(

.venv-new\Lib\site-packages\pydantic\_internal\_config.py:373
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\pydantic\_internal\_config.py:373: UserWarning: Valid config keys have changed in V2:
  * 'anystr_strip_whitespace' has been renamed to 'str_strip_whitespace'
    warnings.warn(message, UserWarning)

src\fba_bench_core\services\toolbox_schemas.py:67
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:67: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    _asin_validator = validator("asin", allow_reuse=True)(_validate_asin)

.venv-new\Lib\site-packages\pydantic\deprecated\class_validators.py:121
.venv-new\Lib\site-packages\pydantic\deprecated\class_validators.py:121
.venv-new\Lib\site-packages\pydantic\deprecated\class_validators.py:121
.venv-new\Lib\site-packages\pydantic\deprecated\class_validators.py:121
.venv-new\Lib\site-packages\pydantic\deprecated\class_validators.py:121
.venv-new\Lib\site-packages\pydantic\deprecated\class_validators.py:121
.venv-new\Lib\site-packages\pydantic\deprecated\class_validators.py:121
.venv-new\Lib\site-packages\pydantic\deprecated\class_validators.py:121
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\pydantic\deprecated\class_validators.py:121: PydanticDeprecatedSince20: `allow_reuse` is deprecated and will be ignored; it should no longer be necessary. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    warn(_ALLOW_REUSE_WARNING_MESSAGE, DeprecationWarning)

src\fba_bench_core\services\toolbox_schemas.py:79
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:79: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    _asin_validator = validator("asin", allow_reuse=True)(_validate_asin)

src\fba_bench_core\services\toolbox_schemas.py:81
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:81: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    @validator("price")

src\fba_bench_core\services\toolbox_schemas.py:87
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:87: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    @validator("inventory")

src\fba_bench_core\services\toolbox_schemas.py:91
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:91: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    @validator("bsr")

src\fba_bench_core\services\toolbox_schemas.py:99
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:99: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    @validator("conversion_rate")

src\fba_bench_core\services\toolbox_schemas.py:119
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:119: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    _asin_validator = validator("asin", allow_reuse=True)(_validate_asin)

src\fba_bench_core\services\toolbox_schemas.py:121
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:121: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    @validator("agent_id")

src\fba_bench_core\services\toolbox_schemas.py:127
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:127: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    @validator("new_price")

src\fba_bench_core\services\toolbox_schemas.py:142
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:142: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    _asin_validator = validator("asin", allow_reuse=True)(_validate_asin)

src\fba_bench_core\services\toolbox_schemas.py:143
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:143: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    _new_price_validator = validator("new_price", allow_reuse=True)(_validate_money_instance)

src\fba_bench_core\services\toolbox_schemas.py:157
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:157: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    _asin_validator = validator("asin", allow_reuse=True)(_validate_asin)

src\fba_bench_core\services\toolbox_schemas.py:158
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:158: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    _initial_price_validator = validator("initial_price", allow_reuse=True)(

src\fba_bench_core\services\toolbox_schemas.py:162
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:162: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    @validator("initial_price")

src\fba_bench_core\services\toolbox_schemas.py:168
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:168: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    @validator("initial_inventory")

src\fba_bench_core\services\toolbox_schemas.py:172
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:172: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    @validator("weight_oz")

src\fba_bench_core\services\toolbox_schemas.py:182
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:182: PydanticDeprecatedSince20: Pydantic V1 style `@root_validator` validators are deprecated. You should migrate to Pydantic V2 style `@model_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    @root_validator(skip_on_failure=True)

src\fba_bench_core\services\toolbox_schemas.py:204
  C:\Users\admin\Downloads\fba\src\fba_bench_core\services\toolbox_schemas.py:204: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    _asin_validator = validator("asin", allow_reuse=True)(_validate_asin)

fba_bench_api\api\routes\agents.py:195
  C:\Users\admin\Downloads\fba\fba_bench_api\api\routes\agents.py:195: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    @validator("name")

fba_bench_api\api\routes\agents.py:218
  C:\Users\admin\Downloads\fba\fba_bench_api\api\routes\agents.py:218: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    @validator("name")

fba_bench_api\core\models.py:170
  C:\Users\admin\Downloads\fba\fba_bench_api\core\models.py:170: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    @validator("updated_at", always=True)

tests\unit\test_eventbus_logging.py:11
  C:\Users\admin\Downloads\fba\tests\unit\test_eventbus_logging.py:11: DeprecationWarning: fba_events.compat is deprecated; import from fba_events.* instead
    from fba_events.compat import TickEvent  # type: ignore

tests/benchmarking/test_engine.py: 12 warnings
tests/unit/test_engine.py: 26 warnings
  <string>:6: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).

tests/benchmarking/test_engine.py::TestBenchmarkEngine::test_cleanup_completed_runs
tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_cleanup_completed_runs
tests/unit/test_engine.py::TestBenchmarkEngine::test_cleanup_completed_runs[30-1]
tests/unit/test_engine.py::TestBenchmarkEngine::test_cleanup_completed_runs[3650-2]
  C:\Users\admin\Downloads\fba\benchmarking\core\engine.py:1953: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    cutoff = _dt.utcnow().timestamp() - (max_age_days * 86400)

tests/benchmarking/test_engine.py::TestBenchmarkEngine::test_execute_scenario_failure
tests/benchmarking/test_engine.py::TestBenchmarkEngine::test_execute_scenario_success
tests/unit/test_engine.py::TestBenchmarkEngine::test_execute_scenario_with_different_statuses[completed-completed]
tests/unit/test_engine.py::TestBenchmarkEngine::test_execute_scenario_with_different_statuses[failed-failed]
tests/unit/test_engine.py::TestBenchmarkEngine::test_execute_scenario_with_different_statuses[timeout-timeout]
  C:\Users\admin\Downloads\fba\benchmarking\core\engine.py:1653: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    t0 = _dt.utcnow()

tests/benchmarking/test_engine.py::TestBenchmarkEngine::test_execute_scenario_failure
tests/unit/test_engine.py::TestBenchmarkEngine::test_execute_scenario_with_different_statuses[completed-completed]
tests/unit/test_engine.py::TestBenchmarkEngine::test_execute_scenario_with_different_statuses[failed-failed]
tests/unit/test_engine.py::TestBenchmarkEngine::test_execute_scenario_with_different_statuses[timeout-timeout]
  C:\Users\admin\Downloads\fba\benchmarking\core\engine.py:1701: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    "execution_time": (_dt.utcnow() - t0).total_seconds(),

tests/benchmarking/test_engine.py::TestBenchmarkEngine::test_execute_scenario_success
  C:\Users\admin\Downloads\fba\benchmarking\core\engine.py:1669: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    measured = (_dt.utcnow() - t0).total_seconds()

tests/benchmarking/test_engine.py::TestBenchmarkEngine::test_run_benchmark_success
tests/unit/test_engine.py::TestBenchmarkEngine::test_run_benchmark_success
tests/unit/test_engine.py::TestBenchmarkEngine::test_run_benchmark_with_timeout
tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_run_benchmark_with_retry
tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_run_benchmark_max_retries_exceeded
tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_run_benchmark_with_retry_disabled
tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_run_benchmark_with_max_duration
tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_run_benchmark_parallel_execution
  C:\Users\admin\Downloads\fba\benchmarking\core\engine.py:1499: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    start_ts = _dt.utcnow()

tests/benchmarking/test_engine.py::TestBenchmarkEngine::test_run_benchmark_success
tests/unit/test_engine.py::TestBenchmarkEngine::test_run_benchmark_success
tests/unit/test_engine.py::TestBenchmarkEngine::test_run_benchmark_with_timeout
tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_run_benchmark_with_retry
tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_run_benchmark_max_retries_exceeded
tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_run_benchmark_with_retry_disabled
tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_run_benchmark_with_max_duration
tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_run_benchmark_parallel_execution
  C:\Users\admin\Downloads\fba\benchmarking\core\engine.py:1598: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    run.end_time = _dt.utcnow()

tests/benchmarking/test_engine.py::TestBenchmarkEngine::test_stop_benchmark
tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_stop_benchmark
tests/unit/test_engine.py::TestBenchmarkEngine::test_stop_benchmark
  C:\Users\admin\Downloads\fba\benchmarking\core\engine.py:1945: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    run.end_time = _dt.utcnow()

tests/benchmarking/test_engine_unit.py::test_timeout_handling
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\pydantic\main.py:463: UserWarning: Pydantic serializer warnings:
    PydanticSerializationUnexpectedValue(Expected `int` - serialized value may not be as expected [input_value=0.05, input_type=float])
    return self.__pydantic_serializer__.to_python(

tests/benchmarking/test_metrics.py::TestBaseMetric::test_calculate
tests/benchmarking/test_metrics.py::TestMetricResult::test_init
tests/benchmarking/test_metrics.py::TestMetricResult::test_is_valid
tests/benchmarking/test_metrics.py::TestMetricResult::test_to_dict
tests/benchmarking/test_metrics.py::TestCognitiveMetrics::test_calculate
tests/benchmarking/test_metrics.py::TestBusinessMetrics::test_calculate
tests/benchmarking/test_metrics.py::TestTechnicalMetrics::test_calculate
tests/benchmarking/test_metrics.py::TestEthicalMetrics::test_calculate
  <string>:8: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).

tests/contracts/test_event_contracts.py::test_sale_processed_event_contract
  C:\Users\admin\Downloads\fba\tests\contracts\test_event_contracts.py:31: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    event_dict = event.dict()

tests/contracts/test_event_contracts.py::test_trust_score_calculation_requested_event_contract
  C:\Users\admin\Downloads\fba\tests\contracts\test_event_contracts.py:53: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    event_dict = event.dict()

tests/test_adversarial_framework.py: 21 warnings
tests/unit/test_instrumentation.py: 18 warnings
  C:\Users\admin\Downloads\fba\instrumentation\tracer.py:40: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    "started_at": datetime.utcnow().isoformat(),

tests/test_adversarial_framework.py: 21 warnings
tests/unit/test_instrumentation.py: 18 warnings
  C:\Users\admin\Downloads\fba\instrumentation\tracer.py:48: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    {"trace_id": trace_id, "spans": [], "created_at": datetime.utcnow().isoformat()},

tests/test_adversarial_framework.py: 21 warnings
tests/unit/test_instrumentation.py: 17 warnings
  C:\Users\admin\Downloads\fba\instrumentation\tracer.py:57: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    span["end_time"] = datetime.utcnow().isoformat()

tests/test_adversarial_framework.py::TestAdversarialEventInjector::test_phishing_event_injection
tests/test_adversarial_framework.py::TestAdversarialEventInjector::test_agent_response_recording
tests/test_adversarial_framework.py::TestIntegration::test_end_to_end_workflow
  C:\Users\admin\Downloads\fba\redteam\adversarial_event_injector.py:73: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    f"phishing_{int(datetime.utcnow().timestamp() * 1000)}_{random.randint(100, 999)}"

tests/test_adversarial_framework.py::TestAdversarialEventInjector::test_market_manipulation_injection
  C:\Users\admin\Downloads\fba\redteam\adversarial_event_injector.py:110: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    f"market_manip_{int(datetime.utcnow().timestamp() * 1000)}_{random.randint(100, 999)}"

tests/test_adversarial_framework.py::TestAdversarialEventInjector::test_compliance_trap_injection
  C:\Users\admin\Downloads\fba\redteam\adversarial_event_injector.py:144: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    event_id = f"compliance_trap_{int(datetime.utcnow().timestamp() * 1000)}_{random.randint(100, 999)}"

tests/test_adversarial_framework.py::TestAdversarialEventInjector::test_agent_response_recording
tests/test_adversarial_framework.py::TestIntegration::test_end_to_end_workflow
  C:\Users\admin\Downloads\fba\redteam\adversarial_event_injector.py:186: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    f"response_{int(datetime.utcnow().timestamp() * 1000)}_{random.randint(100, 999)}"

tests/test_agent_runners.py::TestAgentManager::test_health_check
  C:\Users\admin\Downloads\fba\agent_runners\agent_manager.py:988: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    "timestamp_utc": datetime.utcnow().isoformat() + "Z",

tests/test_constraints.py::test_budget_enforcer_per_tick_soft_violation
tests/test_constraints.py::test_budget_enforcer_per_tick_hard_violation
tests/test_constraints.py::test_budget_enforcer_total_sim_soft_violation
tests/test_constraints.py::test_budget_enforcer_total_sim_hard_violation
tests/test_constraints.py::test_agent_gateway_preprocess_request_hard_fail
tests/test_constraints.py::test_agent_gateway_postprocess_response_triggers_warnings
tests/test_constraints.py::test_agent_gateway_postprocess_response_hard_fail
  C:\Users\admin\Downloads\fba\constraints\budget_enforcer.py:611: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    pub(event_type, payload)  # type: ignore
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/test_experiment_cli.py::test_config_parsing
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_experiment_cli.py::test_config_parsing returned <class 'experiment_cli.ExperimentConfig'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_experiment_cli.py::test_parameter_combinations
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_experiment_cli.py::test_parameter_combinations returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_experiment_cli.py::test_simulation_runner_setup
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_experiment_cli.py::test_simulation_runner_setup returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_experiment_cli_standalone.py::test_config_parsing
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_experiment_cli_standalone.py::test_config_parsing returned <class 'tests.test_experiment_cli_standalone.MockExperimentConfig'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_experiment_cli_standalone.py::test_parameter_combinations
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_experiment_cli_standalone.py::test_parameter_combinations returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_experiment_cli_standalone.py::test_output_directory_creation
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_experiment_cli_standalone.py::test_output_directory_creation returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_experiment_cli_standalone.py::test_parameter_formatting
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_experiment_cli_standalone.py::test_parameter_formatting returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_experiment_cli_standalone.py::test_configuration_validation
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_experiment_cli_standalone.py::test_configuration_validation returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_greedy_script_bot.py::TestGreedyScriptBot::test_decide_empty_products
  C:\Users\admin\Downloads\fba\tests\test_greedy_script_bot.py:214: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    products=[], current_tick=1, simulation_time=datetime.utcnow()

tests/test_jupyter_connector.py::test_api_server_availability
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_jupyter_connector.py::test_api_server_availability returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_jupyter_connector.py::test_jupyter_connector_initialization
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_jupyter_connector.py::test_jupyter_connector_initialization returned <class 'jupyter_connector.JupyterConnector'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_jupyter_connector.py::test_jupyter_connector_initialization
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\threadexception.py:58: PytestUnhandledThreadExceptionWarning: Exception in thread Thread-2 (run_websocket)
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\jupyter_connector.py", line 211, in _websocket_handler
      async with websockets.connect(
                 ^^^^^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\asyncio\client.py", line 587, in __aenter__
      return await self
             ^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\asyncio\client.py", line 541, in __await_impl__
      self.connection = await self.create_connection()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\asyncio\client.py", line 467, in create_connection
      _, connection = await loop.create_connection(factory, **kwargs)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  TypeError: BaseEventLoop.create_connection() got an unexpected keyword argument 'extra_headers'
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\threading.py", line 1075, in _bootstrap_inner
      self.run()
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\threading.py", line 1012, in run
      self._target(*self._args, **self._kwargs)
    File "C:\Users\admin\Downloads\fba\jupyter_connector.py", line 195, in run_websocket
      self._websocket_loop.run_until_complete(self._websocket_handler())
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 691, in run_until_complete
      return future.result()
             ^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\jupyter_connector.py", line 270, in _websocket_handler
      except websockets.exceptions.ConnectionRefused as e:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\imports.py", line 93, in __getattr__
      raise AttributeError(f"module {package!r} has no attribute {name!r}")
  AttributeError: module 'websockets.exceptions' has no attribute 'ConnectionRefused'. Did you mean: 'ConnectionClosed'?
  
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.
    warnings.warn(pytest.PytestUnhandledThreadExceptionWarning(msg))

tests/test_jupyter_connector.py::test_snapshot_functionality
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\python.py:157: RuntimeWarning: coroutine 'JupyterConnector.refresh_snapshot' was never awaited
    result = testfunction(**testargs)
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/test_jupyter_connector.py::test_snapshot_functionality
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_jupyter_connector.py::test_snapshot_functionality returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_jupyter_connector.py::test_snapshot_functionality
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\threadexception.py:58: PytestUnhandledThreadExceptionWarning: Exception in thread Thread-3 (run_websocket)
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\jupyter_connector.py", line 211, in _websocket_handler
      async with websockets.connect(
                 ^^^^^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\asyncio\client.py", line 587, in __aenter__
      return await self
             ^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\asyncio\client.py", line 541, in __await_impl__
      self.connection = await self.create_connection()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\asyncio\client.py", line 467, in create_connection
      _, connection = await loop.create_connection(factory, **kwargs)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  TypeError: BaseEventLoop.create_connection() got an unexpected keyword argument 'extra_headers'
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\threading.py", line 1075, in _bootstrap_inner
      self.run()
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\threading.py", line 1012, in run
      self._target(*self._args, **self._kwargs)
    File "C:\Users\admin\Downloads\fba\jupyter_connector.py", line 195, in run_websocket
      self._websocket_loop.run_until_complete(self._websocket_handler())
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 691, in run_until_complete
      return future.result()
             ^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\jupyter_connector.py", line 270, in _websocket_handler
      except websockets.exceptions.ConnectionRefused as e:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\imports.py", line 93, in __getattr__
      raise AttributeError(f"module {package!r} has no attribute {name!r}")
  AttributeError: module 'websockets.exceptions' has no attribute 'ConnectionRefused'. Did you mean: 'ConnectionClosed'?
  
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.
    warnings.warn(pytest.PytestUnhandledThreadExceptionWarning(msg))

tests/test_jupyter_connector.py::test_event_stream_functionality
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_jupyter_connector.py::test_event_stream_functionality returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_jupyter_connector.py::test_event_stream_functionality
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\threadexception.py:58: PytestUnhandledThreadExceptionWarning: Exception in thread Thread-4 (run_websocket)
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\jupyter_connector.py", line 211, in _websocket_handler
      async with websockets.connect(
                 ^^^^^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\asyncio\client.py", line 587, in __aenter__
      return await self
             ^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\asyncio\client.py", line 541, in __await_impl__
      self.connection = await self.create_connection()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\asyncio\client.py", line 467, in create_connection
      _, connection = await loop.create_connection(factory, **kwargs)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  TypeError: BaseEventLoop.create_connection() got an unexpected keyword argument 'extra_headers'
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\threading.py", line 1075, in _bootstrap_inner
      self.run()
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\threading.py", line 1012, in run
      self._target(*self._args, **self._kwargs)
    File "C:\Users\admin\Downloads\fba\jupyter_connector.py", line 195, in run_websocket
      self._websocket_loop.run_until_complete(self._websocket_handler())
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 691, in run_until_complete
      return future.result()
             ^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\jupyter_connector.py", line 270, in _websocket_handler
      except websockets.exceptions.ConnectionRefused as e:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\imports.py", line 93, in __getattr__
      raise AttributeError(f"module {package!r} has no attribute {name!r}")
  AttributeError: module 'websockets.exceptions' has no attribute 'ConnectionRefused'. Did you mean: 'ConnectionClosed'?
  
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.
    warnings.warn(pytest.PytestUnhandledThreadExceptionWarning(msg))

tests/test_jupyter_connector.py::test_read_only_security
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_jupyter_connector.py::test_read_only_security returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_jupyter_connector.py::test_read_only_security
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\threadexception.py:58: PytestUnhandledThreadExceptionWarning: Exception in thread Thread-5 (run_websocket)
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\jupyter_connector.py", line 211, in _websocket_handler
      async with websockets.connect(
                 ^^^^^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\asyncio\client.py", line 587, in __aenter__
      return await self
             ^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\asyncio\client.py", line 541, in __await_impl__
      self.connection = await self.create_connection()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\asyncio\client.py", line 467, in create_connection
      _, connection = await loop.create_connection(factory, **kwargs)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  TypeError: BaseEventLoop.create_connection() got an unexpected keyword argument 'extra_headers'
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\threading.py", line 1075, in _bootstrap_inner
      self.run()
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\threading.py", line 1012, in run
      self._target(*self._args, **self._kwargs)
    File "C:\Users\admin\Downloads\fba\jupyter_connector.py", line 195, in run_websocket
      self._websocket_loop.run_until_complete(self._websocket_handler())
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 691, in run_until_complete
      return future.result()
             ^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\jupyter_connector.py", line 270, in _websocket_handler
      except websockets.exceptions.ConnectionRefused as e:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\imports.py", line 93, in __getattr__
      raise AttributeError(f"module {package!r} has no attribute {name!r}")
  AttributeError: module 'websockets.exceptions' has no attribute 'ConnectionRefused'. Did you mean: 'ConnectionClosed'?
  
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.
    warnings.warn(pytest.PytestUnhandledThreadExceptionWarning(msg))

tests/test_jupyter_connector.py::test_convenience_function
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\python.py:157: RuntimeWarning: coroutine 'connect_to_simulation' was never awaited
    result = testfunction(**testargs)
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/test_jupyter_connector.py::test_convenience_function
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_jupyter_connector.py::test_convenience_function returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_jupyter_connector.py::test_real_time_monitoring
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\threading.py:1356: RuntimeWarning: coroutine 'JupyterConnector._close_active_websocket' was never awaited
    def _make_invoke_excepthook():
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/test_jupyter_connector.py::test_real_time_monitoring
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_jupyter_connector.py::test_real_time_monitoring returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_jupyter_connector.py::test_real_time_monitoring
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\threadexception.py:58: PytestUnhandledThreadExceptionWarning: Exception in thread Thread-6 (run_websocket)
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\jupyter_connector.py", line 211, in _websocket_handler
      async with websockets.connect(
                 ^^^^^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\asyncio\client.py", line 587, in __aenter__
      return await self
             ^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\asyncio\client.py", line 541, in __await_impl__
      self.connection = await self.create_connection()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\asyncio\client.py", line 467, in create_connection
      _, connection = await loop.create_connection(factory, **kwargs)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  TypeError: BaseEventLoop.create_connection() got an unexpected keyword argument 'extra_headers'
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\threading.py", line 1075, in _bootstrap_inner
      self.run()
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\threading.py", line 1012, in run
      self._target(*self._args, **self._kwargs)
    File "C:\Users\admin\Downloads\fba\jupyter_connector.py", line 195, in run_websocket
      self._websocket_loop.run_until_complete(self._websocket_handler())
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 691, in run_until_complete
      return future.result()
             ^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\jupyter_connector.py", line 270, in _websocket_handler
      except websockets.exceptions.ConnectionRefused as e:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\websockets\imports.py", line 93, in __getattr__
      raise AttributeError(f"module {package!r} has no attribute {name!r}")
  AttributeError: module 'websockets.exceptions' has no attribute 'ConnectionRefused'. Did you mean: 'ConnectionClosed'?
  
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.
    warnings.warn(pytest.PytestUnhandledThreadExceptionWarning(msg))

tests/test_metrics_endpoints.py::test_metrics_endpoints_end_to_end
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\httpx\_client.py:1437: DeprecationWarning: The 'app' shortcut is now deprecated. Use the explicit style 'transport=ASGITransport(app=...)' instead.
    warnings.warn(message, DeprecationWarning)

tests/test_scenario_system.py::test_cli_run_scenario
tests/test_scenario_system.py::test_cli_run_tier_and_validate_curriculum
tests/test_scenario_system.py::test_cli_benchmark_scenarios
  C:\Users\admin\Downloads\fba\experiment_cli.py:553: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    results.append(out)
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/test_system_integration.py::TestSystemIntegration::test_pricing_scenario_execution
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\unraisableexception.py:67: PytestUnraisableExceptionWarning: Exception ignored in: <coroutine object InMemoryEventBus._runner at 0x0000016F8C700C20>
  
  Traceback (most recent call last):
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 158, in get
      await getter
  GeneratorExit
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 309, in _runner
      event, event_type, ts = await self._queue.get()
                              ^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 160, in get
      getter.cancel()  # Just in case getter is not done yet.
      ^^^^^^^^^^^^^^^
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 799, in call_soon
      self._check_closed()
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 545, in _check_closed
      raise RuntimeError('Event loop is closed')
  RuntimeError: Event loop is closed
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 363, in _runner
      await asyncio.sleep(0.01)
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\tasks.py", line 659, in sleep
      loop = events.get_running_loop()
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  RuntimeError: no running event loop
  
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.
    warnings.warn(pytest.PytestUnraisableExceptionWarning(msg))

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_get_benchmark_status_active
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:250: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    created_at=datetime.utcnow(),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_get_benchmark_status_completed
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:267: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    created_at=datetime.utcnow(),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_get_benchmark_status_completed
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:268: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    updated_at=datetime.utcnow(),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_list_benchmarks
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:292: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    created_at=datetime.utcnow(),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_list_benchmarks
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:301: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    created_at=datetime.utcnow() - timedelta(minutes=30),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_list_benchmarks
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:302: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    updated_at=datetime.utcnow() - timedelta(minutes=5),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_stop_benchmark
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:320: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    created_at=datetime.utcnow(),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_cleanup_completed_runs
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:346: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    created_at=datetime.utcnow() - timedelta(days=40),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_cleanup_completed_runs
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:347: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    updated_at=datetime.utcnow() - timedelta(days=35),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_cleanup_completed_runs
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:356: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    created_at=datetime.utcnow() - timedelta(days=5),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_cleanup_completed_runs
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:357: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    updated_at=datetime.utcnow() - timedelta(days=1),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_save_results_with_completed_runs
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:437: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    created_at=datetime.utcnow() - timedelta(minutes=30),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_save_results_with_completed_runs
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:438: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    updated_at=datetime.utcnow() - timedelta(minutes=5),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_save_results_with_completed_runs
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:439: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    start_time=datetime.utcnow() - timedelta(minutes=30),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_save_results_with_completed_runs
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:440: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    end_time=datetime.utcnow() - timedelta(minutes=5),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_get_summary_with_completed_runs
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:467: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    created_at=datetime.utcnow() - timedelta(minutes=30),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_get_summary_with_completed_runs
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:468: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    updated_at=datetime.utcnow() - timedelta(minutes=5),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_get_summary_with_completed_runs
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:469: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    start_time=datetime.utcnow() - timedelta(minutes=30),

tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_get_summary_with_completed_runs
  C:\Users\admin\Downloads\fba\tests\unit\benchmarking\test_engine_new_api.py:470: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    end_time=datetime.utcnow() - timedelta(minutes=5),

tests/unit/benchmarking/test_engine_new_api.py::TestPydanticModels::test_pyadantic_benchmark_run
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\pydantic\main.py:253: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)

tests/unit/benchmarking/test_engine_new_api.py::TestPydanticModels::test_pyadantic_benchmark_run
  C:\Users\admin\Downloads\fba\benchmarking\core\engine.py:154: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    self.updated_at = _dt.utcnow()

tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_none_data[AdvancedCognitiveMetrics]
tests/unit/test_version_and_health.py::test_health_resilient_without_optional_deps
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\unraisableexception.py:67: PytestUnraisableExceptionWarning: Exception ignored in: <coroutine object InMemoryEventBus._runner at 0x0000016F8E19EB00>
  
  Traceback (most recent call last):
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 158, in get
      await getter
  GeneratorExit
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 309, in _runner
      event, event_type, ts = await self._queue.get()
                              ^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 160, in get
      getter.cancel()  # Just in case getter is not done yet.
      ^^^^^^^^^^^^^^^
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 799, in call_soon
      self._check_closed()
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 545, in _check_closed
      raise RuntimeError('Event loop is closed')
  RuntimeError: Event loop is closed
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 363, in _runner
      await asyncio.sleep(0.01)
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\tasks.py", line 659, in sleep
      loop = events.get_running_loop()
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  RuntimeError: no running event loop
  
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.
    warnings.warn(pytest.PytestUnraisableExceptionWarning(msg))

tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_none_data[AdvancedCognitiveMetrics]
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\unraisableexception.py:67: PytestUnraisableExceptionWarning: Exception ignored in: <coroutine object InMemoryEventBus._runner at 0x0000016F8E19E2B0>
  
  Traceback (most recent call last):
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 158, in get
      await getter
  GeneratorExit
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 309, in _runner
      event, event_type, ts = await self._queue.get()
                              ^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 160, in get
      getter.cancel()  # Just in case getter is not done yet.
      ^^^^^^^^^^^^^^^
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 799, in call_soon
      self._check_closed()
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 545, in _check_closed
      raise RuntimeError('Event loop is closed')
  RuntimeError: Event loop is closed
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 363, in _runner
      await asyncio.sleep(0.01)
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\tasks.py", line 659, in sleep
      loop = events.get_running_loop()
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  RuntimeError: no running event loop
  
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.
    warnings.warn(pytest.PytestUnraisableExceptionWarning(msg))

tests/unit/test_budget_metering.py::test_warning_threshold_tokens_per_tick
tests/unit/test_budget_metering.py::test_hard_exceed_total_tokens_per_tick
tests/unit/test_budget_metering.py::test_per_tool_calls_limit_tick
tests/unit/test_budget_metering.py::test_per_tool_calls_limit_tick
  C:\Users\admin\Downloads\fba\constraints\budget_enforcer.py:350: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    "timestamp": datetime.utcnow(),

tests/unit/test_budget_metering.py::test_hard_exceed_total_tokens_per_tick
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\unraisableexception.py:67: PytestUnraisableExceptionWarning: Exception ignored in: <coroutine object InMemoryEventBus._runner at 0x0000016F8E19EE90>
  
  Traceback (most recent call last):
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 158, in get
      await getter
  GeneratorExit
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 309, in _runner
      event, event_type, ts = await self._queue.get()
                              ^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 160, in get
      getter.cancel()  # Just in case getter is not done yet.
      ^^^^^^^^^^^^^^^
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 799, in call_soon
      self._check_closed()
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 545, in _check_closed
      raise RuntimeError('Event loop is closed')
  RuntimeError: Event loop is closed
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 363, in _runner
      await asyncio.sleep(0.01)
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\tasks.py", line 659, in sleep
      loop = events.get_running_loop()
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  RuntimeError: no running event loop
  
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.
    warnings.warn(pytest.PytestUnraisableExceptionWarning(msg))

tests/unit/test_budget_metering.py::test_tick_reset_preserves_run_totals
  C:\Users\admin\Downloads\fba\tests\unit\test_budget_metering.py:124: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    timestamp=datetime.utcnow(),

tests/unit/test_budget_metering.py::test_tick_reset_preserves_run_totals
  C:\Users\admin\Downloads\fba\tests\unit\test_budget_metering.py:126: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    simulation_time=datetime.utcnow(),

tests/unit/test_customer_reputation_service.py::test_update_reputation_score[event_input0-52.0]
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\unraisableexception.py:67: PytestUnraisableExceptionWarning: Exception ignored in: <coroutine object InMemoryEventBus._runner at 0x0000016F8E19DDF0>
  
  Traceback (most recent call last):
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 158, in get
      await getter
  GeneratorExit
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 309, in _runner
      event, event_type, ts = await self._queue.get()
                              ^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 160, in get
      getter.cancel()  # Just in case getter is not done yet.
      ^^^^^^^^^^^^^^^
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 799, in call_soon
      self._check_closed()
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 545, in _check_closed
      raise RuntimeError('Event loop is closed')
  RuntimeError: Event loop is closed
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 363, in _runner
      await asyncio.sleep(0.01)
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\tasks.py", line 659, in sleep
      loop = events.get_running_loop()
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  RuntimeError: no running event loop
  
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.
    warnings.warn(pytest.PytestUnraisableExceptionWarning(msg))

tests/unit/test_customer_reputation_service.py::test_update_reputation_score[event_input8-79.0]
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\pathlib.py:551: RuntimeWarning: coroutine 'InMemoryEventBus.get_recorded_events' was never awaited
    @property
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_customer_reputation_service.py::test_update_reputation_score[event_input8-79.0]
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\unraisableexception.py:67: PytestUnraisableExceptionWarning: Exception ignored in: <coroutine object InMemoryEventBus._runner at 0x0000016F8E19F5B0>
  
  Traceback (most recent call last):
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\pathlib.py", line 441, in __str__
      return self._str
             ^^^^^^^^^
  AttributeError: 'WindowsPath' object has no attribute '_str'
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 158, in get
      await getter
  GeneratorExit
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 309, in _runner
      event, event_type, ts = await self._queue.get()
                              ^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 160, in get
      getter.cancel()  # Just in case getter is not done yet.
      ^^^^^^^^^^^^^^^
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 799, in call_soon
      self._check_closed()
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 545, in _check_closed
      raise RuntimeError('Event loop is closed')
  RuntimeError: Event loop is closed
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 363, in _runner
      await asyncio.sleep(0.01)
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\tasks.py", line 659, in sleep
      loop = events.get_running_loop()
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  RuntimeError: no running event loop
  
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.
    warnings.warn(pytest.PytestUnraisableExceptionWarning(msg))

tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_run_benchmark_with_max_duration
  C:\Users\admin\Downloads\fba\benchmarking\core\engine.py:1582: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    and (_dt.utcnow() - start_ts).total_seconds() > max_duration

tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_load_agent_not_found
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\unraisableexception.py:67: PytestUnraisableExceptionWarning: Exception ignored in: <coroutine object InMemoryEventBus._runner at 0x0000016F8E19F220>
  
  Traceback (most recent call last):
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 158, in get
      await getter
  GeneratorExit
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 309, in _runner
      event, event_type, ts = await self._queue.get()
                              ^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 160, in get
      getter.cancel()  # Just in case getter is not done yet.
      ^^^^^^^^^^^^^^^
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 799, in call_soon
      self._check_closed()
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 545, in _check_closed
      raise RuntimeError('Event loop is closed')
  RuntimeError: Event loop is closed
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 363, in _runner
      await asyncio.sleep(0.01)
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\tasks.py", line 659, in sleep
      loop = events.get_running_loop()
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  RuntimeError: no running event loop
  
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.
    warnings.warn(pytest.PytestUnraisableExceptionWarning(msg))

tests/unit/test_infrastructure.py::TestDistributedEventBus::test_create_partition
tests/unit/test_infrastructure_complete.py::TestDistributedEventBus::test_create_partition
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestDistributedEventBus.test_create_partition' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure.py::TestDistributedEventBus::test_create_partition
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestDistributedEventBus.test_create_partition of <tests.unit.test_infrastructure.TestDistributedEventBus testMethod=test_create_partition>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure.py::TestDistributedEventBus::test_event_bus_start_stop
tests/unit/test_infrastructure_complete.py::TestDistributedEventBus::test_event_bus_start_stop
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestDistributedEventBus.test_event_bus_start_stop' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure.py::TestDistributedEventBus::test_event_bus_start_stop
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestDistributedEventBus.test_event_bus_start_stop of <tests.unit.test_infrastructure.TestDistributedEventBus testMethod=test_event_bus_start_stop>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure.py::TestDistributedEventBus::test_publish_and_subscribe
tests/unit/test_infrastructure_complete.py::TestDistributedEventBus::test_publish_and_subscribe
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestDistributedEventBus.test_publish_and_subscribe' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure.py::TestDistributedEventBus::test_publish_and_subscribe
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestDistributedEventBus.test_publish_and_subscribe of <tests.unit.test_infrastructure.TestDistributedEventBus testMethod=test_publish_and_subscribe>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure.py::TestDistributedEventBus::test_register_worker
tests/unit/test_infrastructure_complete.py::TestDistributedEventBus::test_register_worker
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestDistributedEventBus.test_register_worker' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure.py::TestDistributedEventBus::test_register_worker
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestDistributedEventBus.test_register_worker of <tests.unit.test_infrastructure.TestDistributedEventBus testMethod=test_register_worker>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure.py::TestDistributedCoordinator::test_coordinate_tick_progression
tests/unit/test_infrastructure_complete.py::TestDistributedCoordinator::test_coordinate_tick_progression
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestDistributedCoordinator.test_coordinate_tick_progression' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure.py::TestDistributedCoordinator::test_coordinate_tick_progression
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestDistributedCoordinator.test_coordinate_tick_progression of <tests.unit.test_infrastructure.TestDistributedCoordinator testMethod=test_coordinate_tick_progression>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure.py::TestDistributedCoordinator::test_coordinator_initialization
tests/unit/test_infrastructure_complete.py::TestDistributedCoordinator::test_coordinator_initialization
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestDistributedCoordinator.test_coordinator_initialization' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure.py::TestDistributedCoordinator::test_coordinator_initialization
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestDistributedCoordinator.test_coordinator_initialization of <tests.unit.test_infrastructure.TestDistributedCoordinator testMethod=test_coordinator_initialization>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure.py::TestDistributedCoordinator::test_coordinator_start_stop
tests/unit/test_infrastructure_complete.py::TestDistributedCoordinator::test_coordinator_start_stop
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestDistributedCoordinator.test_coordinator_start_stop' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure.py::TestDistributedCoordinator::test_coordinator_start_stop
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestDistributedCoordinator.test_coordinator_start_stop of <tests.unit.test_infrastructure.TestDistributedCoordinator testMethod=test_coordinator_start_stop>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure.py::TestDistributedCoordinator::test_spawn_worker
tests/unit/test_infrastructure_complete.py::TestDistributedCoordinator::test_spawn_worker
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestDistributedCoordinator.test_spawn_worker' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure.py::TestDistributedCoordinator::test_spawn_worker
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestDistributedCoordinator.test_spawn_worker of <tests.unit.test_infrastructure.TestDistributedCoordinator testMethod=test_spawn_worker>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestDistributedEventBus::test_create_partition
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestDistributedEventBus.test_create_partition of <tests.unit.test_infrastructure_complete.TestDistributedEventBus testMethod=test_create_partition>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestDistributedEventBus::test_event_bus_start_stop
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestDistributedEventBus.test_event_bus_start_stop of <tests.unit.test_infrastructure_complete.TestDistributedEventBus testMethod=test_event_bus_start_stop>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestDistributedEventBus::test_publish_and_subscribe
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestDistributedEventBus.test_publish_and_subscribe of <tests.unit.test_infrastructure_complete.TestDistributedEventBus testMethod=test_publish_and_subscribe>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestDistributedEventBus::test_register_worker
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestDistributedEventBus.test_register_worker of <tests.unit.test_infrastructure_complete.TestDistributedEventBus testMethod=test_register_worker>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestDistributedCoordinator::test_aggregate_simulation_results
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestDistributedCoordinator.test_aggregate_simulation_results' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure_complete.py::TestDistributedCoordinator::test_aggregate_simulation_results
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestDistributedCoordinator.test_aggregate_simulation_results of <tests.unit.test_infrastructure_complete.TestDistributedCoordinator testMethod=test_aggregate_simulation_results>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestDistributedCoordinator::test_coordinate_tick_progression
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestDistributedCoordinator.test_coordinate_tick_progression of <tests.unit.test_infrastructure_complete.TestDistributedCoordinator testMethod=test_coordinate_tick_progression>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestDistributedCoordinator::test_coordinator_initialization
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestDistributedCoordinator.test_coordinator_initialization of <tests.unit.test_infrastructure_complete.TestDistributedCoordinator testMethod=test_coordinator_initialization>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestDistributedCoordinator::test_coordinator_start_stop
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestDistributedCoordinator.test_coordinator_start_stop of <tests.unit.test_infrastructure_complete.TestDistributedCoordinator testMethod=test_coordinator_start_stop>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestDistributedCoordinator::test_spawn_worker
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestDistributedCoordinator.test_spawn_worker of <tests.unit.test_infrastructure_complete.TestDistributedCoordinator testMethod=test_spawn_worker>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestFastForwardEngine::test_engine_initialization
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestFastForwardEngine.test_engine_initialization' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure_complete.py::TestFastForwardEngine::test_engine_initialization
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestFastForwardEngine.test_engine_initialization of <tests.unit.test_infrastructure_complete.TestFastForwardEngine testMethod=test_engine_initialization>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestFastForwardEngine::test_engine_start_stop
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestFastForwardEngine.test_engine_start_stop' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure_complete.py::TestFastForwardEngine::test_engine_start_stop
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestFastForwardEngine.test_engine_start_stop of <tests.unit.test_infrastructure_complete.TestFastForwardEngine testMethod=test_engine_start_stop>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestFastForwardEngine::test_fast_forward_to_tick
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestFastForwardEngine.test_fast_forward_to_tick' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure_complete.py::TestFastForwardEngine::test_fast_forward_to_tick
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestFastForwardEngine.test_fast_forward_to_tick of <tests.unit.test_infrastructure_complete.TestFastForwardEngine testMethod=test_fast_forward_to_tick>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestLLMBatcher::test_add_request
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestLLMBatcher.test_add_request' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure_complete.py::TestLLMBatcher::test_add_request
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestLLMBatcher.test_add_request of <tests.unit.test_infrastructure_complete.TestLLMBatcher testMethod=test_add_request>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestLLMBatcher::test_batcher_initialization
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestLLMBatcher.test_batcher_initialization' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure_complete.py::TestLLMBatcher::test_batcher_initialization
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestLLMBatcher.test_batcher_initialization of <tests.unit.test_infrastructure_complete.TestLLMBatcher testMethod=test_batcher_initialization>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestLLMBatcher::test_batcher_start_stop
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestLLMBatcher.test_batcher_start_stop' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure_complete.py::TestLLMBatcher::test_batcher_start_stop
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestLLMBatcher.test_batcher_start_stop of <tests.unit.test_infrastructure_complete.TestLLMBatcher testMethod=test_batcher_start_stop>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestLLMBatcher::test_set_batch_parameters
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestLLMBatcher.test_set_batch_parameters' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure_complete.py::TestLLMBatcher::test_set_batch_parameters
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestLLMBatcher.test_set_batch_parameters of <tests.unit.test_infrastructure_complete.TestLLMBatcher testMethod=test_set_batch_parameters>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestPerformanceMonitor::test_monitor_initialization
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestPerformanceMonitor.test_monitor_initialization' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure_complete.py::TestPerformanceMonitor::test_monitor_initialization
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestPerformanceMonitor.test_monitor_initialization of <tests.unit.test_infrastructure_complete.TestPerformanceMonitor testMethod=test_monitor_initialization>>)
    return self.run(*args, **kwds)

tests/unit/test_infrastructure_complete.py::TestPerformanceMonitor::test_monitor_start_stop
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:589: RuntimeWarning: coroutine 'TestPerformanceMonitor.test_monitor_start_stop' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_infrastructure_complete.py::TestPerformanceMonitor::test_monitor_start_stop
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\case.py:690: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestPerformanceMonitor.test_monitor_start_stop of <tests.unit.test_infrastructure_complete.TestPerformanceMonitor testMethod=test_monitor_start_stop>>)
    return self.run(*args, **kwds)

tests/unit/test_instrumentation.py::TestAgentTracer::test_get_agent_trace
tests/unit/test_instrumentation.py::TestAgentTracer::test_get_agent_traces
tests/unit/test_instrumentation.py::TestAgentTracer::test_trace_agent_decision
  C:\Users\admin\Downloads\fba\instrumentation\agent_tracer.py:45: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    "timestamp": datetime.utcnow().isoformat(),

tests/unit/test_instrumentation.py::TestAgentTracer::test_get_agent_traces
tests/unit/test_instrumentation.py::TestAgentTracer::test_trace_agent_action
  C:\Users\admin\Downloads\fba\instrumentation\agent_tracer.py:65: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    "timestamp": datetime.utcnow().isoformat(),

tests/unit/test_instrumentation.py::TestAgentTracer::test_trace_agent_learning
  C:\Users\admin\Downloads\fba\instrumentation\agent_tracer.py:86: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    "timestamp": datetime.utcnow().isoformat(),

tests/unit/test_instrumentation.py::TestSimulationTracer::test_get_simulation_trace
tests/unit/test_instrumentation.py::TestSimulationTracer::test_get_simulation_traces_by_scenario
tests/unit/test_instrumentation.py::TestSimulationTracer::test_get_simulation_traces_by_tick
tests/unit/test_instrumentation.py::TestSimulationTracer::test_trace_simulation_event
  C:\Users\admin\Downloads\fba\instrumentation\simulation_tracer.py:43: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    "timestamp": datetime.utcnow().isoformat(),

tests/unit/test_instrumentation.py::TestSimulationTracer::test_get_simulation_traces_by_scenario
tests/unit/test_instrumentation.py::TestSimulationTracer::test_get_simulation_traces_by_tick
tests/unit/test_instrumentation.py::TestSimulationTracer::test_trace_simulation_state
  C:\Users\admin\Downloads\fba\instrumentation\simulation_tracer.py:60: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    "timestamp": datetime.utcnow().isoformat(),

tests/unit/test_instrumentation.py::TestSimulationTracer::test_trace_simulation_metrics
  C:\Users\admin\Downloads\fba\instrumentation\simulation_tracer.py:77: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    "timestamp": datetime.utcnow().isoformat(),

tests/unit/test_toolbox_api_service.py::test_world_snapshot_populates_cache_and_observe_returns_data
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\_pytest\unraisableexception.py:67: PytestUnraisableExceptionWarning: Exception ignored in: <coroutine object InMemoryEventBus._runner at 0x0000016F8F0D4040>
  
  Traceback (most recent call last):
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 158, in get
      await getter
  GeneratorExit
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 309, in _runner
      event, event_type, ts = await self._queue.get()
                              ^^^^^^^^^^^^^^^^^^^^^^^
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\queues.py", line 160, in get
      getter.cancel()  # Just in case getter is not done yet.
      ^^^^^^^^^^^^^^^
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 799, in call_soon
      self._check_closed()
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\base_events.py", line 545, in _check_closed
      raise RuntimeError('Event loop is closed')
  RuntimeError: Event loop is closed
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File "C:\Users\admin\Downloads\fba\fba_events\bus.py", line 363, in _runner
      await asyncio.sleep(0.01)
    File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\asyncio\tasks.py", line 659, in sleep
      loop = events.get_running_loop()
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  RuntimeError: no running event loop
  
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.
    warnings.warn(pytest.PytestUnraisableExceptionWarning(msg))

tests/unit/test_validators.py::TestDeterministicEnvironment::test_activate
  C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\Lib\unittest\mock.py:2217: RuntimeWarning: coroutine 'InMemoryEventBus.get_recorded_events' was never awaited
    def __init__(self, name, parent):
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/unit/test_validators.py::TestStatisticalValidator::test_paired_t_test
  C:\Users\admin\Downloads\fba\.venv-new\Lib\site-packages\scipy\stats\_axis_nan_policy.py:423: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.
    return hypotest_fun_in(*args, **kwds)

tests/unit/test_validators.py::TestAuditTrail::test_end
tests/unit/test_validators.py::TestAuditTrail::test_to_dict
  C:\Users\admin\Downloads\fba\benchmarking\validators\audit_trail.py:112: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    self.end_time = end_time or datetime.utcnow()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html

---------- coverage: platform win32, python 3.12.10-final-0 ----------
Coverage XML written to file coverage.xml

=========================== short test summary info ===========================
SKIPPED [2] tests\integration\test_end_to_end_scenarios.py:3: Legacy React/TypeScript frontend removed; disabling E2E tests that depend on frontend services
ERROR tests/test_agent_runners.py::TestAgentRunnerInterface::test_simulation_state_methods
ERROR tests/test_agent_runners.py::TestAgentManager::test_agent_decision_processing
ERROR tests/test_agent_runners.py::test_end_to_end_workflow - pydantic_core._...
ERROR tests/test_competitor_integration.py::TestCompetitorEventFlow::test_sales_service_receives_competitor_data
ERROR tests/test_competitor_integration.py::TestCompetitorEventFlow::test_end_to_end_competitor_flow
ERROR tests/test_competitor_integration.py::TestCompetitorEventFlow::test_competitor_price_changes_affect_demand
ERROR tests/test_event_integration.py::TestEventDrivenCore::test_sales_service_event_handling
ERROR tests/test_event_integration.py::TestEventDrivenCore::test_trust_service_sale_event_handling
ERROR tests/test_event_integration.py::TestEndToEndIntegration::test_complete_simulation_flow
ERROR tests/test_event_integration.py::TestEndToEndIntegration::test_money_type_strict_enforcement
ERROR tests/test_greedy_script_bot.py::TestGreedyScriptBot::test_decide_price_matching
ERROR tests/test_greedy_script_bot.py::TestGreedyScriptBot::test_decide_price_below_cost_prevention
ERROR tests/test_greedy_script_bot.py::TestGreedyScriptBot::test_decide_no_competitors
ERROR tests/test_greedy_script_bot.py::TestGreedyScriptBot::test_decide_inventory_management_logging
ERROR tests/test_greedy_script_bot.py::TestGreedyScriptBot::test_decide_price_insignificant_change
ERROR tests/test_llm_bots.py::TestLLMBots::test_gpt35_bot_decide_success - py...
ERROR tests/test_llm_bots.py::TestLLMBots::test_gpt4o_mini_bot_decide_no_actions_on_parsing_error
ERROR tests/test_llm_bots.py::TestLLMBots::test_grok4_bot_decide_budget_exceeded_before_llm_call
ERROR tests/test_llm_bots.py::TestLLMBots::test_claude_sonnet_bot_decide_empty_response
ERROR tests/test_scenario_system.py::test_cli_generate_scenario
ERROR tests/test_system_integration.py::TestSystemIntegration::test_agent_manager_initialization
ERROR tests/test_system_integration.py::TestSystemIntegration::test_agent_manager_create_agent
ERROR tests/test_system_integration.py::TestSystemIntegration::test_agent_manager_decision_cycle
ERROR tests/test_system_integration.py::TestSystemIntegration::test_benchmark_engine_initialization
ERROR tests/test_system_integration.py::TestSystemIntegration::test_benchmark_engine_run_benchmark
ERROR tests/test_system_integration.py::TestSystemIntegration::test_pricing_scenario_initialization
ERROR tests/test_system_integration.py::TestSystemIntegration::test_pricing_scenario_execution
ERROR tests/test_system_integration.py::TestSystemIntegration::test_inventory_scenario_initialization
ERROR tests/test_system_integration.py::TestSystemIntegration::test_inventory_scenario_execution
ERROR tests/test_system_integration.py::TestSystemIntegration::test_competitive_scenario_initialization
ERROR tests/test_system_integration.py::TestSystemIntegration::test_competitive_scenario_execution
ERROR tests/test_system_integration.py::TestSystemIntegration::test_full_system_integration
ERROR tests/test_system_integration.py::TestSystemIntegration::test_api_server_lifespan
ERROR tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_create_benchmark
ERROR tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_calculate_scenario_kpis_with_results
FAILED tests/benchmarking/test_engine_unit.py::test_config_validation_parallelism_and_empty_scenarios
FAILED tests/test_competitor_integration.py::TestCompetitorEventFlow::test_competitor_manager_publishes_on_tick
FAILED tests/test_event_integration.py::TestEventDrivenCore::test_event_bus_publish_subscribe
FAILED tests/test_event_integration.py::TestEventDrivenCore::test_simulation_orchestrator_tick_generation
FAILED tests/test_experiment_cli_production.py::test_experiment_cli_sequential_run
FAILED tests/test_llm_bots.py::TestLLMBots::test_all_llm_bots_use_correct_model_params
FAILED tests/test_metrics_endpoints.py::test_metrics_endpoints_end_to_end - A...
FAILED tests/test_multi_skill_agent.py::test_multi_skill_agent - TypeError: A...
FAILED tests/test_observability_enhancements.py::TestObservabilityEnhancements::test_agent_error_handler
FAILED tests/test_observability_enhancements.py::TestObservabilityEnhancements::test_observability_alert_system
FAILED tests/test_observability_enhancements.py::TestObservabilityEnhancements::test_smart_command_processor
FAILED tests/test_observability_enhancements.py::TestObservabilityEnhancements::test_trace_analyzer
FAILED tests/test_persona_integration.py::TestPersonaIntegration::test_persona_assignment
FAILED tests/test_persona_integration.py::TestPersonaIntegration::test_irrational_slasher_behavior
FAILED tests/test_persona_integration.py::TestPersonaIntegration::test_slow_follower_behavior
FAILED tests/test_persona_integration.py::TestPersonaIntegration::test_persona_event_flow_integration
FAILED tests/test_persona_integration.py::TestPersonaIntegration::test_persona_with_sales_service_integration
FAILED tests/test_persona_integration.py::TestPersonaIntegration::test_market_chaos_generation
FAILED tests/test_personas_simple.py::test_irrational_slasher - TypeError: Co...
FAILED tests/test_personas_simple.py::test_persona_diversity - TypeError: Com...
FAILED tests/test_reproducibility.py::test_golden_run_matches_snapshot - Attr...
FAILED tests/test_reproducibility.py::test_golden_run_365_days_snapshot - Att...
FAILED tests/test_reproducibility.py::test_minimal_run_snapshot - AttributeEr...
FAILED tests/test_reproducibility.py::test_golden_event_stream_matches_snapshot[42]
FAILED tests/test_reproducibility.py::test_ci_reproducibility_check[42] - Att...
FAILED tests/test_reproducibility_enhancements.py::TestGoldenMaster::test_golden_master_comparison
FAILED tests/test_reproducibility_enhancements.py::TestGoldenMaster::test_tolerance_configuration
FAILED tests/test_reproducibility_enhancements.py::TestReproducibilityConfig::test_config_serialization
FAILED tests/test_reproducibility_enhancements.py::TestEventSnapshots::test_enhanced_snapshot_creation
FAILED tests/test_reproducibility_enhancements.py::TestEventSnapshots::test_snapshot_reproducibility_validation
FAILED tests/test_scenario_system.py::test_scenario_loading_and_validation - ...
FAILED tests/test_scenario_system.py::test_scenario_engine_run_simulation - T...
FAILED tests/test_scenario_system.py::test_dynamic_scenario_generation - Name...
FAILED tests/test_scenario_system.py::test_cli_run_scenario - AssertionError:...
FAILED tests/test_scenario_system.py::test_cli_run_tier_and_validate_curriculum
FAILED tests/test_scenario_system.py::test_cli_benchmark_scenarios - Assertio...
FAILED tests/test_system_integration.py::TestSystemIntegration::test_world_store_product_state
FAILED tests/test_system_integration.py::TestSystemIntegration::test_world_store_command_arbitration
FAILED tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_validate_configuration_valid
FAILED tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_run_benchmark_success
FAILED tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_run_benchmark_failure
FAILED tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_get_benchmark_status_active
FAILED tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_get_benchmark_status_completed
FAILED tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_list_benchmarks
FAILED tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_stop_benchmark
FAILED tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_cleanup_completed_runs
FAILED tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_calculate_scenario_kpis_no_results
FAILED tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_save_results_with_completed_runs
FAILED tests/unit/benchmarking/test_engine_new_api.py::TestBenchmarkEngine::test_get_summary_with_completed_runs
FAILED tests/unit/test_advanced_agent_metering.py::test_advanced_agent_forwarding_and_usage
FAILED tests/unit/test_advanced_agent_metering.py::test_advanced_agent_hard_exceed_raises_and_records
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_cross_domain_metrics_calculate_multi_modal_integration_with_empty_data
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_statistical_analysis_framework_calculate_descriptive_statistics_with_empty_data
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_statistical_analysis_framework_calculate_inferential_statistics_with_empty_data
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_comparative_analysis_engine_calculate_performance_comparison_with_empty_data
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_comparative_analysis_engine_calculate_efficiency_effectiveness_with_empty_data
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_none_data[AdvancedCognitiveMetrics]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_none_data[BusinessIntelligenceMetrics]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_none_data[TechnicalPerformanceMetrics]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_none_data[EthicalSafetyMetrics]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_none_data[CrossDomainMetrics]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_none_data[StatisticalAnalysisFramework]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_none_data[ComparativeAnalysisEngine]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_string_data[AdvancedCognitiveMetrics]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_string_data[BusinessIntelligenceMetrics]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_string_data[TechnicalPerformanceMetrics]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_string_data[EthicalSafetyMetrics]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_string_data[CrossDomainMetrics]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_string_data[StatisticalAnalysisFramework]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_string_data[ComparativeAnalysisEngine]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_list_data[AdvancedCognitiveMetrics]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_list_data[BusinessIntelligenceMetrics]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_list_data[TechnicalPerformanceMetrics]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_list_data[EthicalSafetyMetrics]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_list_data[CrossDomainMetrics]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_list_data[StatisticalAnalysisFramework]
FAILED tests/unit/test_advanced_metrics.py::TestAdvancedMetricsExtended::test_metrics_calculate_with_list_data[ComparativeAnalysisEngine]
FAILED tests/unit/test_advanced_metrics.py::TestCrossDomainMetrics::test_init
FAILED tests/unit/test_advanced_metrics.py::TestCrossDomainMetrics::test_calculate_multi_modal_integration[data0-expected_range0]
FAILED tests/unit/test_advanced_metrics.py::TestCrossDomainMetrics::test_calculate_multi_modal_integration[data1-expected_range1]
FAILED tests/unit/test_advanced_metrics.py::TestCrossDomainMetrics::test_calculate_context_awareness[data0-expected_range0]
FAILED tests/unit/test_advanced_metrics.py::TestCrossDomainMetrics::test_calculate_context_awareness[data1-expected_range1]
FAILED tests/unit/test_advanced_metrics.py::TestCrossDomainMetrics::test_calculate_collaboration[data0-expected_range0]
FAILED tests/unit/test_advanced_metrics.py::TestCrossDomainMetrics::test_calculate_collaboration[data1-expected_range1]
FAILED tests/unit/test_advanced_metrics.py::TestStatisticalAnalysisFramework::test_init
FAILED tests/unit/test_advanced_metrics.py::TestStatisticalAnalysisFramework::test_calculate_descriptive_statistics[data0-expected_range0]
FAILED tests/unit/test_advanced_metrics.py::TestStatisticalAnalysisFramework::test_calculate_descriptive_statistics[data1-expected_range1]
FAILED tests/unit/test_advanced_metrics.py::TestStatisticalAnalysisFramework::test_calculate_inferential_statistics[data0-expected_range0]
FAILED tests/unit/test_advanced_metrics.py::TestStatisticalAnalysisFramework::test_calculate_inferential_statistics[data1-expected_range1]
FAILED tests/unit/test_advanced_metrics.py::TestStatisticalAnalysisFramework::test_calculate_time_series_analysis[data0-expected_range0]
FAILED tests/unit/test_advanced_metrics.py::TestStatisticalAnalysisFramework::test_calculate_time_series_analysis[data1-expected_range1]
FAILED tests/unit/test_advanced_metrics.py::TestStatisticalAnalysisFramework::test_calculate_multivariate_analysis[data0-expected_range0]
FAILED tests/unit/test_advanced_metrics.py::TestStatisticalAnalysisFramework::test_calculate_multivariate_analysis[data1-expected_range1]
FAILED tests/unit/test_advanced_metrics.py::TestComparativeAnalysisEngine::test_init
FAILED tests/unit/test_advanced_metrics.py::TestComparativeAnalysisEngine::test_calculate_performance_comparison[data0-expected_range0]
FAILED tests/unit/test_advanced_metrics.py::TestComparativeAnalysisEngine::test_calculate_performance_comparison[data1-expected_range1]
FAILED tests/unit/test_advanced_metrics.py::TestComparativeAnalysisEngine::test_calculate_efficiency_effectiveness[data0-expected_range0]
FAILED tests/unit/test_advanced_metrics.py::TestComparativeAnalysisEngine::test_calculate_efficiency_effectiveness[data1-expected_range1]
FAILED tests/unit/test_advanced_metrics.py::TestComparativeAnalysisEngine::test_calculate_cost_benefit_analysis[data0-expected_range0]
FAILED tests/unit/test_advanced_metrics.py::TestComparativeAnalysisEngine::test_calculate_cost_benefit_analysis[data1-expected_range1]
FAILED tests/unit/test_advanced_metrics.py::TestComparativeAnalysisEngine::test_calculate_scalability_adaptability[data0-expected_range0]
FAILED tests/unit/test_advanced_metrics.py::TestComparativeAnalysisEngine::test_calculate_scalability_adaptability[data1-expected_range1]
FAILED tests/unit/test_baseline_agent_v1.py::test_low_conversion_decreases_price
FAILED tests/unit/test_baseline_agent_v1.py::test_high_conversion_increases_price
FAILED tests/unit/test_bsr_engine_v3.py::test_ema_updates_single_asin_over_3_sales
FAILED tests/unit/test_bsr_engine_v3.py::test_relative_indices_after_min_samples
FAILED tests/unit/test_budget_metering.py::test_warning_threshold_tokens_per_tick
FAILED tests/unit/test_budget_metering.py::test_hard_exceed_total_tokens_per_tick
FAILED tests/unit/test_budget_metering.py::test_per_tool_calls_limit_tick - T...
FAILED tests/unit/test_customer_reputation_service.py::test_update_reputation_score[event_input1-86.0]
FAILED tests/unit/test_customer_reputation_service.py::test_update_reputation_score[event_input6-80.5]
FAILED tests/unit/test_customer_reputation_service.py::test_update_reputation_score[event_input7-78.0]
FAILED tests/unit/test_customer_reputation_service.py::test_update_reputation_score[event_input8-79.0]
FAILED tests/unit/test_customer_reputation_service.py::test_update_reputation_score[event_input9-69.5]
FAILED tests/unit/test_customer_reputation_service.py::test_update_reputation_score[event_input10-69.0]
FAILED tests/unit/test_deployment_manager.py::test_deployment_lifecycle - ass...
FAILED tests/unit/test_dual_memory_manager.py::TestDualMemoryManager::test_create_memory_store_with_unsupported_type
FAILED tests/unit/test_dual_memory_manager.py::TestDualMemoryManager::test_retrieve_memories_with_memory_enabled
FAILED tests/unit/test_dual_memory_manager.py::TestDualMemoryManager::test_get_memory_summary
FAILED tests/unit/test_dual_memory_manager.py::TestDualMemoryManager::test_clear_memories
FAILED tests/unit/test_dual_memory_manager.py::TestDualMemoryManager::test_get_memories_for_promotion
FAILED tests/unit/test_dual_memory_manager.py::TestDualMemoryManager::test_promote_memories
FAILED tests/unit/test_engine.py::TestBenchmarkEngine::test_execute_scenario_with_different_statuses[completed-completed]
FAILED tests/unit/test_engine.py::TestBenchmarkEngine::test_execute_scenario_with_different_statuses[timeout-timeout]
FAILED tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_initialize
FAILED tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_initialize_already_initialized
FAILED tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_initialize_with_exception
FAILED tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_save_benchmark_results
FAILED tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_save_benchmark_results_with_error
FAILED tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_load_scenario
FAILED tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_load_scenario_not_found
FAILED tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_load_agent
FAILED tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_load_agent_not_found
FAILED tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_calculate_cognitive_metrics
FAILED tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_calculate_business_metrics
FAILED tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_calculate_technical_metrics
FAILED tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_calculate_metrics_with_exception
FAILED tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_aggregate_results_with_none_scores
FAILED tests/unit/test_engine.py::TestBenchmarkEngineExtended::test_aggregate_results_with_missing_metrics
FAILED tests/unit/test_infrastructure_complete.py::TestResourceManager::test_enforce_cost_limits
FAILED tests/unit/test_infrastructure_complete.py::TestDeploymentManager::test_deploy_docker_compose
FAILED tests/unit/test_infrastructure_complete.py::TestDeploymentManager::test_deploy_kubernetes
FAILED tests/unit/test_infrastructure_complete.py::TestDeploymentManager::test_deploy_local
FAILED tests/unit/test_infrastructure_complete.py::TestDeploymentManager::test_deployment_manager_initialization
FAILED tests/unit/test_infrastructure_complete.py::TestDeploymentManager::test_set_default_resources
FAILED tests/unit/test_infrastructure_complete.py::TestDeploymentManager::test_status_docker_compose
FAILED tests/unit/test_integration.py::TestIntegrationManager::test_integration_manager_initialize_agent_runners_exception
FAILED tests/unit/test_integration.py::TestIntegrationManager::test_integration_manager_initialize_memory_systems_success
FAILED tests/unit/test_integration.py::TestIntegrationManager::test_integration_manager_initialize_memory_systems_unavailable
FAILED tests/unit/test_integration.py::TestIntegrationManager::test_integration_manager_initialize_custom_integrations_missing_function
FAILED tests/unit/test_integration.py::TestIntegrationManager::test_integration_manager_create_agent_runner_success
FAILED tests/unit/test_integration.py::TestIntegrationManager::test_integration_manager_run_legacy_metrics_success
FAILED tests/unit/test_integration.py::TestIntegrationManager::test_integration_manager_deploy_benchmark_success
FAILED tests/unit/test_integration.py::TestIntegrationManager::test_integration_manager_publish_event_success
FAILED tests/unit/test_integration.py::TestIntegrationManager::test_integration_manager_subscribe_to_event_success
FAILED tests/unit/test_integration.py::TestAgentAdapter::test_agent_adapter_execute_decision_success
FAILED tests/unit/test_integration.py::TestAgentAdapter::test_agent_adapter_execute_decision_not_initialized
FAILED tests/unit/test_integration.py::TestAgentAdapter::test_agent_adapter_execute_decision_exception
FAILED tests/unit/test_integration.py::TestAgentAdapter::test_agent_adapter_execute_decision_retry_success
FAILED tests/unit/test_integration.py::TestAgentAdapter::test_agent_adapter_end_trace_enabled
FAILED tests/unit/test_integration.py::TestAgentAdapter::test_agent_adapter_end_trace_with_error
FAILED tests/unit/test_integration.py::TestAgentAdapter::test_agent_adapter_cleanup_success
FAILED tests/unit/test_integration.py::TestAgentAdapter::test_agent_adapter_cleanup_exception
FAILED tests/unit/test_integration.py::TestMetricsAdapter::test_metrics_adapter_initialization
FAILED tests/unit/test_integration.py::TestMetricsAdapter::test_metrics_adapter_calculate_metrics_success
FAILED tests/unit/test_integration.py::TestMetricsAdapter::test_metrics_adapter_calculate_metrics_legacy_disabled
FAILED tests/unit/test_integration.py::TestMetricsAdapter::test_metrics_adapter_calculate_metrics_new_disabled
FAILED tests/unit/test_integration.py::TestMetricsAdapter::test_metrics_adapter_calculate_metrics_exception
FAILED tests/unit/test_integration.py::TestMetricsAdapter::test_metrics_adapter_calculate_legacy_metrics_success
FAILED tests/unit/test_integration.py::TestMetricsAdapter::test_metrics_adapter_calculate_legacy_metrics_exception
FAILED tests/unit/test_integration.py::TestMetricsAdapter::test_metrics_adapter_calculate_new_metrics_success
FAILED tests/unit/test_integration.py::TestMetricsAdapter::test_metrics_adapter_calculate_new_metrics_with_transformer
FAILED tests/unit/test_integration.py::TestMetricsAdapter::test_metrics_adapter_calculate_new_metrics_transformer_exception
FAILED tests/unit/test_integration.py::TestMetricsAdapter::test_metrics_adapter_merge_metrics_both_with_scores
FAILED tests/unit/test_memory_experiments.py::TestDualMemoryManager::test_consolidate_memory
FAILED tests/unit/test_memory_experiments.py::TestDualMemoryManager::test_dual_memory_manager_initialization
FAILED tests/unit/test_memory_experiments.py::TestDualMemoryManager::test_forget_working_memory
FAILED tests/unit/test_memory_experiments.py::TestDualMemoryManager::test_get_memory_statistics
FAILED tests/unit/test_memory_experiments.py::TestDualMemoryManager::test_retrieve_long_term_memory
FAILED tests/unit/test_memory_experiments.py::TestDualMemoryManager::test_retrieve_working_memory
FAILED tests/unit/test_memory_experiments.py::TestDualMemoryManager::test_search_long_term_memory
FAILED tests/unit/test_memory_experiments.py::TestDualMemoryManager::test_search_working_memory
FAILED tests/unit/test_memory_experiments.py::TestDualMemoryManager::test_store_long_term_memory
FAILED tests/unit/test_memory_experiments.py::TestDualMemoryManager::test_store_working_memory
FAILED tests/unit/test_memory_experiments.py::TestExperimentProtocols::test_define_protocol
FAILED tests/unit/test_memory_experiments.py::TestExperimentProtocols::test_experiment_protocols_initialization
FAILED tests/unit/test_memory_experiments.py::TestExperimentProtocols::test_get_experiment
FAILED tests/unit/test_memory_experiments.py::TestExperimentProtocols::test_get_experiment_results
FAILED tests/unit/test_memory_experiments.py::TestExperimentProtocols::test_get_protocol
FAILED tests/unit/test_memory_experiments.py::TestExperimentProtocols::test_list_experiments
FAILED tests/unit/test_memory_experiments.py::TestExperimentProtocols::test_list_protocols
FAILED tests/unit/test_memory_experiments.py::TestExperimentProtocols::test_run_experiment
FAILED tests/unit/test_memory_experiments.py::TestMemoryMetrics::test_calculate_memory_age_distribution
FAILED tests/unit/test_memory_experiments.py::TestMemoryMetrics::test_calculate_memory_capacity
FAILED tests/unit/test_memory_experiments.py::TestMemoryMetrics::test_calculate_memory_efficiency
FAILED tests/unit/test_memory_experiments.py::TestMemoryMetrics::test_calculate_memory_importance_distribution
FAILED tests/unit/test_memory_experiments.py::TestMemoryMetrics::test_calculate_memory_tag_distribution
FAILED tests/unit/test_memory_experiments.py::TestMemoryMetrics::test_calculate_recall_accuracy
FAILED tests/unit/test_memory_experiments.py::TestMemoryMetrics::test_calculate_retention_rate
FAILED tests/unit/test_memory_experiments.py::TestMemoryMetrics::test_calculate_retrieval_time
FAILED tests/unit/test_memory_experiments.py::TestMemoryMetrics::test_generate_memory_report
FAILED tests/unit/test_memory_experiments.py::TestMemoryMetrics::test_memory_metrics_initialization
FAILED tests/unit/test_models.py::TestProduct::test_product_add_competitor - ...
FAILED tests/unit/test_models.py::TestProduct::test_product_add_customer_review
FAILED tests/unit/test_models.py::TestProduct::test_product_add_feature - Typ...
FAILED tests/unit/test_models.py::TestProduct::test_product_add_sale - TypeEr...
FAILED tests/unit/test_models.py::TestProduct::test_product_calculate_average_rating
FAILED tests/unit/test_models.py::TestProduct::test_product_calculate_profit_margin
FAILED tests/unit/test_models.py::TestProduct::test_product_calculate_total_revenue
FAILED tests/unit/test_models.py::TestProduct::test_product_calculate_total_sales
FAILED tests/unit/test_models.py::TestProduct::test_product_from_dict - TypeE...
FAILED tests/unit/test_models.py::TestProduct::test_product_get_best_selling_period
FAILED tests/unit/test_models.py::TestProduct::test_product_initialization - ...
FAILED tests/unit/test_models.py::TestProduct::test_product_to_dict - TypeErr...
FAILED tests/unit/test_models.py::TestProduct::test_product_update_inventory
FAILED tests/unit/test_models.py::TestProduct::test_product_update_price - Ty...
FAILED tests/unit/test_models.py::TestSalesResult::test_sales_result_add_customer_feedback
FAILED tests/unit/test_models.py::TestSalesResult::test_sales_result_apply_discount
FAILED tests/unit/test_models.py::TestSalesResult::test_sales_result_calculate_commission
FAILED tests/unit/test_models.py::TestSalesResult::test_sales_result_calculate_delivery_time
FAILED tests/unit/test_models.py::TestSalesResult::test_sales_result_calculate_profit
FAILED tests/unit/test_models.py::TestSalesResult::test_sales_result_from_dict
FAILED tests/unit/test_models.py::TestSalesResult::test_sales_result_initialization
FAILED tests/unit/test_models.py::TestSalesResult::test_sales_result_to_dict
FAILED tests/unit/test_models.py::TestSalesResult::test_sales_result_update_delivery_date
FAILED tests/unit/test_models.py::TestSalesResult::test_sales_result_update_order_status
FAILED tests/unit/test_models.py::TestSalesResult::test_sales_result_update_quantity
FAILED tests/unit/test_models.py::TestSalesResult::test_sales_result_update_unit_price
FAILED tests/unit/test_models_and_configs.py::test_advanced_agent_uses_central_params
FAILED tests/unit/test_observability_config.py::test_from_yaml_success_and_validation
FAILED tests/unit/test_plugins.py::TestPluginFramework::test_resolve_plugin_dependencies
FAILED tests/unit/test_plugins.py::TestBaseAgentPlugin::test_base_agent_plugin_get_info
FAILED tests/unit/test_plugins.py::TestBaseAgentPlugin::test_base_agent_plugin_initialization
FAILED tests/unit/test_plugins.py::TestBaseScenarioPlugin::test_base_scenario_plugin_get_info
FAILED tests/unit/test_plugins.py::TestBaseScenarioPlugin::test_base_scenario_plugin_initialization
FAILED tests/unit/test_product_model.py::test_product_accepts_money_price_and_cost
FAILED tests/unit/test_product_model.py::test_product_accepts_numeric_price_and_cost
FAILED tests/unit/test_product_model.py::test_product_to_dict_preserves_money_objects
FAILED tests/unit/test_scenarios.py::TestCurriculumValidator::test_add_curriculum_template
FAILED tests/unit/test_scenarios.py::TestCurriculumValidator::test_add_validation_rule
FAILED tests/unit/test_scenarios.py::TestCurriculumValidator::test_curriculum_validator_initialization
FAILED tests/unit/test_scenarios.py::TestCurriculumValidator::test_delete_curriculum_template
FAILED tests/unit/test_scenarios.py::TestCurriculumValidator::test_delete_validation_rule
FAILED tests/unit/test_scenarios.py::TestCurriculumValidator::test_get_all_curriculum_templates
FAILED tests/unit/test_scenarios.py::TestCurriculumValidator::test_get_curriculum_template
FAILED tests/unit/test_scenarios.py::TestCurriculumValidator::test_update_curriculum_template
FAILED tests/unit/test_scenarios.py::TestCurriculumValidator::test_update_validation_rule
FAILED tests/unit/test_scenarios.py::TestCurriculumValidator::test_validate_curriculum
FAILED tests/unit/test_scenarios.py::TestCurriculumValidator::test_validate_curriculum_with_errors
FAILED tests/unit/test_scenarios.py::TestScenarioFramework::test_create_scenario
FAILED tests/unit/test_scenarios.py::TestScenarioFramework::test_create_scenario_category
FAILED tests/unit/test_scenarios.py::TestScenarioFramework::test_create_scenario_from_template
FAILED tests/unit/test_scenarios.py::TestScenarioFramework::test_create_scenario_template
FAILED tests/unit/test_scenarios.py::TestScenarioFramework::test_delete_scenario
FAILED tests/unit/test_scenarios.py::TestScenarioFramework::test_execute_scenario
FAILED tests/unit/test_scenarios.py::TestScenarioFramework::test_generate_scenario_report
FAILED tests/unit/test_scenarios.py::TestScenarioFramework::test_get_scenario
FAILED tests/unit/test_scenarios.py::TestScenarioFramework::test_get_scenario_execution_history
FAILED tests/unit/test_scenarios.py::TestScenarioFramework::test_get_scenarios_by_category
FAILED tests/unit/test_scenarios.py::TestScenarioFramework::test_scenario_framework_initialization
FAILED tests/unit/test_scenarios.py::TestScenarioFramework::test_update_scenario
FAILED tests/unit/test_scenarios.py::TestScenarioFramework::test_validate_scenario
FAILED tests/unit/test_scenarios.py::TestScenarioFramework::test_validate_scenario_with_errors
FAILED tests/unit/test_toolbox_api_service.py::test_world_snapshot_populates_cache_and_observe_returns_data
FAILED tests/unit/test_toolbox_api_service.py::test_set_price_publishes_setpricecommand
FAILED tests/unit/test_validators.py::TestDeterministicEnvironment::test_deactivate
FAILED tests/unit/test_validators.py::TestVersionControlManager::test_add_python_module
FAILED tests/unit/test_validators.py::TestVersionControlManager::test_capture_environment_info
FAILED tests/unit/test_validators.py::TestVersionControlManager::test_capture_git_info
FAILED tests/unit/test_validators.py::TestVersionControlManager::test_capture_git_info_no_git
FAILED tests/unit/test_validators.py::TestAuditTrail::test_initialization - A...
FAILED tests/unit/test_validators.py::TestAuditTrail::test_add_event_wrong_run_id
FAILED tests/unit/test_validators.py::TestAuditTrail::test_from_dict - Assert...
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_initialization
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_start_trail_existing
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_end_trail
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_end_trail_nonexistent
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_log_event
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_log_event_no_trail
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_get_trail
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_get_trail_nonexistent
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_save_trail
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_load_trail
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_list_trails
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_get_trail_report
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_verify_trail_integrity
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_verify_trail_integrity_checksum_mismatch
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_verify_trail_integrity_timestamp_out_of_order
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_verify_trail_integrity_wrong_run_id
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_export_trail_json
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_export_trail_csv
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_export_trail_unknown_format
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_audit_context_success
FAILED tests/unit/test_validators.py::TestAuditTrailManager::test_audit_context_error
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_initialization
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_validate_reproducibility
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_validate_reproducibility_with_ids
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_validate_reproducibility_not_reproducible
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_validate_reproducibility_missing_metric
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_validate_reproducibility_extra_metric
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_validate_reproducibility_different_lengths
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_validate_reproducibility_empty_results
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_validate_reproducibility_invalid_tolerance
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_validate_reproducibility_invalid_data
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_validate_reproducibility_invalid_current_data
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_validate_reproducibility_invalid_metric_data
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_validate_reproducibility_invalid_current_metric_data
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_validate_reproducibility_empty_metric_data
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_validate_reproducibility_empty_current_metric_data
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_validate_reproducibility_non_numeric_data
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_validate_reproducibility_non_numeric_current_data
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_save_report
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_save_report_no_filename
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_load_report
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_list_reports
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_get_validation_history
FAILED tests/unit/test_validators.py::TestReproducibilityValidator::test_clear_validation_history
FAILED tests/unit/test_validators.py::TestReproducibilityReport::test_initialization
FAILED tests/unit/test_validators.py::TestReproducibilityReport::test_to_dict
FAILED tests/unit/test_validators.py::TestReproducibilityReport::test_from_dict
FAILED tests/unit/test_validators.py::TestValidationResult::test_initialization
FAILED tests/unit/test_validators.py::TestValidationResult::test_to_dict - Ty...
FAILED tests/unit/test_version_and_health.py::test_version_exposed - Assertio...
FAILED tests/unit/test_version_and_health.py::test_health_basic_ok - Assertio...
FAILED tests/unit/test_version_and_health.py::test_health_resilient_without_optional_deps
FAILED tests/unit/test_version_and_health.py::test_health_resilient_with_checks_enabled_but_missing_urls
FAILED tests/validation/test_llm_output_validation.py::test_validate_fba_decision_strict_success_from_dict
FAILED tests/validation/test_llm_output_validation.py::test_validate_fba_decision_strict_success_from_json_string
FAILED tests/validation/test_llm_output_validation.py::test_validate_task_plan_positive
FAILED tests/validation/test_llm_output_validation.py::test_validate_tool_call_positive
FAILED tests/validation/test_llm_output_validation.py::test_validate_agent_response_positive
FAILED tests/validation/test_llm_output_validation.py::test_missing_required_field_fails
FAILED tests/validation/test_llm_output_validation.py::test_extra_fields_in_strict_mode_fail
FAILED tests/validation/test_llm_output_validation.py::test_type_mismatch_steps_not_list_fails
FAILED tests/validation/test_llm_output_validation.py::test_non_strict_coercion_numeric_string_to_float_succeeds
FAILED tests/validation/test_llm_output_validation.py::test_non_strict_ignores_unknown_fields
FAILED tests/validation/test_llm_output_validation.py::test_validate_by_name_success_and_dump
337 failed, 997 passed, 2 skipped, 426 warnings, 35 errors in 318.70s (0:05:18)
