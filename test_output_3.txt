C:\Users\admin\AppData\Roaming\Python\Python313\site-packages\pytest_asyncio\plugin.py:252: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts =============================
platform win32 -- Python 3.13.5, pytest-8.3.4, pluggy-1.5.0 -- C:\ProgramData\anaconda3\python.exe
cachedir: .pytest_cache
rootdir: C:\Users\admin\GitHub-projects\fba\FBA-Bench-Enterprise\tests
configfile: pytest.ini
plugins: anyio-4.11.0, Faker-37.11.0, asyncio-1.2.0, cov-7.0.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 1 item

tests\integration\test_tier2_golden_master.py::test_create_enterprise_v1_0_baseline 2026-01-05 04:58:31.279 | INFO | src.services.external_service | [req=-] | ExternalServiceManager initialized
2026-01-05 04:58:31.293 | INFO | src.services.world_store.persistence | [req=-] | InMemoryStorageBackend initialized.
2026-01-05 04:58:31.293 | INFO | src.services.world_store.arbitration | [req=-] | CommandArbitrator initialized
2026-01-05 04:58:31.294 | INFO | src.services.world_store | [req=-] | WorldStore initialized - ready for multi-agent command processing
2026-01-05 04:58:31.294 | INFO | src.services.trust_score_service | [req=-] | TrustScoreService initialized with config: {}
FAILED

================================== FAILURES ===================================
____________________ test_create_enterprise_v1_0_baseline _____________________

    @pytest.mark.asyncio
    async def test_create_enterprise_v1_0_baseline():
        """
        Runs the Tier 2 detailed scenario ("supply chain crisis") and saves the
        results as the 'Enterprise Version 1.0 Baseline'.
        """
        scenario_data = load_tier2_scenario()
    
        # Extract config
        # Duration is in days in the yaml
        duration_days = scenario_data.get("expected_duration", 180)
    
        # Use a fixed seed for the Golden Master
        seed = 42
    
        # Configure simulation
        # Note: sim_factory uses SimulationConfig internally.
        # We depend on sim_factory fixture which is defined in conftest or similar,
        # but here we might need to use the one from test_reproducibility.py or define our own.
        # To avoid dependency issues, I'll instantiate the stack mostly manually or use the fixture if available.
        # But sim_factory is defined in test_reproducibility.py usually.
        # Let's import the specific factory if possible or copy the logic.
        # In integration tests, we usually have access to 'create_test_simulation' from IntegrationTestSuite.
        # But verify_golden_masters runs specific files.
    
        # Let's try to do it cleanly using the orchestration classes directly
        # to ensure we have full control over event injection.
    
    
        # We need a robust way to run the simulation logic.
        # IntegrationTestSuite.create_test_simulation is good but we need to inject the specific events
        # defined in the YAML.
    
        from integration_tests.test_scientific_reproducibility import TestScientificReproducibility
        suite = TestScientificReproducibility()
        suite.setup_method() # Minimal setup
    
        # Create env
>       env = await suite.create_test_simulation(tier="T2", seed=seed)

tests\integration\test_tier2_golden_master.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <integration_tests.test_scientific_reproducibility.TestScientificReproducibility object at 0x000001A3E27497F0>
tier = 'T2', seed = 42

    async def create_test_simulation(self, tier: str = "T0", seed: int = None) -> Dict[str, Any]:
        """
        Create a complete test simulation environment.
    
        Returns:
            Dict containing all initialized components:
            - orchestrator: SimulationOrchestrator
            - event_bus: EventBus
            - metric_suite: MetricSuite
            - budget_enforcer: BudgetEnforcer
            - world_store: WorldStore
            - services: Dict of initialized services
        """
        seed = seed or self.config.seed
    
        # Initialize core components
        # Allow override via SIM_* env vars; fall back to existing defaults to preserve behavior when unset
        max_ticks = int(os.getenv("SIM_MAX_TICKS", "1000"))
        tick_interval = float(os.getenv("SIM_TICK_INTERVAL_SECONDS", "0.01"))
        time_accel = float(os.getenv("SIM_TIME_ACCELERATION", "50.0"))
        sim_config = SimulationConfig(
            seed=seed,
            max_ticks=max_ticks,
            tick_interval_seconds=tick_interval,
            time_acceleration=time_accel,
        )
        orchestrator = SimulationOrchestrator(sim_config)
        event_bus: EventBus = get_event_bus()  # Use forward reference for type hint
    
        # Initialize services
        world_store = WorldStore()
        fee_service = FeeCalculationService(config={})  # Assume default config for now
        sales_service = SalesService(
            config={}, fee_service=fee_service
        )  # Pass config and fee_service
        trust_service = TrustScoreService(config={})  # Pass config
>       financial_audit_service = FinancialAuditService(config={})  # Pass config
E       TypeError: FinancialAuditService.__init__() got an unexpected keyword argument 'config'

integration_tests\__init__.py:168: TypeError
------------------------------ Captured log call ------------------------------
WARNING  benchmarking.agents.unified_agent:unified_agent.py:29 Services module not available. Some agent functionality may be limited.
============================== warnings summary ===============================
src\fba_bench_api\models\agents.py:7
  C:\Users\admin\GitHub-projects\fba\FBA-Bench-Enterprise\src\fba_bench_api\models\agents.py:7: DeprecationWarning: The legacy schema-based configuration system in 'benchmarking.config' is deprecated and will be removed in a future version. Please use the Pydantic-based configuration models (e.g., PydanticBenchmarkConfig) instead.
    from benchmarking.config.pydantic_config import UnifiedAgentRunnerConfig

src\fba_events\adversarial.py:9
  C:\Users\admin\GitHub-projects\fba\FBA-Bench-Enterprise\src\fba_events\adversarial.py:9: DeprecationWarning: The 'money' package is deprecated; use 'fba_bench_core.money'. This shim will be removed in a future release.
    from money import Money

integration/test_tier2_golden_master.py::test_create_enterprise_v1_0_baseline
  C:\Users\admin\GitHub-projects\fba\FBA-Bench-Enterprise\events\__init__.py:18: DeprecationWarning: fba_events.compat is deprecated; import from fba_events.* instead
    from fba_events.compat import (

integration/test_tier2_golden_master.py: 19 warnings
  C:\Users\admin\AppData\Roaming\Python\Python313\site-packages\pydantic\_internal\_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)

integration/test_tier2_golden_master.py::test_create_enterprise_v1_0_baseline
  C:\ProgramData\anaconda3\Lib\site-packages\pythonjsonlogger\jsonlogger.py:11: DeprecationWarning: pythonjsonlogger.jsonlogger has been moved to pythonjsonlogger.json
    warnings.warn(

integration/test_tier2_golden_master.py::test_create_enterprise_v1_0_baseline
  C:\Users\admin\GitHub-projects\fba\FBA-Bench-Enterprise\integration_tests\test_scientific_reproducibility.py:134: PytestUnknownMarkWarning: Unknown pytest.mark.reproducibility - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.reproducibility

integration/test_tier2_golden_master.py::test_create_enterprise_v1_0_baseline
  C:\Users\admin\GitHub-projects\fba\FBA-Bench-Enterprise\integration_tests\test_scientific_reproducibility.py:265: PytestUnknownMarkWarning: Unknown pytest.mark.reproducibility - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.reproducibility

integration/test_tier2_golden_master.py::test_create_enterprise_v1_0_baseline
  C:\Users\admin\GitHub-projects\fba\FBA-Bench-Enterprise\integration_tests\test_scientific_reproducibility.py:402: PytestUnknownMarkWarning: Unknown pytest.mark.reproducibility - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.reproducibility

integration/test_tier2_golden_master.py::test_create_enterprise_v1_0_baseline
  C:\Users\admin\GitHub-projects\fba\FBA-Bench-Enterprise\integration_tests\test_scientific_reproducibility.py:519: PytestUnknownMarkWarning: Unknown pytest.mark.reproducibility - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.reproducibility

integration/test_tier2_golden_master.py::test_create_enterprise_v1_0_baseline
  C:\Users\admin\GitHub-projects\fba\FBA-Bench-Enterprise\integration_tests\test_scientific_reproducibility.py:710: PytestUnknownMarkWarning: Unknown pytest.mark.reproducibility - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.reproducibility

integration/test_tier2_golden_master.py::test_create_enterprise_v1_0_baseline
  C:\Users\admin\GitHub-projects\fba\FBA-Bench-Enterprise\integration_tests\test_scientific_reproducibility.py:838: PytestUnknownMarkWarning: Unknown pytest.mark.reproducibility - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.reproducibility

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED tests\integration\test_tier2_golden_master.py::test_create_enterprise_v1_0_baseline
======================= 1 failed, 29 warnings in 1.68s ========================
